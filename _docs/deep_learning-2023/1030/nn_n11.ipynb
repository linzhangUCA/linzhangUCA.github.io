{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpCXZyr9B2NS"
      },
      "source": [
        "# NNN11 - Cats vs. Dogs Classification\n",
        "```console\n",
        "pip install torch opencv-python pandas\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LRkskvlB2NW"
      },
      "source": [
        "## 1. Create Annotation File\n",
        "Data is not always stored in NumPy arrays. Most cases, you will have to organize and annotate the raw data stored in your hard drive. For image data, you want it to be organized as the following way.\n",
        "``` console\n",
        "root/dog/xxx.jpg\n",
        "root/dog/xxy.jpg\n",
        "root/dog/xxz.jpg\n",
        "\n",
        "root/cat/123.jpg\n",
        "root/cat/456.jpg\n",
        "root/cat/789.jpg\n",
        "```\n",
        "Then we can use the following coding block to grab image information and store them in an comma-seperated values (CSV) file.\n",
        "1. Visit the data directory, grab all images' paths and corresponding categories.\n",
        "2. Save the paths and categories of images in an `.csv` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfMoAUSXB2NX",
        "outputId": "38c878c6-0412-45b8-ec20-8627bba84b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 279 cat images, and 278 dog images in the training dataset\n",
            "There are 70 cat images, and 70 dog images in the test dataset\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Locate train and test directories\n",
        "root_dir = os.path.join(sys.path[0], \"dataset\")  # locate dataset directory from this repo in the whole system\n",
        "train_dir = os.path.join(root_dir, \"train\")\n",
        "test_dir = os.path.join(root_dir, \"test\")\n",
        "categories = ['cats', 'dogs']\n",
        "\n",
        "# Glob training files\n",
        "train_cat_files = glob(os.path.join(train_dir, categories[0], \"*.jpg\"))\n",
        "train_dog_files = glob(os.path.join(train_dir, categories[1], \"*.jpg\"))\n",
        "print(f\"There are {len(train_cat_files)} cat images, and {len(train_dog_files)} dog images in the training dataset\")\n",
        "train_image_files = train_cat_files + train_dog_files\n",
        "train_labels = ['cat'] * len(train_cat_files) + ['dog'] * len(train_dog_files)\n",
        "train_data_dict = {'path': train_image_files, 'label': train_labels}\n",
        "df_train = pd.DataFrame(train_data_dict)\n",
        "# print(df_train)\n",
        "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
        "\n",
        "# Glob test files\n",
        "test_cat_files = glob(os.path.join(test_dir, categories[0], \"*.jpg\"))\n",
        "test_dog_files = glob(os.path.join(test_dir, categories[1], \"*.jpg\"))\n",
        "print(f\"There are {len(test_cat_files)} cat images, and {len(test_dog_files)} dog images in the test dataset\")\n",
        "test_image_files = test_cat_files + test_dog_files\n",
        "test_labels = ['cat'] * len(test_cat_files) + ['dog'] * len(test_dog_files)\n",
        "test_data_dict = {'path': test_image_files, 'label': test_labels}\n",
        "df_test = pd.DataFrame(test_data_dict)\n",
        "# print(df_test)\n",
        "df_test.to_csv('annotation_test.csv', header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQVIRhFCB2NZ"
      },
      "source": [
        "## 2. Create Dataset\n",
        "[PyTorch](https://pytorch.org/) as a popuplar deep learning programming framework has a bunch of handy tools that allow you to easily access the data.\n",
        "1. Inherit the `Dataset` class to build a customized `CatsDogsDataset` class.\n",
        "2. Instantiate the customized class to a `dataset_train` and `dataset_test` .\n",
        "3. Further create dataloaders to shuffle the data and access the full matrix of the features and the targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msfzah-fB2NZ",
        "outputId": "bcdb5a76-b58d-4752-a5b9-56c058b75b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (100, 100, 3) 0.0\n",
            "100 (100, 100, 3) 0.0\n",
            "200 (100, 100, 3) 0.0\n",
            "300 (100, 100, 3) 1.0\n",
            "400 (100, 100, 3) 1.0\n",
            "500 (100, 100, 3) 1.0\n",
            "556 (100, 100, 3) 1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Create customized dataset\n",
        "class CatsDogsDataset(Dataset):\n",
        "    def __init__(self, annotations_file):\n",
        "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        img_path = self.imgs_info.iloc[idx, 0]\n",
        "        image_raw = cv.imread(img_path)\n",
        "        image_rgb = cv.cvtColor(image_raw, cv.COLOR_BGR2RGB)\n",
        "        image = cv.resize(image_rgb, (100, 100))\n",
        "        category = 1. if self.imgs_info.iloc[idx, 1] == 'dog' else 0.\n",
        "        sample = {'image': image, 'category': category}\n",
        "        return sample\n",
        "\n",
        "# Loop training dataset\n",
        "dataset_train = CatsDogsDataset(annotations_file='annotation_train.csv')\n",
        "for i, sample in enumerate(dataset_train):\n",
        "    image = sample['image']\n",
        "    category = sample['category']\n",
        "    if not i%100:\n",
        "        print(i, image.shape, category)\n",
        "print(i, image.shape, category)\n",
        "\n",
        "dataset_test = CatsDogsDataset(annotations_file='annotation_test.csv')\n",
        "\n",
        "# Create shuffled data loader\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=1000, shuffle=True)\n",
        "samples = next(iter(dataloader_train))\n",
        "fig, axs = plt.subplots(1, 4)\n",
        "for i in range(4):\n",
        "    image = samples['image'][i]\n",
        "    category = samples['category'][i]\n",
        "    axs[i] = plt.subplot(1, 4, i + 1)\n",
        "    axs[i].set_title(f'Sample #{i+1}: {category}')\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(image)\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_TkhJkPB2NZ"
      },
      "source": [
        "## 3. Preprocess the Data\n",
        "A typical binary classification dataset is made up with a feature matrix: $\\mathbf{X} = [^{(1)}\\mathbf{x}, ^{(2)}\\mathbf{x}, ..., ^{(M)}\\mathbf{x}]^T$. and a target vector $\\mathbf{y} = [^{(1)}y, ^{(2)}y, ..., ^{(M)}y]^T$. Where $M$ is the total number of instances in the dataset, $^{(m)}\\mathbf{x}$ is a normalized and flattened image array, and $^{(m)}y \\in {0, 1}$.\n",
        "\n",
        "- A colored image is usually represented by a **3-dimensional array with shape $(width, height, 3)$**. Where, $width$ indicates number of pixels on horizontal edge, $height$ indicates number of pixels on vertical edge.\n",
        "- Usually, we use an **integer ranged 0~255** to describe a pixel, which is the intensity of that color channel.\n",
        "- At current stage, we would like to convert an image array into a row vector, or a **2d array with shape $(1, width*height*3)$**.\n",
        "\n",
        "![](https://miro.medium.com/v2/format:webp/1*pFywKuWmz7Xk07OXxPiX2Q.png)\n",
        "\n",
        "We will access the raw data by extracting it from the dataloaders. Then, process and prepare the raw data so that it can be used in later steps.\n",
        "1. Separate raw feature array and target array.\n",
        "2. Reshape feature array and target array.\n",
        "3. Rescale feature arrary, represent each pixel with a float numbers in range 0~1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9i4oeIMB2Na",
        "outputId": "02ec6280-0155-4309-9df2-6c4225f9f696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(557, 30000) (140, 30000) (557, 1) (140, 1)\n"
          ]
        }
      ],
      "source": [
        "# Extract features/images and targets/labels\n",
        "data_train = next(iter(dataloader_train))\n",
        "data_test = next(iter(dataloader_test))\n",
        "\n",
        "\n",
        "# Separate features from targets\n",
        "raw_feature_train = data_train['image'].numpy()\n",
        "raw_feature_test = data_test['image'].numpy()\n",
        "raw_target_train = data_train['category'].numpy()\n",
        "raw_target_test = data_test['category'].numpy()\n",
        "\n",
        "# Reshape feature matrix to (M, width*height*3), target vector to (M, 1)\n",
        "reshaped_feature_train = raw_feature_train.reshape(raw_feature_train.shape[0], -1)\n",
        "reshaped_feature_test = raw_feature_test.reshape(raw_feature_test.shape[0], -1)\n",
        "reshaped_target_train = raw_target_train.reshape(raw_target_train.shape[0], 1)\n",
        "reshaped_target_test = raw_target_test.reshape(raw_target_test.shape[0], 1)\n",
        "\n",
        "# Rescale features within range: 0~1\n",
        "rescaled_feature_train = reshaped_feature_train / 255\n",
        "rescaled_feature_test = reshaped_feature_test / 255\n",
        "\n",
        "# Finalize data to be used later\n",
        "feature_train = rescaled_feature_train\n",
        "feature_test = rescaled_feature_test\n",
        "target_train = reshaped_target_train\n",
        "target_test = reshaped_target_test\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "print(feature_train.shape, feature_test.shape, target_train.shape, target_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5OLpp5OB2Na"
      },
      "source": [
        "## 4. Forward Pass\n",
        "\n",
        "1. Compute first (hidden) layer:\n",
        "    1. $\\mathbf{Z}^{[1]} = \\mathbf{X}^{[0]} \\cdot \\mathbf{W}^{[1]T} + \\mathbf{b}^{[1]}$\n",
        "    2. $\\mathbf{X}^{[1]} = \\sigma(\\mathbf{Z}^{[1]}) = 1 / (1 + e^{-\\mathbf{Z}^{[1]}})$\n",
        "2. Compute second (output) layer:\n",
        "    1. $\\mathbf{Z}^{[2]} = \\mathbf{X}^{[1]} \\cdot \\mathbf{w}^{[2]T} + b^{[2]}$\n",
        "    2. $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{Z}^{[2]}) = 1 / (1 + e^{-\\mathbf{Z}^{[2]}})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvX_G3guB2Nb",
        "outputId": "3eaf95a4-f577-4c79-a44a-e7e847fb83eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.49603533]\n",
            " [0.49767655]\n",
            " [0.4342204 ]\n",
            " [0.29758724]\n",
            " [0.33296668]\n",
            " [0.32336101]\n",
            " [0.51174221]\n",
            " [0.61595378]\n",
            " [0.51984815]\n",
            " [0.19275071]]\n",
            "(10, 1)\n",
            "(10, 5)\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\" Sigmoid function\n",
        "    Args:\n",
        "        x: independent variable, could be an arrary of any shape or a scalar.\n",
        "    Returns:\n",
        "        y: dependent variable, could be an arrary of any shape or a scalar.\n",
        "    \"\"\"\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "\n",
        "def linear(feature, weight, bias):\n",
        "    return np.dot(feature, weight.T) + bias\n",
        "\n",
        "def forward(feature_0, params):\n",
        "    \"\"\" Logistic model function\n",
        "    Args:\n",
        "        feature: feature matrix, 2d-array with shape (# samples, # pixels)\n",
        "        params: a dictionary contians all weights (W_1, w_2) and biases (b_1, b_2)\n",
        "    Returns:\n",
        "        prediction: model predicted value, a 2-D numpy array with shape (# samples, 1)\n",
        "    \"\"\"\n",
        "    Z_1 = linear(feature_0, params['W_1'], params['b_1'])\n",
        "    feature_1 = sigmoid(Z_1)\n",
        "    Z_2 = linear(feature_1, params['w_2'], params['b_2'])\n",
        "    prediction = sigmoid(Z_2)\n",
        "    return prediction, feature_1\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "np.random.seed(3321)\n",
        "X0 = np.random.normal(size=(10, 8))  # dummy input, 8 samples, 10 features in each sample.\n",
        "W1 = np.random.normal(size=(5, 8))  # First layer weight matrix, (num_1nd_layer_features, num_input_layer_features)\n",
        "b1 = np.random.normal(size=(1, 5))  # First layer bias vector, (1, num_1st_layer_features)\n",
        "w2 = np.random.normal(size=(1, 5))  # Second layer weight vector, (1, num_2nd_layer_features)\n",
        "b2 = np.random.normal()  # Second layer bias, scalar\n",
        "dummy_params = {\n",
        "    'W_1': W1,\n",
        "    'b_1': b1,\n",
        "    'w_2': w2,\n",
        "    'b_2': b2,\n",
        "}\n",
        "yhat, X1 = forward(X0, dummy_params)\n",
        "print(yhat)\n",
        "print(yhat.shape)  # (10, 1)\n",
        "print(X1.shape)  # (10, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn4gWFoIB2Nb"
      },
      "source": [
        "## 5. Binary Cross Entropy\n",
        "It is OK to use a Mean square error function to compute the loss (difference between target and prediction). It is better to use a binary cross entropy function to compute the loss for a binary classification problem.\n",
        "\n",
        "#### $\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{M} \\Sigma [-\\mathbf{\\hat{y}} \\log \\mathbf{y} - (1 - \\mathbf{\\hat{y}}) \\log(1 - \\mathbf{y})]$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQbWxxKWB2Nc",
        "outputId": "314329a9-e5e6-4fda-8608-779b8cccde9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7463187674914531\n"
          ]
        }
      ],
      "source": [
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    \"\"\"\n",
        "    Binary Cross Entropy function\n",
        "        Args:\n",
        "            prediction: model predicted value, a 2-D numpy array with shape (# samples, 1)\n",
        "            target: labeled value from data set, a 2-D numpy array with shape (# samples, 1)\n",
        "        Returns:\n",
        "            loss_value: averaged CE error, a scalar\n",
        "    \"\"\"\n",
        "    loss_value = -np.mean(target * np.log(prediction) + (1 - target) * np.log(1 - prediction))\n",
        "    return loss_value\n",
        "\n",
        "# Sanity check\n",
        "np.random.seed(3321)\n",
        "print(binary_cross_entropy_loss(yhat, np.random.randint(0, 2, size=(10, 1))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTFn0WebB2Nc"
      },
      "source": [
        "## 6. Backpropagation\n",
        "\n",
        "### 2nd Layer:\n",
        "\n",
        "### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}^{[2]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{w}^{[2]}} = \\frac{1}{M} (\\mathbf{\\hat{y} - y})^T \\cdot \\mathbf{X}^{[1]}$\n",
        "### $\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}^{[2]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial b^{[2]}} = \\frac{1}{M} \\Sigma (\\mathbf{\\hat{y} - y})$\n",
        "### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{Z}^{[2]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{X}^{[1]}} = (\\mathbf{\\hat{y} - y}) \\cdot \\mathbf{w}^{[2]}$\n",
        "\n",
        "### 1st Layer:\n",
        "\n",
        "### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} \\cdot \\frac{\\partial \\mathbf{X}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{w}^{[1]}} = [(\\mathbf{\\hat{y} - y}) \\cdot \\mathbf{w}^{[2]} * \\mathbf{X}^{[1]} * (1 - \\mathbf{X}^{[1]})]^T \\cdot \\mathbf{X}^{[0]}$\n",
        "### $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}^{[1]}} \\cdot \\frac{\\partial \\mathbf{X}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{b}^{[1]}} = \\frac{1}{M} \\Sigma (\\mathbf{\\hat{y} - y}) \\cdot \\mathbf{w}^{[2]} * \\mathbf{X}^{[1]} * (1 - \\mathbf{X}^{[1]}) \\text{, axis=0}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPu2ZcL-B2Nc",
        "outputId": "9ed08a7c-d8dc-461b-e064-a18efcb83763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1 training loss: 0.6931472656902652, test loss: 0.6931471404338816 \n",
            "train accuracy: 0.4991023339317774, test accuracy: 0.5\n",
            "Iteration 2 training loss: 0.6931470813397187, test loss: 0.6931471444920269 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 3 training loss: 0.6931469174476007, test loss: 0.6931471582470322 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 4 training loss: 0.693146771719409, test loss: 0.6931471800285081 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 5 training loss: 0.6931466421179653, test loss: 0.693147208387017 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 6 training loss: 0.6931465268345594, test loss: 0.693147242067357 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 7 training loss: 0.6931464242633292, test loss: 0.6931472799849512 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 8 training loss: 0.693146332978514, test loss: 0.6931473212049911 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 9 training loss: 0.693146251714256, test loss: 0.6931473649240177 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 10 training loss: 0.6931461793466692, test loss: 0.6931474104536619 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 11 training loss: 0.6931461148779172, test loss: 0.6931474572062969 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 12 training loss: 0.6931460574220768, test loss: 0.6931475046823808 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 13 training loss: 0.6931460061925871, test loss: 0.6931475524593002 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 14 training loss: 0.6931459604911056, test loss: 0.6931476001815348 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 15 training loss: 0.6931459196976144, test loss: 0.6931476475519988 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 16 training loss: 0.6931458832616354, test loss: 0.6931476943244146 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 17 training loss: 0.693145850694431, test loss: 0.6931477402966071 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 18 training loss: 0.6931458215620796, test loss: 0.693147785304606 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 19 training loss: 0.6931457954793273, test loss: 0.6931478292174654 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 20 training loss: 0.693145772104129, test loss: 0.6931478719327154 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 21 training loss: 0.6931457511328024, test loss: 0.693147913372374 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 22 training loss: 0.693145732295725, test loss: 0.6931479534794491 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 23 training loss: 0.6931457153535135, test loss: 0.6931479922148803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 24 training loss: 0.6931457000936329, test loss: 0.69314802955486 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 25 training loss: 0.6931456863273838, test loss: 0.6931480654884975 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 26 training loss: 0.6931456738872298, test loss: 0.6931481000157776 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 27 training loss: 0.693145662624423, test loss: 0.6931481331457849 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 28 training loss: 0.6931456524068974, test loss: 0.6931481648951585 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 29 training loss: 0.6931456431173967, test loss: 0.6931481952867493 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 30 training loss: 0.6931456346518138, test loss: 0.6931482243484581 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 31 training loss: 0.6931456269177154, test loss: 0.6931482521122299 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 32 training loss: 0.6931456198330332, test loss: 0.6931482786131863 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 33 training loss: 0.6931456133249003, test loss: 0.6931483038888802 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 34 training loss: 0.6931456073286202, test loss: 0.6931483279786546 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 35 training loss: 0.6931456017867489, test loss: 0.6931483509230962 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 36 training loss: 0.6931455966482828, test loss: 0.6931483727635698 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 37 training loss: 0.6931455918679352, test loss: 0.6931483935418234 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 38 training loss: 0.693145587405496, test loss: 0.6931484132996554 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 39 training loss: 0.693145583225261, test loss: 0.6931484320786362 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 40 training loss: 0.6931455792955277, test loss: 0.6931484499198765 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 41 training loss: 0.6931455755881454, test loss: 0.693148466863837 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 42 training loss: 0.6931455720781177, test loss: 0.6931484829501718 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 43 training loss: 0.6931455687432478, test loss: 0.6931484982176047 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 44 training loss: 0.6931455655638252, test loss: 0.6931485127038299 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 45 training loss: 0.6931455625223464, test loss: 0.6931485264454372 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 46 training loss: 0.6931455596032672, test loss: 0.6931485394778552 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 47 training loss: 0.6931455567927832, test loss: 0.6931485518353118 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 48 training loss: 0.6931455540786347, test loss: 0.6931485635508098 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 49 training loss: 0.6931455514499333, test loss: 0.6931485746561122 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 50 training loss: 0.693145548897008, test loss: 0.6931485851817407 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 51 training loss: 0.6931455464112684, test loss: 0.6931485951569804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 52 training loss: 0.6931455439850843, test loss: 0.6931486046098919 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 53 training loss: 0.6931455416116773, test loss: 0.6931486135673315 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 54 training loss: 0.6931455392850251, test loss: 0.6931486220549724 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 55 training loss: 0.6931455369997774, test loss: 0.6931486300973329 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 56 training loss: 0.69314553475118, test loss: 0.6931486377178067 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 57 training loss: 0.6931455325350081, test loss: 0.6931486449386947 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 58 training loss: 0.6931455303475068, test loss: 0.6931486517812396 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 59 training loss: 0.6931455281853385, test loss: 0.6931486582656617 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 60 training loss: 0.693145526045536, test loss: 0.6931486644111947 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 61 training loss: 0.693145523925461, test loss: 0.6931486702361227 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 62 training loss: 0.6931455218227673, test loss: 0.6931486757578167 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 63 training loss: 0.6931455197353681, test loss: 0.6931486809927725 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 64 training loss: 0.693145517661406, test loss: 0.6931486859566458 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 65 training loss: 0.693145515599229, test loss: 0.6931486906642891 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 66 training loss: 0.6931455135473659, test loss: 0.693148695129786 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 67 training loss: 0.6931455115045063, test loss: 0.693148699366487 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 68 training loss: 0.6931455094694837, test loss: 0.6931487033870415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 69 training loss: 0.6931455074412578, test loss: 0.6931487072034315 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 70 training loss: 0.693145505418901, test loss: 0.6931487108270031 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 71 training loss: 0.6931455034015863, test loss: 0.6931487142684968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 72 training loss: 0.6931455013885751, test loss: 0.6931487175380766 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 73 training loss: 0.6931454993792079, test loss: 0.6931487206453594 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 74 training loss: 0.6931454973728949, test loss: 0.6931487235994415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 75 training loss: 0.6931454953691089, test loss: 0.6931487264089252 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 76 training loss: 0.6931454933673774, test loss: 0.6931487290819435 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 77 training loss: 0.6931454913672775, test loss: 0.6931487316261846 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 78 training loss: 0.6931454893684291, test loss: 0.6931487340489147 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 79 training loss: 0.6931454873704913, test loss: 0.693148736357 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 80 training loss: 0.6931454853731572, test loss: 0.6931487385569276 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 81 training loss: 0.6931454833761505, test loss: 0.6931487406548257 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 82 training loss: 0.6931454813792213, test loss: 0.6931487426564823 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 83 training loss: 0.6931454793821445, test loss: 0.6931487445673635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 84 training loss: 0.6931454773847159, test loss: 0.6931487463926311 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 85 training loss: 0.6931454753867501, test loss: 0.6931487481371587 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 86 training loss: 0.6931454733880787, test loss: 0.6931487498055464 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 87 training loss: 0.6931454713885482, test loss: 0.6931487514021383 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 88 training loss: 0.6931454693880178, test loss: 0.6931487529310332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 89 training loss: 0.6931454673863591, test loss: 0.6931487543960998 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 90 training loss: 0.6931454653834538, test loss: 0.6931487558009894 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 91 training loss: 0.6931454633791926, test loss: 0.6931487571491475 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 92 training loss: 0.6931454613734748, test loss: 0.6931487584438253 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 93 training loss: 0.6931454593662068, test loss: 0.6931487596880895 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 94 training loss: 0.6931454573573014, test loss: 0.6931487608848347 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 95 training loss: 0.6931454553466772, test loss: 0.6931487620367905 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 96 training loss: 0.6931454533342577, test loss: 0.6931487631465328 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 97 training loss: 0.6931454513199713, test loss: 0.6931487642164909 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 98 training loss: 0.6931454493037498, test loss: 0.6931487652489561 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 99 training loss: 0.693145447285529, test loss: 0.6931487662460902 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 100 training loss: 0.6931454452652478, test loss: 0.6931487672099318 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 101 training loss: 0.6931454432428474, test loss: 0.6931487681424034 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 102 training loss: 0.6931454412182723, test loss: 0.6931487690453189 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 103 training loss: 0.6931454391914681, test loss: 0.6931487699203882 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 104 training loss: 0.6931454371623835, test loss: 0.693148770769224 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 105 training loss: 0.6931454351309682, test loss: 0.6931487715933475 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 106 training loss: 0.693145433097173, test loss: 0.693148772394193 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 107 training loss: 0.6931454310609514, test loss: 0.6931487731731132 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 108 training loss: 0.6931454290222565, test loss: 0.6931487739313833 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 109 training loss: 0.6931454269810436, test loss: 0.6931487746702062 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 110 training loss: 0.6931454249372685, test loss: 0.6931487753907161 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 111 training loss: 0.6931454228908877, test loss: 0.6931487760939825 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 112 training loss: 0.6931454208418583, test loss: 0.6931487767810136 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 113 training loss: 0.6931454187901386, test loss: 0.6931487774527607 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 114 training loss: 0.6931454167356869, test loss: 0.6931487781101201 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 115 training loss: 0.6931454146784619, test loss: 0.6931487787539377 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 116 training loss: 0.6931454126184236, test loss: 0.6931487793850107 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 117 training loss: 0.693145410555531, test loss: 0.6931487800040911 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 118 training loss: 0.6931454084897448, test loss: 0.6931487806118874 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 119 training loss: 0.6931454064210248, test loss: 0.6931487812090685 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 120 training loss: 0.6931454043493319, test loss: 0.693148781796265 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 121 training loss: 0.6931454022746266, test loss: 0.6931487823740712 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 122 training loss: 0.6931454001968702, test loss: 0.6931487829430476 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 123 training loss: 0.6931453981160235, test loss: 0.6931487835037232 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 124 training loss: 0.6931453960320478, test loss: 0.6931487840565964 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 125 training loss: 0.6931453939449043, test loss: 0.6931487846021377 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 126 training loss: 0.6931453918545543, test loss: 0.6931487851407905 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 127 training loss: 0.6931453897609595, test loss: 0.6931487856729727 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 128 training loss: 0.6931453876640812, test loss: 0.6931487861990793 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 129 training loss: 0.693145385563881, test loss: 0.6931487867194824 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 130 training loss: 0.6931453834603203, test loss: 0.6931487872345329 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 131 training loss: 0.6931453813533608, test loss: 0.6931487877445619 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 132 training loss: 0.6931453792429639, test loss: 0.693148788249882 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 133 training loss: 0.6931453771290914, test loss: 0.6931487887507877 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 134 training loss: 0.6931453750117048, test loss: 0.6931487892475571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 135 training loss: 0.6931453728907654, test loss: 0.6931487897404522 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 136 training loss: 0.6931453707662348, test loss: 0.6931487902297212 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 137 training loss: 0.6931453686380744, test loss: 0.6931487907155973 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 138 training loss: 0.6931453665062457, test loss: 0.6931487911983009 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 139 training loss: 0.6931453643707101, test loss: 0.6931487916780401 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 140 training loss: 0.6931453622314288, test loss: 0.6931487921550115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 141 training loss: 0.6931453600883631, test loss: 0.6931487926294001 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 142 training loss: 0.6931453579414744, test loss: 0.6931487931013813 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 143 training loss: 0.6931453557907234, test loss: 0.6931487935711201 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 144 training loss: 0.6931453536360718, test loss: 0.6931487940387727 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 145 training loss: 0.6931453514774801, test loss: 0.6931487945044863 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 146 training loss: 0.6931453493149098, test loss: 0.6931487949684004 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 147 training loss: 0.6931453471483213, test loss: 0.6931487954306459 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 148 training loss: 0.6931453449776757, test loss: 0.6931487958913475 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 149 training loss: 0.6931453428029336, test loss: 0.6931487963506222 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 150 training loss: 0.6931453406240559, test loss: 0.6931487968085814 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 151 training loss: 0.693145338441003, test loss: 0.6931487972653296 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 152 training loss: 0.6931453362537355, test loss: 0.6931487977209662 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 153 training loss: 0.6931453340622139, test loss: 0.6931487981755845 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 154 training loss: 0.6931453318663985, test loss: 0.6931487986292735 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 155 training loss: 0.6931453296662496, test loss: 0.693148799082117 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 156 training loss: 0.6931453274617274, test loss: 0.6931487995341943 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 157 training loss: 0.693145325252792, test loss: 0.6931487999855804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 158 training loss: 0.6931453230394037, test loss: 0.6931488004363466 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 159 training loss: 0.693145320821522, test loss: 0.6931488008865602 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 160 training loss: 0.693145318599107, test loss: 0.6931488013362846 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 161 training loss: 0.6931453163721183, test loss: 0.6931488017855807 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 162 training loss: 0.6931453141405158, test loss: 0.6931488022345054 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 163 training loss: 0.6931453119042591, test loss: 0.6931488026831131 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 164 training loss: 0.6931453096633076, test loss: 0.6931488031314552 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 165 training loss: 0.6931453074176205, test loss: 0.6931488035795806 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 166 training loss: 0.6931453051671573, test loss: 0.6931488040275359 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 167 training loss: 0.6931453029118774, test loss: 0.6931488044753645 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 168 training loss: 0.6931453006517395, test loss: 0.6931488049231088 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 169 training loss: 0.693145298386703, test loss: 0.6931488053708084 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 170 training loss: 0.6931452961167265, test loss: 0.6931488058185007 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 171 training loss: 0.6931452938417689, test loss: 0.693148806266222 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 172 training loss: 0.6931452915617892, test loss: 0.6931488067140064 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 173 training loss: 0.6931452892767458, test loss: 0.6931488071618863 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 174 training loss: 0.6931452869865972, test loss: 0.693148807609893 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 175 training loss: 0.6931452846913018, test loss: 0.6931488080580556 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 176 training loss: 0.6931452823908177, test loss: 0.6931488085064026 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 177 training loss: 0.6931452800851036, test loss: 0.693148808954961 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 178 training loss: 0.6931452777741173, test loss: 0.6931488094037563 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 179 training loss: 0.6931452754578168, test loss: 0.6931488098528131 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 180 training loss: 0.6931452731361599, test loss: 0.6931488103021551 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 181 training loss: 0.6931452708091046, test loss: 0.6931488107518045 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 182 training loss: 0.6931452684766085, test loss: 0.6931488112017827 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 183 training loss: 0.6931452661386291, test loss: 0.6931488116521108 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 184 training loss: 0.6931452637951239, test loss: 0.6931488121028082 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 185 training loss: 0.6931452614460502, test loss: 0.6931488125538938 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 186 training loss: 0.6931452590913652, test loss: 0.6931488130053862 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 187 training loss: 0.6931452567310259, test loss: 0.6931488134573026 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 188 training loss: 0.6931452543649895, test loss: 0.6931488139096602 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 189 training loss: 0.6931452519932129, test loss: 0.693148814362475 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 190 training loss: 0.6931452496156527, test loss: 0.6931488148157626 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 191 training loss: 0.6931452472322658, test loss: 0.6931488152695385 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 192 training loss: 0.6931452448430082, test loss: 0.6931488157238169 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 193 training loss: 0.6931452424478368, test loss: 0.6931488161786123 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 194 training loss: 0.693145240046708, test loss: 0.693148816633938 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 195 training loss: 0.6931452376395774, test loss: 0.6931488170898074 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 196 training loss: 0.6931452352264015, test loss: 0.6931488175462335 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 197 training loss: 0.693145232807136, test loss: 0.6931488180032281 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 198 training loss: 0.6931452303817369, test loss: 0.693148818460804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 199 training loss: 0.69314522795016, test loss: 0.6931488189189727 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 200 training loss: 0.6931452255123602, test loss: 0.6931488193777456 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 201 training loss: 0.6931452230682937, test loss: 0.6931488198371338 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 202 training loss: 0.6931452206179152, test loss: 0.6931488202971484 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 203 training loss: 0.6931452181611802, test loss: 0.6931488207577999 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 204 training loss: 0.6931452156980438, test loss: 0.6931488212190985 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 205 training loss: 0.6931452132284607, test loss: 0.6931488216810543 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 206 training loss: 0.6931452107523859, test loss: 0.6931488221436775 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 207 training loss: 0.6931452082697738, test loss: 0.6931488226069776 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 208 training loss: 0.693145205780579, test loss: 0.6931488230709644 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 209 training loss: 0.6931452032847559, test loss: 0.693148823535647 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 210 training loss: 0.693145200782259, test loss: 0.6931488240010346 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 211 training loss: 0.6931451982730421, test loss: 0.6931488244671364 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 212 training loss: 0.6931451957570592, test loss: 0.6931488249339612 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 213 training loss: 0.6931451932342643, test loss: 0.6931488254015179 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 214 training loss: 0.6931451907046111, test loss: 0.6931488258698147 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 215 training loss: 0.6931451881680529, test loss: 0.6931488263388608 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 216 training loss: 0.6931451856245435, test loss: 0.6931488268086641 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 217 training loss: 0.693145183074036, test loss: 0.6931488272792332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 218 training loss: 0.6931451805164836, test loss: 0.693148827750576 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 219 training loss: 0.6931451779518393, test loss: 0.693148828222701 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 220 training loss: 0.6931451753800557, test loss: 0.6931488286956159 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 221 training loss: 0.6931451728010859, test loss: 0.6931488291693291 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 222 training loss: 0.6931451702148823, test loss: 0.6931488296438479 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 223 training loss: 0.6931451676213972, test loss: 0.6931488301191808 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 224 training loss: 0.6931451650205831, test loss: 0.6931488305953354 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 225 training loss: 0.693145162412392, test loss: 0.6931488310723191 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 226 training loss: 0.6931451597967759, test loss: 0.6931488315501397 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 227 training loss: 0.6931451571736866, test loss: 0.6931488320288052 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 228 training loss: 0.6931451545430759, test loss: 0.6931488325083228 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 229 training loss: 0.693145151904895, test loss: 0.6931488329887004 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 230 training loss: 0.6931451492590956, test loss: 0.6931488334699452 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 231 training loss: 0.6931451466056289, test loss: 0.693148833952065 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 232 training loss: 0.6931451439444457, test loss: 0.6931488344350669 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 233 training loss: 0.6931451412754972, test loss: 0.6931488349189584 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 234 training loss: 0.6931451385987338, test loss: 0.6931488354037474 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 235 training loss: 0.6931451359141064, test loss: 0.6931488358894407 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 236 training loss: 0.6931451332215653, test loss: 0.6931488363760461 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 237 training loss: 0.6931451305210609, test loss: 0.693148836863571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 238 training loss: 0.6931451278125429, test loss: 0.6931488373520224 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 239 training loss: 0.6931451250959618, test loss: 0.6931488378414077 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 240 training loss: 0.6931451223712668, test loss: 0.6931488383317346 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 241 training loss: 0.693145119638408, test loss: 0.6931488388230103 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 242 training loss: 0.6931451168973346, test loss: 0.6931488393152418 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 243 training loss: 0.693145114147996, test loss: 0.6931488398084369 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 244 training loss: 0.6931451113903411, test loss: 0.6931488403026029 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 245 training loss: 0.6931451086243191, test loss: 0.6931488407977467 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 246 training loss: 0.6931451058498785, test loss: 0.6931488412938761 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 247 training loss: 0.6931451030669683, test loss: 0.6931488417909983 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 248 training loss: 0.6931451002755366, test loss: 0.6931488422891204 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 249 training loss: 0.6931450974755318, test loss: 0.6931488427882503 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 250 training loss: 0.6931450946669019, test loss: 0.6931488432883949 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 251 training loss: 0.6931450918495948, test loss: 0.6931488437895615 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 252 training loss: 0.6931450890235583, test loss: 0.693148844291758 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 253 training loss: 0.69314508618874, test loss: 0.6931488447949915 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 254 training loss: 0.6931450833450873, test loss: 0.6931488452992692 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 255 training loss: 0.6931450804925472, test loss: 0.693148845804599 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 256 training loss: 0.6931450776310669, test loss: 0.6931488463109879 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 257 training loss: 0.6931450747605931, test loss: 0.6931488468184436 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 258 training loss: 0.6931450718810727, test loss: 0.6931488473269736 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 259 training loss: 0.6931450689924518, test loss: 0.693148847836585 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 260 training loss: 0.693145066094677, test loss: 0.6931488483472857 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 261 training loss: 0.6931450631876942, test loss: 0.6931488488590833 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 262 training loss: 0.6931450602714494, test loss: 0.6931488493719851 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 263 training loss: 0.6931450573458885, test loss: 0.6931488498859986 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 264 training loss: 0.6931450544109568, test loss: 0.6931488504011316 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 265 training loss: 0.6931450514665995, test loss: 0.6931488509173914 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 266 training loss: 0.6931450485127623, test loss: 0.693148851434786 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 267 training loss: 0.6931450455493896, test loss: 0.6931488519533229 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 268 training loss: 0.6931450425764265, test loss: 0.6931488524730095 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 269 training loss: 0.6931450395938175, test loss: 0.6931488529938538 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 270 training loss: 0.6931450366015068, test loss: 0.6931488535158635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 271 training loss: 0.6931450335994389, test loss: 0.6931488540390461 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 272 training loss: 0.6931450305875575, test loss: 0.6931488545634098 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 273 training loss: 0.6931450275658065, test loss: 0.693148855088962 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 274 training loss: 0.6931450245341296, test loss: 0.6931488556157105 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 275 training loss: 0.6931450214924699, test loss: 0.6931488561436633 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 276 training loss: 0.6931450184407709, test loss: 0.6931488566728282 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 277 training loss: 0.6931450153789754, test loss: 0.6931488572032132 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 278 training loss: 0.6931450123070262, test loss: 0.6931488577348262 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 279 training loss: 0.6931450092248659, test loss: 0.6931488582676749 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 280 training loss: 0.6931450061324367, test loss: 0.6931488588017675 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 281 training loss: 0.6931450030296807, test loss: 0.6931488593371118 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 282 training loss: 0.6931449999165404, test loss: 0.6931488598737162 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 283 training loss: 0.693144996792957, test loss: 0.6931488604115884 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 284 training loss: 0.693144993658872, test loss: 0.6931488609507366 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 285 training loss: 0.693144990514227, test loss: 0.6931488614911689 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 286 training loss: 0.6931449873589628, test loss: 0.6931488620328937 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 287 training loss: 0.6931449841930205, test loss: 0.6931488625759187 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 288 training loss: 0.6931449810163406, test loss: 0.6931488631202523 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 289 training loss: 0.6931449778288636, test loss: 0.6931488636659027 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 290 training loss: 0.6931449746305298, test loss: 0.6931488642128784 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 291 training loss: 0.6931449714212791, test loss: 0.6931488647611874 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 292 training loss: 0.6931449682010512, test loss: 0.6931488653108383 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 293 training loss: 0.6931449649697858, test loss: 0.6931488658618392 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 294 training loss: 0.6931449617274222, test loss: 0.6931488664141986 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 295 training loss: 0.6931449584738995, test loss: 0.6931488669679249 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 296 training loss: 0.6931449552091565, test loss: 0.6931488675230266 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 297 training loss: 0.6931449519331317, test loss: 0.693148868079512 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 298 training loss: 0.693144948645764, test loss: 0.69314886863739 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 299 training loss: 0.6931449453469912, test loss: 0.6931488691966687 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 300 training loss: 0.6931449420367515, test loss: 0.6931488697573572 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 301 training loss: 0.6931449387149822, test loss: 0.6931488703194635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 302 training loss: 0.6931449353816213, test loss: 0.6931488708829968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 303 training loss: 0.6931449320366057, test loss: 0.6931488714479653 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 304 training loss: 0.6931449286798725, test loss: 0.6931488720143782 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 305 training loss: 0.6931449253113587, test loss: 0.693148872582244 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 306 training loss: 0.6931449219310006, test loss: 0.6931488731515715 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 307 training loss: 0.6931449185387345, test loss: 0.6931488737223694 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 308 training loss: 0.6931449151344965, test loss: 0.6931488742946469 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 309 training loss: 0.6931449117182226, test loss: 0.6931488748684126 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 310 training loss: 0.693144908289848, test loss: 0.6931488754436756 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 311 training loss: 0.6931449048493082, test loss: 0.6931488760204447 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 312 training loss: 0.6931449013965385, test loss: 0.6931488765987289 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 313 training loss: 0.6931448979314734, test loss: 0.6931488771785373 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 314 training loss: 0.6931448944540477, test loss: 0.6931488777598792 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 315 training loss: 0.6931448909641954, test loss: 0.6931488783427635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 316 training loss: 0.6931448874618511, test loss: 0.6931488789271993 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 317 training loss: 0.6931448839469482, test loss: 0.6931488795131956 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 318 training loss: 0.6931448804194202, test loss: 0.6931488801007621 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 319 training loss: 0.6931448768792008, test loss: 0.6931488806899079 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 320 training loss: 0.6931448733262227, test loss: 0.693148881280642 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 321 training loss: 0.693144869760419, test loss: 0.6931488818729741 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 322 training loss: 0.6931448661817218, test loss: 0.6931488824669133 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 323 training loss: 0.6931448625900638, test loss: 0.6931488830624692 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 324 training loss: 0.6931448589853767, test loss: 0.6931488836596511 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 325 training loss: 0.6931448553675924, test loss: 0.6931488842584687 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 326 training loss: 0.6931448517366424, test loss: 0.6931488848589312 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 327 training loss: 0.6931448480924579, test loss: 0.6931488854610487 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 328 training loss: 0.6931448444349696, test loss: 0.6931488860648302 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 329 training loss: 0.6931448407641085, test loss: 0.6931488866702858 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 330 training loss: 0.6931448370798047, test loss: 0.6931488872774249 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 331 training loss: 0.6931448333819886, test loss: 0.6931488878862575 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 332 training loss: 0.6931448296705898, test loss: 0.693148888496793 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 333 training loss: 0.693144825945538, test loss: 0.6931488891090415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 334 training loss: 0.6931448222067624, test loss: 0.6931488897230129 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 335 training loss: 0.6931448184541921, test loss: 0.6931488903387166 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 336 training loss: 0.6931448146877558, test loss: 0.6931488909561634 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 337 training loss: 0.6931448109073819, test loss: 0.6931488915753624 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 338 training loss: 0.6931448071129986, test loss: 0.693148892196324 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 339 training loss: 0.6931448033045339, test loss: 0.6931488928190584 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 340 training loss: 0.6931447994819152, test loss: 0.6931488934435754 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 341 training loss: 0.6931447956450699, test loss: 0.6931488940698853 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 342 training loss: 0.6931447917939251, test loss: 0.6931488946979985 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 343 training loss: 0.6931447879284073, test loss: 0.6931488953279246 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 344 training loss: 0.6931447840484433, test loss: 0.6931488959596747 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 345 training loss: 0.6931447801539588, test loss: 0.6931488965932585 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 346 training loss: 0.6931447762448799, test loss: 0.6931488972286866 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 347 training loss: 0.693144772321132, test loss: 0.6931488978659692 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 348 training loss: 0.6931447683826406, test loss: 0.6931488985051171 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 349 training loss: 0.6931447644293305, test loss: 0.6931488991461403 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 350 training loss: 0.6931447604611264, test loss: 0.6931488997890499 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 351 training loss: 0.6931447564779525, test loss: 0.6931489004338561 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 352 training loss: 0.693144752479733, test loss: 0.6931489010805697 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 353 training loss: 0.6931447484663915, test loss: 0.6931489017292013 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 354 training loss: 0.6931447444378516, test loss: 0.6931489023797616 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 355 training loss: 0.6931447403940362, test loss: 0.6931489030322613 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 356 training loss: 0.6931447363348683, test loss: 0.6931489036867114 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 357 training loss: 0.6931447322602702, test loss: 0.6931489043431227 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 358 training loss: 0.6931447281701643, test loss: 0.693148905001506 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 359 training loss: 0.6931447240644724, test loss: 0.6931489056618723 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 360 training loss: 0.6931447199431159, test loss: 0.6931489063242326 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 361 training loss: 0.6931447158060162, test loss: 0.693148906988598 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 362 training loss: 0.693144711653094, test loss: 0.6931489076549796 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 363 training loss: 0.6931447074842703, test loss: 0.6931489083233884 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 364 training loss: 0.693144703299465, test loss: 0.6931489089938356 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 365 training loss: 0.6931446990985979, test loss: 0.6931489096663325 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 366 training loss: 0.6931446948815891, test loss: 0.6931489103408907 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 367 training loss: 0.6931446906483576, test loss: 0.6931489110175209 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 368 training loss: 0.6931446863988224, test loss: 0.6931489116962347 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 369 training loss: 0.6931446821329021, test loss: 0.693148912377044 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 370 training loss: 0.693144677850515, test loss: 0.6931489130599597 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 371 training loss: 0.6931446735515789, test loss: 0.6931489137449935 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 372 training loss: 0.6931446692360117, test loss: 0.6931489144321571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 373 training loss: 0.6931446649037304, test loss: 0.6931489151214622 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 374 training loss: 0.6931446605546523, test loss: 0.6931489158129202 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 375 training loss: 0.6931446561886936, test loss: 0.693148916506543 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 376 training loss: 0.6931446518057707, test loss: 0.6931489172023423 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 377 training loss: 0.6931446474057996, test loss: 0.6931489179003302 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 378 training loss: 0.6931446429886957, test loss: 0.6931489186005182 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 379 training loss: 0.6931446385543744, test loss: 0.6931489193029186 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 380 training loss: 0.6931446341027504, test loss: 0.6931489200075429 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 381 training loss: 0.6931446296337382, test loss: 0.6931489207144036 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 382 training loss: 0.6931446251472523, test loss: 0.6931489214235126 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 383 training loss: 0.6931446206432059, test loss: 0.6931489221348823 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 384 training loss: 0.6931446161215129, test loss: 0.6931489228485246 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 385 training loss: 0.6931446115820861, test loss: 0.693148923564452 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 386 training loss: 0.6931446070248384, test loss: 0.6931489242826765 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 387 training loss: 0.6931446024496822, test loss: 0.6931489250032106 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 388 training loss: 0.6931445978565294, test loss: 0.6931489257260667 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 389 training loss: 0.6931445932452915, test loss: 0.6931489264512575 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 390 training loss: 0.69314458861588, test loss: 0.6931489271787954 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 391 training loss: 0.6931445839682058, test loss: 0.6931489279086929 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 392 training loss: 0.6931445793021791, test loss: 0.6931489286409626 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 393 training loss: 0.6931445746177103, test loss: 0.6931489293756172 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 394 training loss: 0.6931445699147092, test loss: 0.6931489301126698 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 395 training loss: 0.693144565193085, test loss: 0.6931489308521327 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 396 training loss: 0.6931445604527469, test loss: 0.693148931594019 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 397 training loss: 0.6931445556936032, test loss: 0.6931489323383417 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 398 training loss: 0.6931445509155626, test loss: 0.6931489330851137 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 399 training loss: 0.6931445461185327, test loss: 0.6931489338343481 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 400 training loss: 0.6931445413024211, test loss: 0.6931489345860578 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 401 training loss: 0.6931445364671347, test loss: 0.693148935340256 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 402 training loss: 0.6931445316125804, test loss: 0.693148936096956 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 403 training loss: 0.6931445267386643, test loss: 0.6931489368561712 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 404 training loss: 0.6931445218452926, test loss: 0.6931489376179147 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 405 training loss: 0.6931445169323706, test loss: 0.6931489383821999 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 406 training loss: 0.6931445119998033, test loss: 0.6931489391490404 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 407 training loss: 0.6931445070474958, test loss: 0.6931489399184496 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 408 training loss: 0.6931445020753519, test loss: 0.6931489406904412 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 409 training loss: 0.693144497083276, test loss: 0.6931489414650285 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 410 training loss: 0.6931444920711713, test loss: 0.6931489422422253 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 411 training loss: 0.6931444870389412, test loss: 0.6931489430220457 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 412 training loss: 0.6931444819864881, test loss: 0.6931489438045031 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 413 training loss: 0.6931444769137143, test loss: 0.6931489445896115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 414 training loss: 0.6931444718205216, test loss: 0.6931489453773848 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 415 training loss: 0.6931444667068117, test loss: 0.6931489461678372 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 416 training loss: 0.6931444615724855, test loss: 0.6931489469609825 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 417 training loss: 0.6931444564174437, test loss: 0.693148947756835 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 418 training loss: 0.6931444512415862, test loss: 0.6931489485554088 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 419 training loss: 0.6931444460448131, test loss: 0.693148949356718 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 420 training loss: 0.6931444408270234, test loss: 0.6931489501607772 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 421 training loss: 0.6931444355881162, test loss: 0.6931489509676004 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 422 training loss: 0.6931444303279901, test loss: 0.6931489517772025 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 423 training loss: 0.6931444250465431, test loss: 0.6931489525895977 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 424 training loss: 0.6931444197436725, test loss: 0.6931489534048006 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 425 training loss: 0.6931444144192758, test loss: 0.693148954222826 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 426 training loss: 0.6931444090732496, test loss: 0.6931489550436883 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 427 training loss: 0.6931444037054904, test loss: 0.6931489558674027 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 428 training loss: 0.6931443983158936, test loss: 0.6931489566939835 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 429 training loss: 0.693144392904355, test loss: 0.693148957523446 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 430 training loss: 0.6931443874707696, test loss: 0.6931489583558049 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 431 training loss: 0.6931443820150316, test loss: 0.6931489591910757 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 432 training loss: 0.6931443765370353, test loss: 0.6931489600292728 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 433 training loss: 0.693144371036674, test loss: 0.6931489608704118 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 434 training loss: 0.6931443655138414, test loss: 0.6931489617145079 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 435 training loss: 0.6931443599684298, test loss: 0.6931489625615765 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 436 training loss: 0.6931443544003315, test loss: 0.6931489634116328 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 437 training loss: 0.6931443488094382, test loss: 0.6931489642646922 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 438 training loss: 0.6931443431956413, test loss: 0.6931489651207704 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 439 training loss: 0.6931443375588318, test loss: 0.693148965979883 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 440 training loss: 0.6931443318988997, test loss: 0.6931489668420452 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 441 training loss: 0.6931443262157351, test loss: 0.6931489677072733 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 442 training loss: 0.6931443205092275, test loss: 0.693148968575583 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 443 training loss: 0.6931443147792657, test loss: 0.6931489694469898 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 444 training loss: 0.6931443090257384, test loss: 0.6931489703215098 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 445 training loss: 0.6931443032485334, test loss: 0.6931489711991592 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 446 training loss: 0.6931442974475382, test loss: 0.693148972079954 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 447 training loss: 0.6931442916226399, test loss: 0.6931489729639103 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 448 training loss: 0.6931442857737251, test loss: 0.6931489738510445 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 449 training loss: 0.6931442799006797, test loss: 0.6931489747413724 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 450 training loss: 0.6931442740033894, test loss: 0.693148975634911 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 451 training loss: 0.6931442680817391, test loss: 0.6931489765316764 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 452 training loss: 0.6931442621356135, test loss: 0.6931489774316851 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 453 training loss: 0.6931442561648968, test loss: 0.6931489783349538 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 454 training loss: 0.6931442501694722, test loss: 0.6931489792414993 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 455 training loss: 0.6931442441492232, test loss: 0.6931489801513381 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 456 training loss: 0.693144238104032, test loss: 0.6931489810644873 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 457 training loss: 0.6931442320337807, test loss: 0.6931489819809635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 458 training loss: 0.693144225938351, test loss: 0.6931489829007841 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 459 training loss: 0.6931442198176239, test loss: 0.6931489838239656 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 460 training loss: 0.6931442136714798, test loss: 0.6931489847505256 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 461 training loss: 0.6931442074997988, test loss: 0.6931489856804809 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 462 training loss: 0.6931442013024602, test loss: 0.6931489866138492 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 463 training loss: 0.6931441950793434, test loss: 0.6931489875506477 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 464 training loss: 0.6931441888303261, test loss: 0.6931489884908938 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 465 training loss: 0.6931441825552869, test loss: 0.6931489894346051 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 466 training loss: 0.6931441762541029, test loss: 0.6931489903817992 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 467 training loss: 0.6931441699266505, test loss: 0.6931489913324937 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 468 training loss: 0.6931441635728067, test loss: 0.6931489922867066 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 469 training loss: 0.693144157192447, test loss: 0.6931489932444554 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 470 training loss: 0.6931441507854466, test loss: 0.6931489942057584 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 471 training loss: 0.69314414435168, test loss: 0.6931489951706331 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 472 training loss: 0.6931441378910216, test loss: 0.6931489961390982 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 473 training loss: 0.6931441314033449, test loss: 0.6931489971111716 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 474 training loss: 0.6931441248885228, test loss: 0.6931489980868715 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 475 training loss: 0.693144118346428, test loss: 0.6931489990662163 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 476 training loss: 0.6931441117769324, test loss: 0.6931490000492244 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 477 training loss: 0.6931441051799071, test loss: 0.6931490010359144 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 478 training loss: 0.6931440985552232, test loss: 0.6931490020263048 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 479 training loss: 0.6931440919027507, test loss: 0.6931490030204142 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 480 training loss: 0.6931440852223595, test loss: 0.6931490040182617 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 481 training loss: 0.6931440785139187, test loss: 0.6931490050198658 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 482 training loss: 0.6931440717772966, test loss: 0.6931490060252455 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 483 training loss: 0.6931440650123614, test loss: 0.6931490070344201 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 484 training loss: 0.6931440582189804, test loss: 0.6931490080474083 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 485 training loss: 0.6931440513970203, test loss: 0.6931490090642297 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 486 training loss: 0.6931440445463475, test loss: 0.6931490100849031 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 487 training loss: 0.6931440376668275, test loss: 0.6931490111094484 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 488 training loss: 0.6931440307583253, test loss: 0.6931490121378847 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 489 training loss: 0.6931440238207055, test loss: 0.6931490131702318 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 490 training loss: 0.6931440168538321, test loss: 0.6931490142065092 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 491 training loss: 0.6931440098575681, test loss: 0.6931490152467366 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 492 training loss: 0.6931440028317759, test loss: 0.693149016290934 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 493 training loss: 0.6931439957763184, test loss: 0.6931490173391209 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 494 training loss: 0.6931439886910562, test loss: 0.6931490183913179 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 495 training loss: 0.6931439815758508, test loss: 0.6931490194475446 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 496 training loss: 0.693143974430562, test loss: 0.6931490205078213 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 497 training loss: 0.6931439672550497, test loss: 0.6931490215721686 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 498 training loss: 0.6931439600491726, test loss: 0.6931490226406064 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 499 training loss: 0.6931439528127895, test loss: 0.6931490237131556 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 500 training loss: 0.6931439455457579, test loss: 0.6931490247898363 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 501 training loss: 0.6931439382479351, test loss: 0.6931490258706696 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 502 training loss: 0.6931439309191774, test loss: 0.6931490269556759 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 503 training loss: 0.6931439235593408, test loss: 0.6931490280448764 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 504 training loss: 0.6931439161682807, test loss: 0.6931490291382918 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 505 training loss: 0.6931439087458516, test loss: 0.693149030235943 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 506 training loss: 0.6931439012919075, test loss: 0.6931490313378514 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 507 training loss: 0.6931438938063017, test loss: 0.6931490324440384 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 508 training loss: 0.6931438862888869, test loss: 0.6931490335545246 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 509 training loss: 0.6931438787395152, test loss: 0.6931490346693323 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 510 training loss: 0.6931438711580379, test loss: 0.6931490357884824 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 511 training loss: 0.6931438635443057, test loss: 0.6931490369119967 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 512 training loss: 0.6931438558981687, test loss: 0.6931490380398971 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 513 training loss: 0.6931438482194766, test loss: 0.6931490391722053 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 514 training loss: 0.693143840508078, test loss: 0.6931490403089433 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 515 training loss: 0.6931438327638206, test loss: 0.6931490414501327 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 516 training loss: 0.6931438249865524, test loss: 0.693149042595796 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 517 training loss: 0.6931438171761197, test loss: 0.6931490437459557 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 518 training loss: 0.693143809332369, test loss: 0.6931490449006336 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 519 training loss: 0.6931438014551452, test loss: 0.6931490460598523 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 520 training loss: 0.6931437935442933, test loss: 0.6931490472236346 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 521 training loss: 0.6931437855996573, test loss: 0.6931490483920028 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 522 training loss: 0.6931437776210806, test loss: 0.6931490495649795 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 523 training loss: 0.6931437696084056, test loss: 0.6931490507425879 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 524 training loss: 0.6931437615614744, test loss: 0.6931490519248511 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 525 training loss: 0.6931437534801284, test loss: 0.6931490531117918 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 526 training loss: 0.6931437453642078, test loss: 0.6931490543034332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 527 training loss: 0.6931437372135526, test loss: 0.6931490554997987 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 528 training loss: 0.693143729028002, test loss: 0.6931490567009115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 529 training loss: 0.6931437208073945, test loss: 0.6931490579067953 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 530 training loss: 0.6931437125515675, test loss: 0.6931490591174736 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 531 training loss: 0.6931437042603582, test loss: 0.6931490603329701 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 532 training loss: 0.6931436959336028, test loss: 0.6931490615533086 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 533 training loss: 0.693143687571137, test loss: 0.6931490627785132 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 534 training loss: 0.6931436791727953, test loss: 0.6931490640086077 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 535 training loss: 0.6931436707384121, test loss: 0.693149065243616 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 536 training loss: 0.6931436622678205, test loss: 0.6931490664835629 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 537 training loss: 0.6931436537608533, test loss: 0.6931490677284725 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 538 training loss: 0.6931436452173424, test loss: 0.6931490689783691 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 539 training loss: 0.6931436366371185, test loss: 0.6931490702332779 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 540 training loss: 0.6931436280200125, test loss: 0.6931490714932228 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 541 training loss: 0.6931436193658537, test loss: 0.6931490727582292 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 542 training loss: 0.6931436106744711, test loss: 0.6931490740283219 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 543 training loss: 0.6931436019456927, test loss: 0.6931490753035257 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 544 training loss: 0.693143593179346, test loss: 0.693149076583866 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 545 training loss: 0.6931435843752574, test loss: 0.6931490778693682 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 546 training loss: 0.6931435755332529, test loss: 0.6931490791600573 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 547 training loss: 0.6931435666531574, test loss: 0.6931490804559591 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 548 training loss: 0.693143557734795, test loss: 0.6931490817570993 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 549 training loss: 0.6931435487779894, test loss: 0.6931490830635033 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 550 training loss: 0.6931435397825634, test loss: 0.6931490843751972 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 551 training loss: 0.6931435307483387, test loss: 0.6931490856922071 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 552 training loss: 0.6931435216751364, test loss: 0.6931490870145589 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 553 training loss: 0.6931435125627768, test loss: 0.6931490883422791 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 554 training loss: 0.6931435034110798, test loss: 0.6931490896753937 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 555 training loss: 0.6931434942198635, test loss: 0.6931490910139291 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 556 training loss: 0.6931434849889463, test loss: 0.6931490923579122 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 557 training loss: 0.6931434757181452, test loss: 0.6931490937073698 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 558 training loss: 0.6931434664072763, test loss: 0.6931490950623284 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 559 training loss: 0.6931434570561552, test loss: 0.6931490964228152 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 560 training loss: 0.6931434476645966, test loss: 0.6931490977888571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 561 training loss: 0.6931434382324144, test loss: 0.6931490991604814 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 562 training loss: 0.6931434287594213, test loss: 0.6931491005377156 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 563 training loss: 0.6931434192454297, test loss: 0.6931491019205869 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 564 training loss: 0.6931434096902508, test loss: 0.693149103309123 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 565 training loss: 0.6931434000936952, test loss: 0.6931491047033517 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 566 training loss: 0.6931433904555726, test loss: 0.6931491061033006 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 567 training loss: 0.6931433807756915, test loss: 0.6931491075089979 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 568 training loss: 0.6931433710538603, test loss: 0.6931491089204721 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 569 training loss: 0.6931433612898855, test loss: 0.6931491103377504 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 570 training loss: 0.6931433514835739, test loss: 0.6931491117608619 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 571 training loss: 0.6931433416347306, test loss: 0.6931491131898351 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 572 training loss: 0.6931433317431601, test loss: 0.6931491146246982 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 573 training loss: 0.6931433218086661, test loss: 0.6931491160654802 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 574 training loss: 0.6931433118310513, test loss: 0.6931491175122103 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 575 training loss: 0.6931433018101175, test loss: 0.6931491189649169 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 576 training loss: 0.6931432917456659, test loss: 0.6931491204236297 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 577 training loss: 0.6931432816374965, test loss: 0.6931491218883779 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 578 training loss: 0.6931432714854084, test loss: 0.6931491233591905 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 579 training loss: 0.6931432612892, test loss: 0.6931491248360976 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 580 training loss: 0.6931432510486687, test loss: 0.6931491263191286 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 581 training loss: 0.6931432407636111, test loss: 0.6931491278083135 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 582 training loss: 0.6931432304338229, test loss: 0.6931491293036822 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 583 training loss: 0.6931432200590983, test loss: 0.6931491308052647 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 584 training loss: 0.6931432096392317, test loss: 0.6931491323130915 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 585 training loss: 0.6931431991740153, test loss: 0.6931491338271927 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 586 training loss: 0.6931431886632416, test loss: 0.6931491353475991 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 587 training loss: 0.6931431781067016, test loss: 0.6931491368743412 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 588 training loss: 0.6931431675041848, test loss: 0.6931491384074501 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 589 training loss: 0.6931431568554808, test loss: 0.6931491399469565 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 590 training loss: 0.6931431461603779, test loss: 0.6931491414928914 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 591 training loss: 0.6931431354186627, test loss: 0.6931491430452862 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 592 training loss: 0.6931431246301221, test loss: 0.6931491446041724 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 593 training loss: 0.6931431137945412, test loss: 0.6931491461695813 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 594 training loss: 0.6931431029117043, test loss: 0.6931491477415449 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 595 training loss: 0.6931430919813951, test loss: 0.693149149320095 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 596 training loss: 0.6931430810033957, test loss: 0.6931491509052631 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 597 training loss: 0.6931430699774878, test loss: 0.6931491524970818 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 598 training loss: 0.6931430589034518, test loss: 0.6931491540955832 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 599 training loss: 0.6931430477810673, test loss: 0.6931491557007998 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 600 training loss: 0.6931430366101127, test loss: 0.693149157312764 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 601 training loss: 0.6931430253903654, test loss: 0.6931491589315087 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 602 training loss: 0.6931430141216024, test loss: 0.6931491605570669 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 603 training loss: 0.6931430028035989, test loss: 0.6931491621894713 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 604 training loss: 0.6931429914361293, test loss: 0.6931491638287551 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 605 training loss: 0.6931429800189676, test loss: 0.693149165474952 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 606 training loss: 0.6931429685518858, test loss: 0.6931491671280952 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 607 training loss: 0.6931429570346557, test loss: 0.6931491687882182 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 608 training loss: 0.6931429454670477, test loss: 0.6931491704553551 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 609 training loss: 0.6931429338488313, test loss: 0.69314917212954 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 610 training loss: 0.6931429221797749, test loss: 0.6931491738108063 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 611 training loss: 0.6931429104596457, test loss: 0.693149175499189 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 612 training loss: 0.6931428986882103, test loss: 0.6931491771947224 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 613 training loss: 0.6931428868652337, test loss: 0.6931491788974407 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 614 training loss: 0.6931428749904804, test loss: 0.693149180607379 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 615 training loss: 0.6931428630637134, test loss: 0.693149182324572 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 616 training loss: 0.6931428510846949, test loss: 0.6931491840490549 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 617 training loss: 0.693142839053186, test loss: 0.6931491857808629 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 618 training loss: 0.6931428269689465, test loss: 0.6931491875200314 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 619 training loss: 0.6931428148317355, test loss: 0.693149189266596 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 620 training loss: 0.6931428026413107, test loss: 0.6931491910205922 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 621 training loss: 0.6931427903974289, test loss: 0.6931491927820564 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 622 training loss: 0.6931427780998457, test loss: 0.6931491945510242 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 623 training loss: 0.6931427657483158, test loss: 0.6931491963275317 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 624 training loss: 0.6931427533425925, test loss: 0.6931491981116158 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 625 training loss: 0.6931427408824281, test loss: 0.6931491999033128 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 626 training loss: 0.6931427283675741, test loss: 0.6931492017026591 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 627 training loss: 0.6931427157977803, test loss: 0.6931492035096922 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 628 training loss: 0.6931427031727959, test loss: 0.6931492053244488 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 629 training loss: 0.6931426904923688, test loss: 0.6931492071469662 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 630 training loss: 0.6931426777562456, test loss: 0.6931492089772819 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 631 training loss: 0.693142664964172, test loss: 0.6931492108154333 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 632 training loss: 0.6931426521158924, test loss: 0.6931492126614582 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 633 training loss: 0.6931426392111503, test loss: 0.6931492145153949 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 634 training loss: 0.6931426262496877, test loss: 0.693149216377281 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 635 training loss: 0.6931426132312456, test loss: 0.6931492182471549 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 636 training loss: 0.693142600155564, test loss: 0.6931492201250553 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 637 training loss: 0.6931425870223815, test loss: 0.6931492220110207 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 638 training loss: 0.6931425738314355, test loss: 0.6931492239050899 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 639 training loss: 0.6931425605824625, test loss: 0.6931492258073019 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 640 training loss: 0.6931425472751976, test loss: 0.6931492277176959 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 641 training loss: 0.6931425339093749, test loss: 0.6931492296363112 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 642 training loss: 0.6931425204847268, test loss: 0.6931492315631873 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 643 training loss: 0.6931425070009852, test loss: 0.6931492334983641 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 644 training loss: 0.6931424934578803, test loss: 0.6931492354418816 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 645 training loss: 0.6931424798551413, test loss: 0.6931492373937794 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 646 training loss: 0.6931424661924962, test loss: 0.6931492393540984 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 647 training loss: 0.6931424524696714, test loss: 0.6931492413228786 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 648 training loss: 0.6931424386863928, test loss: 0.693149243300161 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 649 training loss: 0.6931424248423845, test loss: 0.693149245285986 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 650 training loss: 0.6931424109373696, test loss: 0.693149247280395 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 651 training loss: 0.6931423969710695, test loss: 0.693149249283429 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 652 training loss: 0.6931423829432051, test loss: 0.6931492512951296 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 653 training loss: 0.6931423688534956, test loss: 0.6931492533155383 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 654 training loss: 0.6931423547016589, test loss: 0.693149255344697 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 655 training loss: 0.6931423404874117, test loss: 0.6931492573826474 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 656 training loss: 0.6931423262104696, test loss: 0.6931492594294321 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 657 training loss: 0.6931423118705468, test loss: 0.6931492614850928 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 658 training loss: 0.693142297467356, test loss: 0.693149263549673 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 659 training loss: 0.693142283000609, test loss: 0.6931492656232148 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 660 training loss: 0.6931422684700159, test loss: 0.6931492677057615 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 661 training loss: 0.693142253875286, test loss: 0.6931492697973559 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 662 training loss: 0.6931422392161268, test loss: 0.6931492718980417 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 663 training loss: 0.6931422244922448, test loss: 0.6931492740078623 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 664 training loss: 0.6931422097033451, test loss: 0.6931492761268616 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 665 training loss: 0.6931421948491312, test loss: 0.6931492782550833 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 666 training loss: 0.6931421799293056, test loss: 0.693149280392572 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 667 training loss: 0.6931421649435695, test loss: 0.6931492825393717 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 668 training loss: 0.6931421498916225, test loss: 0.6931492846955272 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 669 training loss: 0.6931421347731632, test loss: 0.6931492868610831 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 670 training loss: 0.6931421195878883, test loss: 0.6931492890360847 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 671 training loss: 0.6931421043354937, test loss: 0.6931492912205768 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 672 training loss: 0.6931420890156735, test loss: 0.6931492934146052 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 673 training loss: 0.6931420736281209, test loss: 0.6931492956182153 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 674 training loss: 0.693142058172527, test loss: 0.693149297831453 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 675 training loss: 0.6931420426485824, test loss: 0.6931493000543644 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 676 training loss: 0.6931420270559756, test loss: 0.6931493022869953 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 677 training loss: 0.693142011394394, test loss: 0.6931493045293929 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 678 training loss: 0.6931419956635236, test loss: 0.6931493067816034 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 679 training loss: 0.6931419798630487, test loss: 0.693149309043674 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 680 training loss: 0.6931419639926527, test loss: 0.6931493113156516 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 681 training loss: 0.693141948052017, test loss: 0.6931493135975836 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 682 training loss: 0.6931419320408224, test loss: 0.6931493158895173 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 683 training loss: 0.693141915958747, test loss: 0.6931493181915012 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 684 training loss: 0.6931418998054687, test loss: 0.6931493205035827 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 685 training loss: 0.6931418835806631, test loss: 0.69314932282581 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 686 training loss: 0.6931418672840048, test loss: 0.6931493251582316 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 687 training loss: 0.6931418509151669, test loss: 0.6931493275008966 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 688 training loss: 0.6931418344738207, test loss: 0.6931493298538534 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 689 training loss: 0.6931418179596364, test loss: 0.6931493322171514 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 690 training loss: 0.6931418013722824, test loss: 0.6931493345908395 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 691 training loss: 0.6931417847114258, test loss: 0.6931493369749678 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 692 training loss: 0.6931417679767323, test loss: 0.6931493393695859 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 693 training loss: 0.693141751167866, test loss: 0.6931493417747436 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 694 training loss: 0.6931417342844893, test loss: 0.6931493441904918 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 695 training loss: 0.6931417173262633, test loss: 0.6931493466168803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 696 training loss: 0.6931417002928475, test loss: 0.6931493490539602 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 697 training loss: 0.6931416831838998, test loss: 0.6931493515017823 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 698 training loss: 0.6931416659990766, test loss: 0.6931493539603977 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 699 training loss: 0.693141648738033, test loss: 0.6931493564298583 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 700 training loss: 0.6931416314004221, test loss: 0.6931493589102152 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 701 training loss: 0.6931416139858957, test loss: 0.6931493614015208 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 702 training loss: 0.6931415964941041, test loss: 0.693149363903827 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 703 training loss: 0.6931415789246957, test loss: 0.6931493664171862 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 704 training loss: 0.693141561277318, test loss: 0.6931493689416511 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 705 training loss: 0.6931415435516161, test loss: 0.6931493714772744 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 706 training loss: 0.6931415257472339, test loss: 0.6931493740241093 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 707 training loss: 0.6931415078638137, test loss: 0.6931493765822095 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 708 training loss: 0.6931414899009959, test loss: 0.6931493791516281 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 709 training loss: 0.69314147185842, test loss: 0.6931493817324194 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 710 training loss: 0.6931414537357231, test loss: 0.693149384324637 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 711 training loss: 0.6931414355325408, test loss: 0.6931493869283358 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 712 training loss: 0.6931414172485075, test loss: 0.6931493895435701 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 713 training loss: 0.6931413988832557, test loss: 0.6931493921703946 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 714 training loss: 0.693141380436416, test loss: 0.693149394808865 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 715 training loss: 0.6931413619076177, test loss: 0.6931493974590363 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 716 training loss: 0.693141343296488, test loss: 0.6931494001209639 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 717 training loss: 0.6931413246026531, test loss: 0.693149402794704 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 718 training loss: 0.693141305825737, test loss: 0.6931494054803128 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 719 training loss: 0.6931412869653619, test loss: 0.6931494081778463 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 720 training loss: 0.6931412680211488, test loss: 0.6931494108873617 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 721 training loss: 0.6931412489927166, test loss: 0.6931494136089154 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 722 training loss: 0.6931412298796825, test loss: 0.693149416342565 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 723 training loss: 0.6931412106816622, test loss: 0.6931494190883677 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 724 training loss: 0.6931411913982696, test loss: 0.6931494218463813 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 725 training loss: 0.6931411720291166, test loss: 0.6931494246166637 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 726 training loss: 0.6931411525738137, test loss: 0.6931494273992732 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 727 training loss: 0.6931411330319693, test loss: 0.6931494301942683 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 728 training loss: 0.6931411134031905, test loss: 0.693149433001708 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 729 training loss: 0.6931410936870823, test loss: 0.693149435821651 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 730 training loss: 0.6931410738832477, test loss: 0.6931494386541569 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 731 training loss: 0.6931410539912886, test loss: 0.6931494414992851 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 732 training loss: 0.6931410340108044, test loss: 0.6931494443570956 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 733 training loss: 0.6931410139413929, test loss: 0.6931494472276486 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 734 training loss: 0.6931409937826507, test loss: 0.6931494501110045 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 735 training loss: 0.6931409735341713, test loss: 0.6931494530072237 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 736 training loss: 0.6931409531955479, test loss: 0.6931494559163678 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 737 training loss: 0.6931409327663705, test loss: 0.6931494588384974 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 738 training loss: 0.6931409122462281, test loss: 0.6931494617736746 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 739 training loss: 0.6931408916347074, test loss: 0.693149464721961 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 740 training loss: 0.6931408709313938, test loss: 0.6931494676834186 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 741 training loss: 0.6931408501358702, test loss: 0.6931494706581102 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 742 training loss: 0.6931408292477177, test loss: 0.693149473646098 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 743 training loss: 0.693140808266516, test loss: 0.6931494766474454 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 744 training loss: 0.6931407871918425, test loss: 0.6931494796622155 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 745 training loss: 0.6931407660232727, test loss: 0.6931494826904719 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 746 training loss: 0.6931407447603805, test loss: 0.6931494857322784 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 747 training loss: 0.6931407234027374, test loss: 0.6931494887876992 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 748 training loss: 0.6931407019499132, test loss: 0.6931494918567989 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 749 training loss: 0.6931406804014759, test loss: 0.693149494939642 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 750 training loss: 0.6931406587569915, test loss: 0.6931494980362936 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 751 training loss: 0.6931406370160239, test loss: 0.6931495011468191 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 752 training loss: 0.693140615178135, test loss: 0.6931495042712842 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 753 training loss: 0.693140593242885, test loss: 0.6931495074097547 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 754 training loss: 0.693140571209832, test loss: 0.693149510562297 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 755 training loss: 0.6931405490785317, test loss: 0.6931495137289776 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 756 training loss: 0.6931405268485386, test loss: 0.6931495169098633 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 757 training loss: 0.6931405045194043, test loss: 0.6931495201050215 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 758 training loss: 0.6931404820906792, test loss: 0.6931495233145195 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 759 training loss: 0.6931404595619112, test loss: 0.6931495265384251 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 760 training loss: 0.693140436932646, test loss: 0.6931495297768064 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 761 training loss: 0.6931404142024279, test loss: 0.6931495330297319 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 762 training loss: 0.6931403913707985, test loss: 0.6931495362972703 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 763 training loss: 0.6931403684372976, test loss: 0.6931495395794907 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 764 training loss: 0.693140345401463, test loss: 0.6931495428764625 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 765 training loss: 0.6931403222628303, test loss: 0.693149546188255 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 766 training loss: 0.6931402990209329, test loss: 0.6931495495149388 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 767 training loss: 0.6931402756753023, test loss: 0.6931495528565836 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 768 training loss: 0.6931402522254679, test loss: 0.6931495562132609 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 769 training loss: 0.6931402286709568, test loss: 0.6931495595850409 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 770 training loss: 0.6931402050112941, test loss: 0.6931495629719953 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 771 training loss: 0.6931401812460027, test loss: 0.6931495663741956 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 772 training loss: 0.6931401573746033, test loss: 0.6931495697917136 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 773 training loss: 0.6931401333966145, test loss: 0.693149573224622 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 774 training loss: 0.693140109311553, test loss: 0.6931495766729933 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 775 training loss: 0.6931400851189328, test loss: 0.6931495801369003 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 776 training loss: 0.6931400608182657, test loss: 0.6931495836164164 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 777 training loss: 0.6931400364090621, test loss: 0.6931495871116153 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 778 training loss: 0.6931400118908294, test loss: 0.693149590622571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 779 training loss: 0.6931399872630729, test loss: 0.6931495941493576 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 780 training loss: 0.6931399625252959, test loss: 0.6931495976920499 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 781 training loss: 0.6931399376769992, test loss: 0.6931496012507231 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 782 training loss: 0.6931399127176819, test loss: 0.693149604825452 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 783 training loss: 0.6931398876468401, test loss: 0.6931496084163129 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 784 training loss: 0.6931398624639681, test loss: 0.6931496120233817 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 785 training loss: 0.6931398371685576, test loss: 0.6931496156467345 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 786 training loss: 0.6931398117600986, test loss: 0.693149619286448 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 787 training loss: 0.693139786238078, test loss: 0.6931496229425999 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 788 training loss: 0.6931397606019809, test loss: 0.6931496266152671 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 789 training loss: 0.69313973485129, test loss: 0.6931496303045278 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 790 training loss: 0.6931397089854856, test loss: 0.6931496340104596 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 791 training loss: 0.6931396830040455, test loss: 0.6931496377331416 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 792 training loss: 0.6931396569064457, test loss: 0.6931496414726526 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 793 training loss: 0.6931396306921591, test loss: 0.6931496452290715 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 794 training loss: 0.6931396043606569, test loss: 0.6931496490024782 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 795 training loss: 0.6931395779114072, test loss: 0.6931496527929526 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 796 training loss: 0.6931395513438765, test loss: 0.693149656600575 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 797 training loss: 0.6931395246575281, test loss: 0.6931496604254261 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 798 training loss: 0.6931394978518237, test loss: 0.6931496642675873 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 799 training loss: 0.6931394709262217, test loss: 0.6931496681271396 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 800 training loss: 0.6931394438801789, test loss: 0.6931496720041651 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 801 training loss: 0.6931394167131489, test loss: 0.693149675898746 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 802 training loss: 0.6931393894245834, test loss: 0.6931496798109649 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 803 training loss: 0.6931393620139314, test loss: 0.6931496837409047 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 804 training loss: 0.6931393344806394, test loss: 0.6931496876886487 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 805 training loss: 0.6931393068241511, test loss: 0.6931496916542809 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 806 training loss: 0.6931392790439085, test loss: 0.6931496956378849 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 807 training loss: 0.6931392511393504, test loss: 0.6931496996395456 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 808 training loss: 0.6931392231099134, test loss: 0.693149703659348 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 809 training loss: 0.693139194955031, test loss: 0.6931497076973769 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 810 training loss: 0.693139166674135, test loss: 0.6931497117537185 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 811 training loss: 0.6931391382666541, test loss: 0.6931497158284585 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 812 training loss: 0.6931391097320144, test loss: 0.6931497199216835 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 813 training loss: 0.6931390810696397, test loss: 0.6931497240334804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 814 training loss: 0.6931390522789508, test loss: 0.6931497281639364 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 815 training loss: 0.6931390233593665, test loss: 0.693149732313139 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 816 training loss: 0.6931389943103022, test loss: 0.6931497364811765 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 817 training loss: 0.6931389651311715, test loss: 0.6931497406681373 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 818 training loss: 0.6931389358213846, test loss: 0.6931497448741104 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 819 training loss: 0.6931389063803494, test loss: 0.6931497490991847 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 820 training loss: 0.6931388768074711, test loss: 0.6931497533434503 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 821 training loss: 0.6931388471021522, test loss: 0.6931497576069968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 822 training loss: 0.6931388172637928, test loss: 0.6931497618899153 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 823 training loss: 0.6931387872917898, test loss: 0.6931497661922963 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 824 training loss: 0.6931387571855373, test loss: 0.6931497705142314 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 825 training loss: 0.6931387269444274, test loss: 0.6931497748558123 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 826 training loss: 0.6931386965678487, test loss: 0.6931497792171311 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 827 training loss: 0.6931386660551875, test loss: 0.6931497835982803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 828 training loss: 0.6931386354058272, test loss: 0.6931497879993533 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 829 training loss: 0.6931386046191482, test loss: 0.6931497924204431 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 830 training loss: 0.6931385736945287, test loss: 0.693149796861644 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 831 training loss: 0.6931385426313433, test loss: 0.6931498013230502 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 832 training loss: 0.6931385114289643, test loss: 0.6931498058047562 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 833 training loss: 0.6931384800867613, test loss: 0.6931498103068574 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 834 training loss: 0.6931384486041005, test loss: 0.6931498148294495 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 835 training loss: 0.6931384169803455, test loss: 0.6931498193726282 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 836 training loss: 0.6931383852148575, test loss: 0.6931498239364906 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 837 training loss: 0.693138353306994, test loss: 0.6931498285211332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 838 training loss: 0.6931383212561103, test loss: 0.6931498331266533 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 839 training loss: 0.6931382890615579, test loss: 0.6931498377531491 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 840 training loss: 0.6931382567226867, test loss: 0.6931498424007186 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 841 training loss: 0.6931382242388425, test loss: 0.6931498470694606 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 842 training loss: 0.6931381916093687, test loss: 0.6931498517594745 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 843 training loss: 0.6931381588336056, test loss: 0.6931498564708596 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 844 training loss: 0.6931381259108907, test loss: 0.693149861203716 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 845 training loss: 0.6931380928405582, test loss: 0.6931498659581447 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 846 training loss: 0.6931380596219392, test loss: 0.6931498707342462 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 847 training loss: 0.6931380262543625, test loss: 0.693149875532122 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 848 training loss: 0.693137992737153, test loss: 0.6931498803518745 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 849 training loss: 0.6931379590696335, test loss: 0.6931498851936058 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 850 training loss: 0.6931379252511226, test loss: 0.6931498900574188 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 851 training loss: 0.693137891280937, test loss: 0.6931498949434167 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 852 training loss: 0.6931378571583892, test loss: 0.6931498998517037 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 853 training loss: 0.6931378228827896, test loss: 0.6931499047823836 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 854 training loss: 0.6931377884534448, test loss: 0.6931499097355616 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 855 training loss: 0.6931377538696585, test loss: 0.6931499147113427 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 856 training loss: 0.6931377191307315, test loss: 0.6931499197098328 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 857 training loss: 0.6931376842359611, test loss: 0.6931499247311379 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 858 training loss: 0.6931376491846416, test loss: 0.693149929775365 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 859 training loss: 0.6931376139760638, test loss: 0.6931499348426211 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 860 training loss: 0.693137578609516, test loss: 0.693149939933014 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 861 training loss: 0.6931375430842824, test loss: 0.6931499450466515 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 862 training loss: 0.6931375073996449, test loss: 0.6931499501836427 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 863 training loss: 0.6931374715548814, test loss: 0.6931499553440966 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 864 training loss: 0.6931374355492671, test loss: 0.693149960528123 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 865 training loss: 0.6931373993820733, test loss: 0.6931499657358319 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 866 training loss: 0.6931373630525688, test loss: 0.693149970967334 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 867 training loss: 0.6931373265600183, test loss: 0.6931499762227405 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 868 training loss: 0.6931372899036838, test loss: 0.6931499815021632 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 869 training loss: 0.6931372530828238, test loss: 0.693149986805714 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 870 training loss: 0.6931372160966933, test loss: 0.693149992133506 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 871 training loss: 0.693137178944544, test loss: 0.6931499974856521 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 872 training loss: 0.6931371416256242, test loss: 0.6931500028622662 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 873 training loss: 0.6931371041391793, test loss: 0.6931500082634626 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 874 training loss: 0.6931370664844503, test loss: 0.693150013689356 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 875 training loss: 0.6931370286606758, test loss: 0.6931500191400616 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 876 training loss: 0.6931369906670903, test loss: 0.6931500246156954 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 877 training loss: 0.6931369525029252, test loss: 0.6931500301163738 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 878 training loss: 0.6931369141674083, test loss: 0.6931500356422134 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 879 training loss: 0.6931368756597639, test loss: 0.6931500411933322 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 880 training loss: 0.6931368369792126, test loss: 0.6931500467698474 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 881 training loss: 0.6931367981249721, test loss: 0.6931500523718781 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 882 training loss: 0.6931367590962558, test loss: 0.6931500579995432 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 883 training loss: 0.6931367198922744, test loss: 0.6931500636529622 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 884 training loss: 0.6931366805122344, test loss: 0.6931500693322553 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 885 training loss: 0.6931366409553384, test loss: 0.6931500750375432 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 886 training loss: 0.6931366012207868, test loss: 0.693150080768947 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 887 training loss: 0.693136561307775, test loss: 0.6931500865265887 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 888 training loss: 0.6931365212154953, test loss: 0.6931500923105905 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 889 training loss: 0.6931364809431362, test loss: 0.6931500981210754 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 890 training loss: 0.6931364404898831, test loss: 0.6931501039581669 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 891 training loss: 0.6931363998549172, test loss: 0.6931501098219887 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 892 training loss: 0.6931363590374158, test loss: 0.6931501157126657 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 893 training loss: 0.6931363180365531, test loss: 0.693150121630323 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 894 training loss: 0.6931362768514991, test loss: 0.6931501275750865 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 895 training loss: 0.6931362354814204, test loss: 0.6931501335470821 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 896 training loss: 0.6931361939254799, test loss: 0.693150139546437 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 897 training loss: 0.693136152182836, test loss: 0.6931501455732786 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 898 training loss: 0.6931361102526442, test loss: 0.6931501516277349 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 899 training loss: 0.6931360681340557, test loss: 0.6931501577099344 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 900 training loss: 0.693136025826218, test loss: 0.6931501638200065 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 901 training loss: 0.6931359833282749, test loss: 0.6931501699580811 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 902 training loss: 0.693135940639366, test loss: 0.6931501761242884 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 903 training loss: 0.6931358977586274, test loss: 0.6931501823187595 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 904 training loss: 0.693135854685191, test loss: 0.6931501885416257 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 905 training loss: 0.6931358114181849, test loss: 0.6931501947930195 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 906 training loss: 0.6931357679567333, test loss: 0.6931502010730737 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 907 training loss: 0.6931357242999565, test loss: 0.6931502073819216 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 908 training loss: 0.6931356804469708, test loss: 0.693150213719697 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 909 training loss: 0.6931356363968885, test loss: 0.6931502200865348 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 910 training loss: 0.6931355921488177, test loss: 0.6931502264825702 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 911 training loss: 0.6931355477018628, test loss: 0.6931502329079389 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 912 training loss: 0.6931355030551242, test loss: 0.6931502393627775 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 913 training loss: 0.6931354582076978, test loss: 0.6931502458472227 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 914 training loss: 0.693135413158676, test loss: 0.6931502523614128 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 915 training loss: 0.6931353679071467, test loss: 0.6931502589054857 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 916 training loss: 0.6931353224521938, test loss: 0.6931502654795805 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 917 training loss: 0.6931352767928971, test loss: 0.6931502720838368 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 918 training loss: 0.6931352309283322, test loss: 0.6931502787183949 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 919 training loss: 0.6931351848575706, test loss: 0.6931502853833956 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 920 training loss: 0.6931351385796797, test loss: 0.6931502920789803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 921 training loss: 0.6931350920937226, test loss: 0.6931502988052912 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 922 training loss: 0.6931350453987579, test loss: 0.6931503055624711 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 923 training loss: 0.6931349984938407, test loss: 0.6931503123506637 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 924 training loss: 0.6931349513780212, test loss: 0.6931503191700129 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 925 training loss: 0.6931349040503453, test loss: 0.6931503260206633 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 926 training loss: 0.6931348565098548, test loss: 0.6931503329027604 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 927 training loss: 0.6931348087555875, test loss: 0.6931503398164505 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 928 training loss: 0.6931347607865764, test loss: 0.6931503467618803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 929 training loss: 0.6931347126018503, test loss: 0.6931503537391968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 930 training loss: 0.6931346642004337, test loss: 0.6931503607485485 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 931 training loss: 0.6931346155813465, test loss: 0.6931503677900839 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 932 training loss: 0.6931345667436043, test loss: 0.6931503748639528 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 933 training loss: 0.6931345176862188, test loss: 0.6931503819703049 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 934 training loss: 0.6931344684081959, test loss: 0.6931503891092912 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 935 training loss: 0.6931344189085386, test loss: 0.6931503962810631 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 936 training loss: 0.6931343691862443, test loss: 0.693150403485773 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 937 training loss: 0.6931343192403062, test loss: 0.6931504107235732 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 938 training loss: 0.6931342690697135, test loss: 0.6931504179946177 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 939 training loss: 0.6931342186734498, test loss: 0.6931504252990607 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 940 training loss: 0.693134168050495, test loss: 0.693150432637057 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 941 training loss: 0.6931341171998243, test loss: 0.6931504400087624 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 942 training loss: 0.6931340661204077, test loss: 0.6931504474143332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 943 training loss: 0.6931340148112113, test loss: 0.6931504548539263 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 944 training loss: 0.6931339632711957, test loss: 0.6931504623277 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 945 training loss: 0.693133911499318, test loss: 0.6931504698358122 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 946 training loss: 0.6931338594945295, test loss: 0.6931504773784224 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 947 training loss: 0.6931338072557774, test loss: 0.6931504849556903 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 948 training loss: 0.6931337547820039, test loss: 0.6931504925677772 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 949 training loss: 0.6931337020721464, test loss: 0.6931505002148436 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 950 training loss: 0.693133649125138, test loss: 0.6931505078970522 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 951 training loss: 0.6931335959399062, test loss: 0.6931505156145659 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 952 training loss: 0.6931335425153743, test loss: 0.693150523367548 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 953 training loss: 0.6931334888504607, test loss: 0.6931505311561631 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 954 training loss: 0.6931334349440785, test loss: 0.6931505389805761 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 955 training loss: 0.6931333807951364, test loss: 0.6931505468409531 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 956 training loss: 0.693133326402538, test loss: 0.6931505547374602 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 957 training loss: 0.6931332717651818, test loss: 0.6931505626702652 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 958 training loss: 0.6931332168819617, test loss: 0.6931505706395361 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 959 training loss: 0.6931331617517663, test loss: 0.6931505786454417 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 960 training loss: 0.6931331063734795, test loss: 0.6931505866881517 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 961 training loss: 0.6931330507459798, test loss: 0.6931505947678362 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 962 training loss: 0.6931329948681408, test loss: 0.693150602884667 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 963 training loss: 0.6931329387388314, test loss: 0.6931506110388154 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 964 training loss: 0.6931328823569147, test loss: 0.6931506192304544 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 965 training loss: 0.6931328257212495, test loss: 0.6931506274597576 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 966 training loss: 0.6931327688306888, test loss: 0.6931506357268992 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 967 training loss: 0.6931327116840807, test loss: 0.6931506440320543 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 968 training loss: 0.6931326542802683, test loss: 0.6931506523753987 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 969 training loss: 0.6931325966180891, test loss: 0.6931506607571092 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 970 training loss: 0.6931325386963758, test loss: 0.6931506691773632 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 971 training loss: 0.6931324805139555, test loss: 0.693150677636339 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 972 training loss: 0.6931324220696502, test loss: 0.6931506861342157 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 973 training loss: 0.6931323633622765, test loss: 0.6931506946711732 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 974 training loss: 0.6931323043906457, test loss: 0.693150703247392 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 975 training loss: 0.6931322451535639, test loss: 0.6931507118630541 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 976 training loss: 0.6931321856498318, test loss: 0.6931507205183414 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 977 training loss: 0.6931321258782445, test loss: 0.6931507292134373 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 978 training loss: 0.6931320658375919, test loss: 0.6931507379485256 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 979 training loss: 0.6931320055266583, test loss: 0.6931507467237914 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 980 training loss: 0.6931319449442227, test loss: 0.6931507555394201 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 981 training loss: 0.6931318840890585, test loss: 0.6931507643955985 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 982 training loss: 0.6931318229599336, test loss: 0.6931507732925138 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 983 training loss: 0.6931317615556103, test loss: 0.6931507822303542 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 984 training loss: 0.6931316998748455, test loss: 0.6931507912093087 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 985 training loss: 0.6931316379163904, test loss: 0.6931508002295674 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 986 training loss: 0.6931315756789906, test loss: 0.6931508092913208 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 987 training loss: 0.693131513161386, test loss: 0.6931508183947608 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 988 training loss: 0.693131450362311, test loss: 0.6931508275400796 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 989 training loss: 0.6931313872804943, test loss: 0.6931508367274708 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 990 training loss: 0.6931313239146586, test loss: 0.6931508459571286 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 991 training loss: 0.6931312602635211, test loss: 0.6931508552292479 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 992 training loss: 0.6931311963257933, test loss: 0.693150864544025 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 993 training loss: 0.6931311321001811, test loss: 0.6931508739016566 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 994 training loss: 0.6931310675853837, test loss: 0.6931508833023406 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 995 training loss: 0.6931310027800953, test loss: 0.6931508927462756 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 996 training loss: 0.693130937683004, test loss: 0.6931509022336609 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 997 training loss: 0.693130872292792, test loss: 0.6931509117646973 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 998 training loss: 0.6931308066081358, test loss: 0.6931509213395862 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 999 training loss: 0.6931307406277052, test loss: 0.6931509309585296 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1000 training loss: 0.693130674350165, test loss: 0.6931509406217308 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1001 training loss: 0.6931306077741735, test loss: 0.693150950329394 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1002 training loss: 0.6931305408983828, test loss: 0.6931509600817242 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1003 training loss: 0.6931304737214393, test loss: 0.6931509698789272 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1004 training loss: 0.6931304062419833, test loss: 0.6931509797212098 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1005 training loss: 0.6931303384586487, test loss: 0.6931509896087801 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1006 training loss: 0.6931302703700636, test loss: 0.6931509995418468 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1007 training loss: 0.6931302019748498, test loss: 0.6931510095206195 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1008 training loss: 0.6931301332716227, test loss: 0.6931510195453088 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1009 training loss: 0.693130064258992, test loss: 0.6931510296161263 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1010 training loss: 0.6931299949355605, test loss: 0.6931510397332845 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1011 training loss: 0.6931299252999253, test loss: 0.693151049896997 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1012 training loss: 0.6931298553506768, test loss: 0.6931510601074783 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1013 training loss: 0.6931297850863997, test loss: 0.6931510703649438 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1014 training loss: 0.6931297145056714, test loss: 0.6931510806696096 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1015 training loss: 0.6931296436070634, test loss: 0.6931510910216935 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1016 training loss: 0.6931295723891411, test loss: 0.6931511014214135 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1017 training loss: 0.693129500850463, test loss: 0.6931511118689891 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1018 training loss: 0.6931294289895812, test loss: 0.6931511223646406 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1019 training loss: 0.6931293568050415, test loss: 0.6931511329085893 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1020 training loss: 0.693129284295383, test loss: 0.6931511435010573 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1021 training loss: 0.6931292114591384, test loss: 0.6931511541422684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1022 training loss: 0.6931291382948337, test loss: 0.6931511648324465 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1023 training loss: 0.6931290648009881, test loss: 0.693151175571817 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1024 training loss: 0.6931289909761147, test loss: 0.6931511863606061 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1025 training loss: 0.6931289168187195, test loss: 0.6931511971990415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1026 training loss: 0.6931288423273014, test loss: 0.6931512080873515 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1027 training loss: 0.6931287675003537, test loss: 0.6931512190257652 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1028 training loss: 0.6931286923363621, test loss: 0.6931512300145132 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1029 training loss: 0.6931286168338056, test loss: 0.693151241053827 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1030 training loss: 0.6931285409911566, test loss: 0.6931512521439391 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1031 training loss: 0.6931284648068803, test loss: 0.693151263285083 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1032 training loss: 0.6931283882794355, test loss: 0.6931512744774935 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1033 training loss: 0.6931283114072737, test loss: 0.6931512857214059 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1034 training loss: 0.6931282341888396, test loss: 0.6931512970170572 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1035 training loss: 0.6931281566225709, test loss: 0.6931513083646851 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1036 training loss: 0.6931280787068984, test loss: 0.6931513197645285 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1037 training loss: 0.6931280004402457, test loss: 0.6931513312168271 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1038 training loss: 0.6931279218210293, test loss: 0.6931513427218221 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1039 training loss: 0.693127842847659, test loss: 0.6931513542797556 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1040 training loss: 0.6931277635185369, test loss: 0.6931513658908705 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1041 training loss: 0.6931276838320584, test loss: 0.6931513775554115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1042 training loss: 0.6931276037866112, test loss: 0.6931513892736235 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1043 training loss: 0.6931275233805764, test loss: 0.6931514010457533 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1044 training loss: 0.6931274426123274, test loss: 0.6931514128720483 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1045 training loss: 0.6931273614802306, test loss: 0.6931514247527569 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1046 training loss: 0.6931272799826447, test loss: 0.6931514366881296 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1047 training loss: 0.6931271981179212, test loss: 0.6931514486784167 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1048 training loss: 0.6931271158844048, test loss: 0.6931514607238702 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1049 training loss: 0.6931270332804318, test loss: 0.6931514728247437 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1050 training loss: 0.6931269503043315, test loss: 0.6931514849812911 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1051 training loss: 0.6931268669544258, test loss: 0.693151497193768 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1052 training loss: 0.6931267832290292, test loss: 0.6931515094624311 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1053 training loss: 0.6931266991264484, test loss: 0.6931515217875379 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1054 training loss: 0.6931266146449824, test loss: 0.6931515341693473 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1055 training loss: 0.6931265297829228, test loss: 0.6931515466081196 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1056 training loss: 0.6931264445385534, test loss: 0.6931515591041156 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1057 training loss: 0.6931263589101507, test loss: 0.6931515716575981 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1058 training loss: 0.693126272895983, test loss: 0.6931515842688303 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1059 training loss: 0.693126186494311, test loss: 0.6931515969380772 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1060 training loss: 0.6931260997033879, test loss: 0.6931516096656045 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1061 training loss: 0.6931260125214584, test loss: 0.6931516224516795 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1062 training loss: 0.6931259249467601, test loss: 0.6931516352965703 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1063 training loss: 0.6931258369775224, test loss: 0.6931516482005465 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1064 training loss: 0.6931257486119665, test loss: 0.693151661163879 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1065 training loss: 0.6931256598483062, test loss: 0.6931516741868394 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1066 training loss: 0.6931255706847467, test loss: 0.693151687269701 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1067 training loss: 0.6931254811194856, test loss: 0.6931517004127382 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1068 training loss: 0.6931253911507121, test loss: 0.6931517136162264 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1069 training loss: 0.6931253007766078, test loss: 0.6931517268804425 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1070 training loss: 0.6931252099953458, test loss: 0.6931517402056646 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1071 training loss: 0.6931251188050909, test loss: 0.693151753592172 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1072 training loss: 0.6931250272039999, test loss: 0.6931517670402453 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1073 training loss: 0.6931249351902213, test loss: 0.693151780550166 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1074 training loss: 0.6931248427618957, test loss: 0.6931517941222174 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1075 training loss: 0.6931247499171543, test loss: 0.693151807756684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1076 training loss: 0.6931246566541214, test loss: 0.6931518214538509 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1077 training loss: 0.6931245629709115, test loss: 0.6931518352140051 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1078 training loss: 0.693124468865632, test loss: 0.6931518490374352 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1079 training loss: 0.6931243743363805, test loss: 0.6931518629244303 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1080 training loss: 0.6931242793812473, test loss: 0.6931518768752808 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1081 training loss: 0.6931241839983131, test loss: 0.6931518908902792 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1082 training loss: 0.6931240881856511, test loss: 0.6931519049697186 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1083 training loss: 0.6931239919413249, test loss: 0.6931519191138937 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1084 training loss: 0.69312389526339, test loss: 0.6931519333231004 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1085 training loss: 0.6931237981498931, test loss: 0.6931519475976361 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1086 training loss: 0.6931237005988721, test loss: 0.693151961937799 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1087 training loss: 0.6931236026083561, test loss: 0.6931519763438896 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1088 training loss: 0.6931235041763655, test loss: 0.6931519908162086 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1089 training loss: 0.6931234053009119, test loss: 0.6931520053550587 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1090 training loss: 0.6931233059799977, test loss: 0.6931520199607443 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1091 training loss: 0.6931232062116168, test loss: 0.6931520346335701 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1092 training loss: 0.6931231059937539, test loss: 0.6931520493738432 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1093 training loss: 0.6931230053243848, test loss: 0.6931520641818714 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1094 training loss: 0.6931229042014758, test loss: 0.6931520790579639 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1095 training loss: 0.693122802622985, test loss: 0.6931520940024322 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1096 training loss: 0.6931227005868604, test loss: 0.6931521090155877 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1097 training loss: 0.6931225980910416, test loss: 0.6931521240977442 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1098 training loss: 0.6931224951334586, test loss: 0.6931521392492166 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1099 training loss: 0.6931223917120324, test loss: 0.6931521544703214 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1100 training loss: 0.6931222878246743, test loss: 0.6931521697613762 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1101 training loss: 0.6931221834692869, test loss: 0.6931521851227002 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1102 training loss: 0.6931220786437626, test loss: 0.6931522005546138 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1103 training loss: 0.6931219733459854, test loss: 0.6931522160574393 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1104 training loss: 0.6931218675738291, test loss: 0.6931522316315001 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1105 training loss: 0.6931217613251579, test loss: 0.6931522472771209 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1106 training loss: 0.6931216545978274, test loss: 0.6931522629946282 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1107 training loss: 0.6931215473896825, test loss: 0.6931522787843494 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1108 training loss: 0.6931214396985592, test loss: 0.6931522946466142 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1109 training loss: 0.6931213315222838, test loss: 0.6931523105817529 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1110 training loss: 0.6931212228586726, test loss: 0.6931523265900978 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1111 training loss: 0.6931211137055322, test loss: 0.6931523426719827 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1112 training loss: 0.6931210040606598, test loss: 0.6931523588277423 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1113 training loss: 0.6931208939218421, test loss: 0.6931523750577137 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1114 training loss: 0.6931207832868564, test loss: 0.6931523913622345 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1115 training loss: 0.6931206721534705, test loss: 0.6931524077416444 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1116 training loss: 0.693120560519441, test loss: 0.693152424196285 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1117 training loss: 0.6931204483825159, test loss: 0.6931524407264982 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1118 training loss: 0.693120335740432, test loss: 0.6931524573326283 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1119 training loss: 0.6931202225909167, test loss: 0.6931524740150213 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1120 training loss: 0.6931201089316871, test loss: 0.693152490774024 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1121 training loss: 0.6931199947604498, test loss: 0.6931525076099854 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1122 training loss: 0.6931198800749018, test loss: 0.6931525245232556 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1123 training loss: 0.6931197648727294, test loss: 0.6931525415141861 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1124 training loss: 0.6931196491516086, test loss: 0.6931525585831311 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1125 training loss: 0.6931195329092052, test loss: 0.6931525757304446 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1126 training loss: 0.6931194161431745, test loss: 0.6931525929564838 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1127 training loss: 0.6931192988511613, test loss: 0.6931526102616064 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1128 training loss: 0.6931191810308001, test loss: 0.6931526276461722 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1129 training loss: 0.6931190626797146, test loss: 0.6931526451105426 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1130 training loss: 0.6931189437955181, test loss: 0.6931526626550801 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1131 training loss: 0.6931188243758132, test loss: 0.6931526802801495 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1132 training loss: 0.6931187044181919, test loss: 0.6931526979861169 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1133 training loss: 0.6931185839202354, test loss: 0.6931527157733497 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1134 training loss: 0.693118462879514, test loss: 0.6931527336422174 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1135 training loss: 0.6931183412935872, test loss: 0.693152751593091 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1136 training loss: 0.6931182191600042, test loss: 0.6931527696263426 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1137 training loss: 0.6931180964763024, test loss: 0.6931527877423473 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1138 training loss: 0.693117973240009, test loss: 0.6931528059414804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1139 training loss: 0.6931178494486396, test loss: 0.6931528242241196 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1140 training loss: 0.693117725099699, test loss: 0.6931528425906439 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1141 training loss: 0.6931176001906811, test loss: 0.6931528610414345 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1142 training loss: 0.6931174747190683, test loss: 0.693152879576874 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1143 training loss: 0.693117348682332, test loss: 0.6931528981973464 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1144 training loss: 0.6931172220779321, test loss: 0.6931529169032378 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1145 training loss: 0.6931170949033175, test loss: 0.6931529356949355 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1146 training loss: 0.6931169671559257, test loss: 0.6931529545728293 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1147 training loss: 0.6931168388331829, test loss: 0.6931529735373103 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1148 training loss: 0.6931167099325031, test loss: 0.6931529925887712 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1149 training loss: 0.6931165804512899, test loss: 0.6931530117276065 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1150 training loss: 0.6931164503869346, test loss: 0.6931530309542122 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1151 training loss: 0.6931163197368171, test loss: 0.6931530502689867 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1152 training loss: 0.6931161884983058, test loss: 0.6931530696723298 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1153 training loss: 0.6931160566687573, test loss: 0.6931530891646426 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1154 training loss: 0.6931159242455164, test loss: 0.6931531087463288 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1155 training loss: 0.6931157912259159, test loss: 0.6931531284177935 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1156 training loss: 0.6931156576072773, test loss: 0.6931531481794433 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1157 training loss: 0.6931155233869095, test loss: 0.6931531680316869 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1158 training loss: 0.6931153885621101, test loss: 0.693153187974935 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1159 training loss: 0.6931152531301644, test loss: 0.6931532080095993 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1160 training loss: 0.6931151170883455, test loss: 0.6931532281360944 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1161 training loss: 0.6931149804339144, test loss: 0.693153248354836 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1162 training loss: 0.6931148431641202, test loss: 0.6931532686662418 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1163 training loss: 0.6931147052761997, test loss: 0.6931532890707313 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1164 training loss: 0.6931145667673771, test loss: 0.6931533095687259 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1165 training loss: 0.6931144276348647, test loss: 0.6931533301606488 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1166 training loss: 0.6931142878758623, test loss: 0.6931533508469252 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1167 training loss: 0.6931141474875572, test loss: 0.693153371627982 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1168 training loss: 0.6931140064671241, test loss: 0.6931533925042481 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1169 training loss: 0.6931138648117252, test loss: 0.693153413476154 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1170 training loss: 0.6931137225185106, test loss: 0.6931534345441327 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1171 training loss: 0.6931135795846167, test loss: 0.6931534557086184 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1172 training loss: 0.6931134360071685, test loss: 0.6931534769700475 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1173 training loss: 0.6931132917832771, test loss: 0.6931534983288584 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1174 training loss: 0.6931131469100414, test loss: 0.6931535197854911 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1175 training loss: 0.6931130013845473, test loss: 0.6931535413403884 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1176 training loss: 0.6931128552038678, test loss: 0.6931535629939938 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1177 training loss: 0.6931127083650628, test loss: 0.6931535847467537 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1178 training loss: 0.6931125608651793, test loss: 0.693153606599116 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1179 training loss: 0.6931124127012509, test loss: 0.6931536285515305 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1180 training loss: 0.6931122638702986, test loss: 0.6931536506044493 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1181 training loss: 0.6931121143693295, test loss: 0.6931536727583264 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1182 training loss: 0.6931119641953382, test loss: 0.6931536950136176 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1183 training loss: 0.6931118133453052, test loss: 0.6931537173707807 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1184 training loss: 0.6931116618161984, test loss: 0.6931537398302756 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1185 training loss: 0.6931115096049717, test loss: 0.6931537623925643 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1186 training loss: 0.6931113567085654, test loss: 0.693153785058111 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1187 training loss: 0.6931112031239068, test loss: 0.6931538078273811 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1188 training loss: 0.6931110488479092, test loss: 0.6931538307008428 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1189 training loss: 0.6931108938774723, test loss: 0.6931538536789663 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1190 training loss: 0.6931107382094821, test loss: 0.6931538767622237 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1191 training loss: 0.6931105818408109, test loss: 0.6931538999510888 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1192 training loss: 0.6931104247683167, test loss: 0.6931539232460382 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1193 training loss: 0.6931102669888441, test loss: 0.69315394664755 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1194 training loss: 0.6931101084992238, test loss: 0.6931539701561045 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1195 training loss: 0.6931099492962719, test loss: 0.6931539937721843 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1196 training loss: 0.6931097893767909, test loss: 0.693154017496274 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1197 training loss: 0.6931096287375689, test loss: 0.6931540413288603 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1198 training loss: 0.6931094673753799, test loss: 0.6931540652704319 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1199 training loss: 0.6931093052869837, test loss: 0.6931540893214799 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1200 training loss: 0.6931091424691254, test loss: 0.6931541134824971 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1201 training loss: 0.6931089789185361, test loss: 0.6931541377539792 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1202 training loss: 0.6931088146319322, test loss: 0.6931541621364228 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1203 training loss: 0.6931086496060157, test loss: 0.6931541866303281 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1204 training loss: 0.6931084838374743, test loss: 0.6931542112361968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1205 training loss: 0.6931083173229802, test loss: 0.6931542359545326 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1206 training loss: 0.6931081500591918, test loss: 0.6931542607858415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1207 training loss: 0.6931079820427523, test loss: 0.6931542857306321 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1208 training loss: 0.69310781327029, test loss: 0.6931543107894147 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1209 training loss: 0.6931076437384185, test loss: 0.693154335962702 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1210 training loss: 0.6931074734437364, test loss: 0.6931543612510092 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1211 training loss: 0.6931073023828269, test loss: 0.693154386654853 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1212 training loss: 0.6931071305522589, test loss: 0.6931544121747534 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1213 training loss: 0.6931069579485853, test loss: 0.6931544378112315 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1214 training loss: 0.6931067845683444, test loss: 0.6931544635648119 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1215 training loss: 0.6931066104080587, test loss: 0.6931544894360203 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1216 training loss: 0.6931064354642356, test loss: 0.6931545154253854 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1217 training loss: 0.6931062597333673, test loss: 0.6931545415334381 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1218 training loss: 0.6931060832119301, test loss: 0.6931545677607113 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1219 training loss: 0.6931059058963851, test loss: 0.6931545941077405 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1220 training loss: 0.6931057277831775, test loss: 0.6931546205750635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1221 training loss: 0.6931055488687367, test loss: 0.6931546471632203 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1222 training loss: 0.693105369149477, test loss: 0.6931546738727533 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1223 training loss: 0.6931051886217959, test loss: 0.6931547007042071 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1224 training loss: 0.6931050072820762, test loss: 0.693154727658129 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1225 training loss: 0.6931048251266837, test loss: 0.6931547547350686 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1226 training loss: 0.6931046421519685, test loss: 0.6931547819355773 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1227 training loss: 0.693104458354265, test loss: 0.6931548092602097 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1228 training loss: 0.693104273729891, test loss: 0.6931548367095224 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1229 training loss: 0.6931040882751479, test loss: 0.6931548642840742 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1230 training loss: 0.6931039019863213, test loss: 0.6931548919844268 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1231 training loss: 0.6931037148596804, test loss: 0.6931549198111439 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1232 training loss: 0.6931035268914774, test loss: 0.693154947764792 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1233 training loss: 0.6931033380779485, test loss: 0.6931549758459397 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1234 training loss: 0.6931031484153132, test loss: 0.6931550040551585 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1235 training loss: 0.6931029578997742, test loss: 0.6931550323930217 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1236 training loss: 0.6931027665275177, test loss: 0.6931550608601054 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1237 training loss: 0.693102574294713, test loss: 0.6931550894569886 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1238 training loss: 0.6931023811975123, test loss: 0.6931551181842522 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1239 training loss: 0.6931021872320514, test loss: 0.69315514704248 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1240 training loss: 0.6931019923944487, test loss: 0.6931551760322581 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1241 training loss: 0.6931017966808054, test loss: 0.693155205154175 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1242 training loss: 0.6931016000872059, test loss: 0.693155234408822 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1243 training loss: 0.6931014026097171, test loss: 0.6931552637967929 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1244 training loss: 0.6931012042443886, test loss: 0.693155293318684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1245 training loss: 0.6931010049872532, test loss: 0.6931553229750942 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1246 training loss: 0.6931008048343252, test loss: 0.693155352766625 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1247 training loss: 0.6931006037816022, test loss: 0.6931553826938803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1248 training loss: 0.6931004018250638, test loss: 0.6931554127574667 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1249 training loss: 0.6931001989606723, test loss: 0.6931554429579936 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1250 training loss: 0.6930999951843718, test loss: 0.6931554732960726 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1251 training loss: 0.6930997904920889, test loss: 0.6931555037723186 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1252 training loss: 0.693099584879732, test loss: 0.6931555343873483 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1253 training loss: 0.693099378343192, test loss: 0.6931555651417818 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1254 training loss: 0.6930991708783414, test loss: 0.6931555960362411 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1255 training loss: 0.6930989624810343, test loss: 0.6931556270713517 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1256 training loss: 0.6930987531471074, test loss: 0.6931556582477415 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1257 training loss: 0.6930985428723783, test loss: 0.6931556895660405 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1258 training loss: 0.6930983316526464, test loss: 0.6931557210268822 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1259 training loss: 0.6930981194836934, test loss: 0.6931557526309021 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1260 training loss: 0.6930979063612814, test loss: 0.6931557843787394 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1261 training loss: 0.6930976922811547, test loss: 0.6931558162710347 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1262 training loss: 0.6930974772390384, test loss: 0.6931558483084329 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1263 training loss: 0.6930972612306393, test loss: 0.6931558804915804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1264 training loss: 0.693097044251645, test loss: 0.6931559128211265 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1265 training loss: 0.6930968262977244, test loss: 0.6931559452977242 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1266 training loss: 0.6930966073645273, test loss: 0.6931559779220281 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1267 training loss: 0.6930963874476845, test loss: 0.6931560106946968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1268 training loss: 0.6930961665428076, test loss: 0.6931560436163906 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1269 training loss: 0.693095944645489, test loss: 0.6931560766877731 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1270 training loss: 0.6930957217513016, test loss: 0.6931561099095109 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1271 training loss: 0.6930954978557992, test loss: 0.6931561432822735 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1272 training loss: 0.693095272954516, test loss: 0.6931561768067327 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1273 training loss: 0.6930950470429665, test loss: 0.6931562104835635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1274 training loss: 0.6930948201166458, test loss: 0.6931562443134437 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1275 training loss: 0.693094592171029, test loss: 0.6931562782970544 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1276 training loss: 0.6930943632015715, test loss: 0.693156312435079 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1277 training loss: 0.693094133203709, test loss: 0.693156346728204 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1278 training loss: 0.693093902172857, test loss: 0.6931563811771193 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1279 training loss: 0.6930936701044108, test loss: 0.693156415782517 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1280 training loss: 0.693093436993746, test loss: 0.6931564505450925 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1281 training loss: 0.6930932028362174, test loss: 0.6931564854655441 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1282 training loss: 0.6930929676271601, test loss: 0.6931565205445732 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1283 training loss: 0.6930927313618883, test loss: 0.6931565557828842 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1284 training loss: 0.693092494035696, test loss: 0.6931565911811841 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1285 training loss: 0.6930922556438563, test loss: 0.6931566267401834 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1286 training loss: 0.6930920161816221, test loss: 0.6931566624605953 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1287 training loss: 0.6930917756442252, test loss: 0.6931566983431361 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1288 training loss: 0.6930915340268766, test loss: 0.6931567343885251 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1289 training loss: 0.6930912913247667, test loss: 0.6931567705974847 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1290 training loss: 0.693091047533064, test loss: 0.6931568069707406 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1291 training loss: 0.6930908026469174, test loss: 0.6931568435090211 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1292 training loss: 0.6930905566614532, test loss: 0.6931568802130581 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1293 training loss: 0.6930903095717771, test loss: 0.693156917083586 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1294 training loss: 0.6930900613729732, test loss: 0.6931569541213426 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1295 training loss: 0.6930898120601044, test loss: 0.6931569913270692 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1296 training loss: 0.6930895616282119, test loss: 0.6931570287015097 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1297 training loss: 0.6930893100723151, test loss: 0.6931570662454115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1298 training loss: 0.6930890573874119, test loss: 0.6931571039595249 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1299 training loss: 0.6930888035684784, test loss: 0.6931571418446032 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1300 training loss: 0.6930885486104685, test loss: 0.6931571799014035 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1301 training loss: 0.6930882925083146, test loss: 0.6931572181306856 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1302 training loss: 0.6930880352569265, test loss: 0.6931572565332127 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1303 training loss: 0.6930877768511922, test loss: 0.6931572951097511 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1304 training loss: 0.693087517285977, test loss: 0.6931573338610706 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1305 training loss: 0.6930872565561242, test loss: 0.6931573727879438 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1306 training loss: 0.6930869946564546, test loss: 0.6931574118911469 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1307 training loss: 0.6930867315817664, test loss: 0.6931574511714592 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1308 training loss: 0.6930864673268349, test loss: 0.6931574906296634 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1309 training loss: 0.693086201886413, test loss: 0.6931575302665456 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1310 training loss: 0.6930859352552305, test loss: 0.6931575700828948 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1311 training loss: 0.6930856674279946, test loss: 0.6931576100795038 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1312 training loss: 0.6930853983993889, test loss: 0.6931576502571684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1313 training loss: 0.6930851281640746, test loss: 0.6931576906166879 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1314 training loss: 0.6930848567166891, test loss: 0.6931577311588647 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1315 training loss: 0.6930845840518467, test loss: 0.693157771884505 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1316 training loss: 0.6930843101641383, test loss: 0.6931578127944182 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1317 training loss: 0.693084035048131, test loss: 0.6931578538894168 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1318 training loss: 0.693083758698369, test loss: 0.6931578951703171 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1319 training loss: 0.693083481109372, test loss: 0.6931579366379388 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1320 training loss: 0.6930832022756362, test loss: 0.693157978293105 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1321 training loss: 0.693082922191634, test loss: 0.6931580201366417 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1322 training loss: 0.6930826408518136, test loss: 0.6931580621693791 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1323 training loss: 0.6930823582505993, test loss: 0.693158104392151 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1324 training loss: 0.6930820743823911, test loss: 0.6931581468057936 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1325 training loss: 0.6930817892415645, test loss: 0.6931581894111478 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1326 training loss: 0.6930815028224709, test loss: 0.6931582322090571 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1327 training loss: 0.6930812151194369, test loss: 0.6931582752003692 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1328 training loss: 0.6930809261267649, test loss: 0.6931583183859351 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1329 training loss: 0.6930806358387323, test loss: 0.693158361766609 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1330 training loss: 0.6930803442495913, test loss: 0.6931584053432494 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1331 training loss: 0.69308005135357, test loss: 0.6931584491167178 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1332 training loss: 0.693079757144871, test loss: 0.6931584930878795 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1333 training loss: 0.6930794616176719, test loss: 0.6931585372576029 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1334 training loss: 0.6930791647661249, test loss: 0.6931585816267614 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1335 training loss: 0.693078866584357, test loss: 0.6931586261962303 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1336 training loss: 0.6930785670664699, test loss: 0.6931586709668898 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1337 training loss: 0.6930782662065393, test loss: 0.6931587159396232 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1338 training loss: 0.6930779639986159, test loss: 0.6931587611153175 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1339 training loss: 0.6930776604367244, test loss: 0.6931588064948637 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1340 training loss: 0.6930773555148632, test loss: 0.6931588520791561 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1341 training loss: 0.6930770492270052, test loss: 0.6931588978690929 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1342 training loss: 0.6930767415670972, test loss: 0.6931589438655763 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1343 training loss: 0.6930764325290597, test loss: 0.6931589900695118 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1344 training loss: 0.693076122106787, test loss: 0.6931590364818087 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1345 training loss: 0.6930758102941471, test loss: 0.6931590831033804 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1346 training loss: 0.6930754970849811, test loss: 0.6931591299351438 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1347 training loss: 0.6930751824731037, test loss: 0.6931591769780198 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1348 training loss: 0.6930748664523032, test loss: 0.6931592242329329 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1349 training loss: 0.6930745490163407, test loss: 0.6931592717008115 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1350 training loss: 0.6930742301589504, test loss: 0.6931593193825878 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1351 training loss: 0.6930739098738394, test loss: 0.6931593672791982 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1352 training loss: 0.693073588154688, test loss: 0.6931594153915824 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1353 training loss: 0.6930732649951489, test loss: 0.6931594637206846 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1354 training loss: 0.693072940388847, test loss: 0.6931595122674522 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1355 training loss: 0.6930726143293809, test loss: 0.693159561032837 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1356 training loss: 0.6930722868103206, test loss: 0.6931596100177949 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1357 training loss: 0.6930719578252086, test loss: 0.6931596592232852 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1358 training loss: 0.6930716273675596, test loss: 0.6931597086502712 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1359 training loss: 0.6930712954308603, test loss: 0.6931597582997208 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1360 training loss: 0.6930709620085694, test loss: 0.6931598081726051 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1361 training loss: 0.6930706270941176, test loss: 0.6931598582699 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1362 training loss: 0.6930702906809071, test loss: 0.6931599085925845 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1363 training loss: 0.6930699527623116, test loss: 0.6931599591416422 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1364 training loss: 0.6930696133316765, test loss: 0.6931600099180609 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1365 training loss: 0.6930692723823181, test loss: 0.693160060922832 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1366 training loss: 0.6930689299075249, test loss: 0.6931601121569512 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1367 training loss: 0.6930685859005555, test loss: 0.6931601636214181 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1368 training loss: 0.6930682403546401, test loss: 0.6931602153172368 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1369 training loss: 0.6930678932629795, test loss: 0.6931602672454151 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1370 training loss: 0.6930675446187453, test loss: 0.6931603194069654 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1371 training loss: 0.69306719441508, test loss: 0.6931603718029035 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1372 training loss: 0.6930668426450965, test loss: 0.6931604244342501 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1373 training loss: 0.6930664893018781, test loss: 0.6931604773020297 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1374 training loss: 0.6930661343784783, test loss: 0.6931605304072709 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1375 training loss: 0.6930657778679208, test loss: 0.6931605837510071 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1376 training loss: 0.6930654197631996, test loss: 0.6931606373342749 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1377 training loss: 0.6930650600572782, test loss: 0.6931606911581164 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1378 training loss: 0.6930646987430902, test loss: 0.6931607452235771 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1379 training loss: 0.693064335813539, test loss: 0.6931607995317066 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1380 training loss: 0.6930639712614972, test loss: 0.6931608540835592 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1381 training loss: 0.6930636050798072, test loss: 0.6931609088801937 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1382 training loss: 0.6930632372612805, test loss: 0.6931609639226727 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1383 training loss: 0.6930628677986979, test loss: 0.6931610192120635 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1384 training loss: 0.6930624966848091, test loss: 0.6931610747494377 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1385 training loss: 0.6930621239123328, test loss: 0.6931611305358709 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1386 training loss: 0.6930617494739567, test loss: 0.6931611865724436 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1387 training loss: 0.6930613733623373, test loss: 0.6931612428602403 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1388 training loss: 0.6930609955700991, test loss: 0.6931612994003498 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1389 training loss: 0.6930606160898357, test loss: 0.6931613561938659 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1390 training loss: 0.6930602349141082, test loss: 0.6931614132418862 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1391 training loss: 0.693059852035447, test loss: 0.6931614705455132 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1392 training loss: 0.6930594674463495, test loss: 0.6931615281058539 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1393 training loss: 0.6930590811392815, test loss: 0.6931615859240192 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1394 training loss: 0.693058693106677, test loss: 0.6931616440011251 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1395 training loss: 0.6930583033409367, test loss: 0.6931617023382917 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1396 training loss: 0.6930579118344297, test loss: 0.693161760936644 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1397 training loss: 0.6930575185794922, test loss: 0.6931618197973114 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1398 training loss: 0.6930571235684276, test loss: 0.6931618789214278 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1399 training loss: 0.6930567267935065, test loss: 0.6931619383101315 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1400 training loss: 0.6930563282469666, test loss: 0.6931619979645657 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1401 training loss: 0.6930559279210123, test loss: 0.6931620578858781 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1402 training loss: 0.6930555258078153, test loss: 0.6931621180752212 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1403 training loss: 0.6930551218995131, test loss: 0.693162178533752 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1404 training loss: 0.6930547161882101, test loss: 0.6931622392626318 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1405 training loss: 0.6930543086659775, test loss: 0.6931623002630269 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1406 training loss: 0.6930538993248516, test loss: 0.6931623615361087 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1407 training loss: 0.6930534881568361, test loss: 0.6931624230830526 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1408 training loss: 0.6930530751538996, test loss: 0.693162484905039 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1409 training loss: 0.693052660307977, test loss: 0.6931625470032533 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1410 training loss: 0.6930522436109687, test loss: 0.693162609378885 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1411 training loss: 0.693051825054741, test loss: 0.6931626720331291 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1412 training loss: 0.6930514046311252, test loss: 0.6931627349671848 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1413 training loss: 0.693050982331918, test loss: 0.6931627981822567 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1414 training loss: 0.6930505581488813, test loss: 0.6931628616795537 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1415 training loss: 0.6930501320737417, test loss: 0.6931629254602896 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1416 training loss: 0.6930497040981911, test loss: 0.6931629895256836 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1417 training loss: 0.693049274213886, test loss: 0.6931630538769588 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1418 training loss: 0.6930488424124469, test loss: 0.6931631185153441 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1419 training loss: 0.6930484086854596, test loss: 0.6931631834420727 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1420 training loss: 0.6930479730244735, test loss: 0.6931632486583831 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1421 training loss: 0.6930475354210026, test loss: 0.6931633141655187 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1422 training loss: 0.6930470958665248, test loss: 0.6931633799647274 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1423 training loss: 0.6930466543524815, test loss: 0.6931634460572627 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1424 training loss: 0.6930462108702784, test loss: 0.6931635124443826 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1425 training loss: 0.6930457654112842, test loss: 0.6931635791273504 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1426 training loss: 0.6930453179668316, test loss: 0.6931636461074343 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1427 training loss: 0.6930448685282162, test loss: 0.6931637133859074 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1428 training loss: 0.6930444170866967, test loss: 0.6931637809640482 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1429 training loss: 0.6930439636334951, test loss: 0.69316384884314 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1430 training loss: 0.6930435081597962, test loss: 0.6931639170244712 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1431 training loss: 0.6930430506567474, test loss: 0.6931639855093354 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1432 training loss: 0.6930425911154584, test loss: 0.6931640542990313 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1433 training loss: 0.693042129527002, test loss: 0.6931641233948627 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1434 training loss: 0.6930416658824123, test loss: 0.6931641927981386 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1435 training loss: 0.6930412001726867, test loss: 0.693164262510173 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1436 training loss: 0.6930407323887836, test loss: 0.6931643325322856 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1437 training loss: 0.6930402625216234, test loss: 0.6931644028658005 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1438 training loss: 0.6930397905620888, test loss: 0.6931644735120478 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1439 training loss: 0.693039316501023, test loss: 0.6931645444723621 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1440 training loss: 0.6930388403292315, test loss: 0.6931646157480842 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1441 training loss: 0.6930383620374805, test loss: 0.6931646873405591 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1442 training loss: 0.6930378816164975, test loss: 0.693164759251138 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1443 training loss: 0.6930373990569706, test loss: 0.6931648314811769 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1444 training loss: 0.693036914349549, test loss: 0.6931649040320373 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1445 training loss: 0.6930364274848424, test loss: 0.693164976905086 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1446 training loss: 0.6930359384534208, test loss: 0.6931650501016953 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1447 training loss: 0.693035447245815, test loss: 0.6931651236232425 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1448 training loss: 0.693034953852515, test loss: 0.6931651974711104 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1449 training loss: 0.6930344582639719, test loss: 0.693165271646688 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1450 training loss: 0.6930339604705958, test loss: 0.6931653461513684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1451 training loss: 0.6930334604627568, test loss: 0.6931654209865515 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1452 training loss: 0.6930329582307846, test loss: 0.6931654961536414 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1453 training loss: 0.6930324537649681, test loss: 0.6931655716540488 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1454 training loss: 0.6930319470555555, test loss: 0.6931656474891893 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1455 training loss: 0.693031438092754, test loss: 0.6931657236604837 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1456 training loss: 0.6930309268667296, test loss: 0.6931658001693591 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1457 training loss: 0.6930304133676073, test loss: 0.693165877017248 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1458 training loss: 0.6930298975854703, test loss: 0.6931659542055879 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1459 training loss: 0.6930293795103606, test loss: 0.6931660317358225 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1460 training loss: 0.6930288591322782, test loss: 0.6931661096094008 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1461 training loss: 0.6930283364411812, test loss: 0.6931661878277775 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1462 training loss: 0.6930278114269858, test loss: 0.6931662663924131 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1463 training loss: 0.6930272840795655, test loss: 0.6931663453047734 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1464 training loss: 0.693026754388752, test loss: 0.6931664245663303 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1465 training loss: 0.6930262223443343, test loss: 0.6931665041785613 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1466 training loss: 0.6930256879360582, test loss: 0.6931665841429493 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1467 training loss: 0.6930251511536272, test loss: 0.6931666644609832 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1468 training loss: 0.6930246119867013, test loss: 0.6931667451341577 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1469 training loss: 0.6930240704248977, test loss: 0.6931668261639731 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1470 training loss: 0.6930235264577894, test loss: 0.6931669075519356 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1471 training loss: 0.6930229800749073, test loss: 0.6931669892995572 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1472 training loss: 0.6930224312657369, test loss: 0.6931670714083555 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1473 training loss: 0.6930218800197211, test loss: 0.6931671538798548 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1474 training loss: 0.6930213263262579, test loss: 0.6931672367155837 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1475 training loss: 0.6930207701747018, test loss: 0.6931673199170783 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1476 training loss: 0.6930202115543621, test loss: 0.6931674034858796 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1477 training loss: 0.6930196504545041, test loss: 0.6931674874235345 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1478 training loss: 0.6930190868643484, test loss: 0.6931675717315968 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1479 training loss: 0.6930185207730704, test loss: 0.6931676564116251 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1480 training loss: 0.6930179521698002, test loss: 0.6931677414651848 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1481 training loss: 0.6930173810436234, test loss: 0.6931678268938467 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1482 training loss: 0.6930168073835795, test loss: 0.6931679126991881 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1483 training loss: 0.6930162311786628, test loss: 0.6931679988827918 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1484 training loss: 0.6930156524178216, test loss: 0.6931680854462473 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1485 training loss: 0.6930150710899582, test loss: 0.6931681723911494 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1486 training loss: 0.6930144871839291, test loss: 0.6931682597190998 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1487 training loss: 0.6930139006885442, test loss: 0.6931683474317054 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1488 training loss: 0.6930133115925672, test loss: 0.6931684355305803 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1489 training loss: 0.6930127198847148, test loss: 0.6931685240173437 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1490 training loss: 0.6930121255536572, test loss: 0.6931686128936216 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1491 training loss: 0.6930115285880172, test loss: 0.6931687021610461 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1492 training loss: 0.6930109289763711, test loss: 0.6931687918212548 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1493 training loss: 0.693010326707247, test loss: 0.6931688818758929 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1494 training loss: 0.6930097217691263, test loss: 0.6931689723266103 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1495 training loss: 0.6930091141504421, test loss: 0.6931690631750643 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1496 training loss: 0.6930085038395797, test loss: 0.6931691544229179 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1497 training loss: 0.6930078908248762, test loss: 0.6931692460718406 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1498 training loss: 0.6930072750946212, test loss: 0.6931693381235079 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1499 training loss: 0.6930066566370549, test loss: 0.6931694305796022 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1500 training loss: 0.6930060354403692, test loss: 0.6931695234418117 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1501 training loss: 0.6930054114927077, test loss: 0.6931696167118312 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1502 training loss: 0.6930047847821641, test loss: 0.6931697103913619 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1503 training loss: 0.6930041552967836, test loss: 0.6931698044821111 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1504 training loss: 0.6930035230245618, test loss: 0.6931698989857931 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1505 training loss: 0.693002887953445, test loss: 0.693169993904128 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1506 training loss: 0.6930022500713292, test loss: 0.693170089238843 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1507 training loss: 0.6930016093660613, test loss: 0.6931701849916713 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1508 training loss: 0.6930009658254372, test loss: 0.6931702811643524 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1509 training loss: 0.6930003194372032, test loss: 0.6931703777586332 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1510 training loss: 0.692999670189055, test loss: 0.6931704747762664 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1511 training loss: 0.6929990180686372, test loss: 0.693170572219011 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1512 training loss: 0.692998363063544, test loss: 0.6931706700886336 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1513 training loss: 0.6929977051613184, test loss: 0.6931707683869066 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1514 training loss: 0.6929970443494522, test loss: 0.6931708671156089 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1515 training loss: 0.6929963806153858, test loss: 0.693170966276527 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1516 training loss: 0.6929957139465079, test loss: 0.6931710658714526 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1517 training loss: 0.6929950443301556, test loss: 0.6931711659021853 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1518 training loss: 0.6929943717536133, test loss: 0.6931712663705308 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1519 training loss: 0.6929936962041142, test loss: 0.6931713672783019 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1520 training loss: 0.6929930176688386, test loss: 0.6931714686273174 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1521 training loss: 0.692992336134914, test loss: 0.6931715704194037 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1522 training loss: 0.6929916515894153, test loss: 0.6931716726563932 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1523 training loss: 0.692990964019365, test loss: 0.6931717753401258 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1524 training loss: 0.6929902734117317, test loss: 0.6931718784724477 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1525 training loss: 0.6929895797534307, test loss: 0.6931719820552118 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1526 training loss: 0.6929888830313239, test loss: 0.6931720860902787 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1527 training loss: 0.6929881832322196, test loss: 0.6931721905795147 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1528 training loss: 0.6929874803428718, test loss: 0.6931722955247938 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1529 training loss: 0.6929867743499807, test loss: 0.6931724009279967 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1530 training loss: 0.6929860652401919, test loss: 0.6931725067910106 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1531 training loss: 0.6929853530000966, test loss: 0.6931726131157306 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1532 training loss: 0.692984637616231, test loss: 0.6931727199040574 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1533 training loss: 0.6929839190750765, test loss: 0.6931728271579001 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1534 training loss: 0.6929831973630594, test loss: 0.6931729348791738 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1535 training loss: 0.6929824724665505, test loss: 0.6931730430698008 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1536 training loss: 0.6929817443718652, test loss: 0.6931731517317106 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1537 training loss: 0.6929810130652629, test loss: 0.69317326086684 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1538 training loss: 0.6929802785329472, test loss: 0.6931733704771322 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1539 training loss: 0.6929795407610655, test loss: 0.6931734805645381 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1540 training loss: 0.6929787997357088, test loss: 0.6931735911310154 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1541 training loss: 0.6929780554429114, test loss: 0.693173702178529 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1542 training loss: 0.6929773078686512, test loss: 0.6931738137090511 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1543 training loss: 0.6929765569988484, test loss: 0.6931739257245604 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1544 training loss: 0.6929758028193667, test loss: 0.6931740382270439 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1545 training loss: 0.6929750453160117, test loss: 0.693174151218495 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1546 training loss: 0.6929742844745324, test loss: 0.6931742647009145 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1547 training loss: 0.6929735202806185, test loss: 0.6931743786763105 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1548 training loss: 0.6929727527199031, test loss: 0.6931744931466984 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1549 training loss: 0.6929719817779602, test loss: 0.6931746081141006 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1550 training loss: 0.6929712074403054, test loss: 0.6931747235805473 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1551 training loss: 0.6929704296923959, test loss: 0.6931748395480757 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1552 training loss: 0.6929696485196299, test loss: 0.6931749560187304 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1553 training loss: 0.6929688639073462, test loss: 0.6931750729945632 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1554 training loss: 0.6929680758408249, test loss: 0.6931751904776337 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1555 training loss: 0.6929672843052859, test loss: 0.6931753084700083 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1556 training loss: 0.6929664892858898, test loss: 0.6931754269737613 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1557 training loss: 0.6929656907677371, test loss: 0.6931755459909746 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1558 training loss: 0.6929648887358679, test loss: 0.693175665523737 \n",
            "train accuracy: 0.5008976660682226, test accuracy: 0.5\n",
            "Iteration 1559 training loss: 0.6929640831752624, test loss: 0.693175785574145 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1560 training loss: 0.6929632740708399, test loss: 0.6931759061443026 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1561 training loss: 0.6929624614074585, test loss: 0.6931760272363214 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1562 training loss: 0.692961645169916, test loss: 0.6931761488523207 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1563 training loss: 0.6929608253429486, test loss: 0.693176270994427 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1564 training loss: 0.6929600019112304, test loss: 0.6931763936647745 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1565 training loss: 0.692959174859375, test loss: 0.693176516865505 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1566 training loss: 0.692958344171933, test loss: 0.6931766405987677 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1567 training loss: 0.6929575098333937, test loss: 0.6931767648667202 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1568 training loss: 0.6929566718281831, test loss: 0.6931768896715269 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.5\n",
            "Iteration 1569 training loss: 0.6929558301406653, test loss: 0.6931770150153601 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1570 training loss: 0.6929549847551415, test loss: 0.6931771409004002 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1571 training loss: 0.6929541356558496, test loss: 0.6931772673288348 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1572 training loss: 0.6929532828269646, test loss: 0.6931773943028594 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1573 training loss: 0.6929524262525975, test loss: 0.6931775218246774 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1574 training loss: 0.6929515659167961, test loss: 0.6931776498964997 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1575 training loss: 0.6929507018035437, test loss: 0.6931777785205453 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1576 training loss: 0.6929498338967601, test loss: 0.6931779076990408 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1577 training loss: 0.6929489621803003, test loss: 0.6931780374342207 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1578 training loss: 0.6929480866379546, test loss: 0.6931781677283273 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1579 training loss: 0.6929472072534486, test loss: 0.6931782985836106 \n",
            "train accuracy: 0.5026929982046678, test accuracy: 0.5\n",
            "Iteration 1580 training loss: 0.6929463240104429, test loss: 0.6931784300023291 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.5\n",
            "Iteration 1581 training loss: 0.6929454368925325, test loss: 0.6931785619867488 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.5\n",
            "Iteration 1582 training loss: 0.6929445458832472, test loss: 0.693178694539143 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.5\n",
            "Iteration 1583 training loss: 0.6929436509660508, test loss: 0.6931788276617944 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.5\n",
            "Iteration 1584 training loss: 0.6929427521243411, test loss: 0.6931789613569922 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.5\n",
            "Iteration 1585 training loss: 0.6929418493414495, test loss: 0.6931790956270346 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.5\n",
            "Iteration 1586 training loss: 0.6929409426006417, test loss: 0.693179230474227 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.5\n",
            "Iteration 1587 training loss: 0.6929400318851155, test loss: 0.6931793659008838 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.5\n",
            "Iteration 1588 training loss: 0.6929391171780028, test loss: 0.6931795019093266 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.5\n",
            "Iteration 1589 training loss: 0.6929381984623678, test loss: 0.6931796385018855 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1590 training loss: 0.6929372757212071, test loss: 0.6931797756808983 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1591 training loss: 0.6929363489374504, test loss: 0.6931799134487113 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1592 training loss: 0.6929354180939588, test loss: 0.6931800518076788 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1593 training loss: 0.6929344831735256, test loss: 0.693180190760163 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1594 training loss: 0.6929335441588755, test loss: 0.6931803303085348 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1595 training loss: 0.6929326010326651, test loss: 0.6931804704551727 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1596 training loss: 0.6929316537774816, test loss: 0.6931806112024638 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1597 training loss: 0.6929307023758433, test loss: 0.6931807525528031 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1598 training loss: 0.6929297468101994, test loss: 0.6931808945085941 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1599 training loss: 0.6929287870629293, test loss: 0.6931810370722484 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4928571428571429\n",
            "Iteration 1600 training loss: 0.6929278231163427, test loss: 0.693181180246186 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4857142857142857\n",
            "Iteration 1601 training loss: 0.692926854952679, test loss: 0.6931813240328353 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4785714285714286\n",
            "Iteration 1602 training loss: 0.692925882554108, test loss: 0.6931814684346325 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4785714285714286\n",
            "Iteration 1603 training loss: 0.6929249059027281, test loss: 0.6931816134540227 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4785714285714286\n",
            "Iteration 1604 training loss: 0.6929239249805675, test loss: 0.6931817590934591 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4785714285714286\n",
            "Iteration 1605 training loss: 0.692922939769583, test loss: 0.6931819053554034 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1606 training loss: 0.6929219502516607, test loss: 0.6931820522423255 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1607 training loss: 0.6929209564086143, test loss: 0.6931821997567038 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1608 training loss: 0.6929199582221869, test loss: 0.6931823479010253 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4785714285714286\n",
            "Iteration 1609 training loss: 0.6929189556740483, test loss: 0.6931824966777851 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1610 training loss: 0.6929179487457972, test loss: 0.6931826460894872 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1611 training loss: 0.692916937418959, test loss: 0.6931827961386438 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.4785714285714286\n",
            "Iteration 1612 training loss: 0.6929159216749867, test loss: 0.6931829468277755 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4785714285714286\n",
            "Iteration 1613 training loss: 0.6929149014952602, test loss: 0.6931830981594117 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4785714285714286\n",
            "Iteration 1614 training loss: 0.6929138768610864, test loss: 0.6931832501360903 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1615 training loss: 0.6929128477536981, test loss: 0.6931834027603577 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1616 training loss: 0.6929118141542548, test loss: 0.6931835560347689 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1617 training loss: 0.692910776043842, test loss: 0.6931837099618876 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1618 training loss: 0.6929097334034706, test loss: 0.6931838645442858 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1619 training loss: 0.6929086862140776, test loss: 0.6931840197845446 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1620 training loss: 0.6929076344565245, test loss: 0.6931841756852535 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1621 training loss: 0.6929065781115981, test loss: 0.6931843322490105 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1622 training loss: 0.6929055171600099, test loss: 0.693184489478423 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1623 training loss: 0.6929044515823961, test loss: 0.6931846473761062 \n",
            "train accuracy: 0.5044883303411131, test accuracy: 0.4714285714285714\n",
            "Iteration 1624 training loss: 0.6929033813593165, test loss: 0.6931848059446846 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4714285714285714\n",
            "Iteration 1625 training loss: 0.6929023064712554, test loss: 0.6931849651867914 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4714285714285714\n",
            "Iteration 1626 training loss: 0.6929012268986207, test loss: 0.6931851251050686 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1627 training loss: 0.6929001426217433, test loss: 0.6931852857021669 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1628 training loss: 0.6928990536208777, test loss: 0.6931854469807454 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1629 training loss: 0.6928979598762013, test loss: 0.693185608943473 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1630 training loss: 0.6928968613678137, test loss: 0.6931857715930269 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1631 training loss: 0.6928957580757376, test loss: 0.6931859349320929 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1632 training loss: 0.692894649979917, test loss: 0.693186098963366 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1633 training loss: 0.6928935370602184, test loss: 0.6931862636895506 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1634 training loss: 0.6928924192964295, test loss: 0.6931864291133589 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1635 training loss: 0.6928912966682597, test loss: 0.6931865952375131 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.4642857142857143\n",
            "Iteration 1636 training loss: 0.692890169155339, test loss: 0.6931867620647439 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45714285714285713\n",
            "Iteration 1637 training loss: 0.6928890367372184, test loss: 0.6931869295977908 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1638 training loss: 0.6928878993933696, test loss: 0.6931870978394029 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1639 training loss: 0.6928867571031843, test loss: 0.6931872667923376 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1640 training loss: 0.6928856098459741, test loss: 0.6931874364593619 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1641 training loss: 0.6928844576009711, test loss: 0.6931876068432518 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1642 training loss: 0.6928833003473257, test loss: 0.6931877779467921 \n",
            "train accuracy: 0.5062836624775583, test accuracy: 0.45\n",
            "Iteration 1643 training loss: 0.6928821380641083, test loss: 0.693187949772777 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1644 training loss: 0.692880970730308, test loss: 0.6931881223240093 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1645 training loss: 0.6928797983248323, test loss: 0.6931882956033018 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1646 training loss: 0.6928786208265073, test loss: 0.6931884696134757 \n",
            "train accuracy: 0.5080789946140036, test accuracy: 0.45\n",
            "Iteration 1647 training loss: 0.6928774382140774, test loss: 0.6931886443573617 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.45\n",
            "Iteration 1648 training loss: 0.6928762504662045, test loss: 0.6931888198377999 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1649 training loss: 0.6928750575614679, test loss: 0.6931889960576391 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1650 training loss: 0.6928738594783649, test loss: 0.6931891730197376 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1651 training loss: 0.6928726561953087, test loss: 0.6931893507269631 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.45\n",
            "Iteration 1652 training loss: 0.6928714476906301, test loss: 0.6931895291821922 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.45\n",
            "Iteration 1653 training loss: 0.6928702339425764, test loss: 0.6931897083883113 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1654 training loss: 0.6928690149293103, test loss: 0.6931898883482156 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.45\n",
            "Iteration 1655 training loss: 0.6928677906289108, test loss: 0.69319006906481 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1656 training loss: 0.6928665610193729, test loss: 0.6931902505410084 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1657 training loss: 0.6928653260786064, test loss: 0.6931904327797345 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1658 training loss: 0.6928640857844364, test loss: 0.693190615783921 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1659 training loss: 0.6928628401146024, test loss: 0.69319079955651 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1660 training loss: 0.692861589046759, test loss: 0.6931909841004533 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1661 training loss: 0.6928603325584745, test loss: 0.6931911694187122 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1662 training loss: 0.6928590706272314, test loss: 0.6931913555142569 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1663 training loss: 0.6928578032304257, test loss: 0.6931915423900679 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.45\n",
            "Iteration 1664 training loss: 0.6928565303453669, test loss: 0.693191730049134 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.44285714285714284\n",
            "Iteration 1665 training loss: 0.6928552519492773, test loss: 0.6931919184944549 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.4357142857142857\n",
            "Iteration 1666 training loss: 0.6928539680192922, test loss: 0.693192107729039 \n",
            "train accuracy: 0.5134649910233393, test accuracy: 0.4357142857142857\n",
            "Iteration 1667 training loss: 0.6928526785324594, test loss: 0.6931922977559043 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.4357142857142857\n",
            "Iteration 1668 training loss: 0.6928513834657388, test loss: 0.6931924885780786 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.4357142857142857\n",
            "Iteration 1669 training loss: 0.6928500827960022, test loss: 0.693192680198599 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4357142857142857\n",
            "Iteration 1670 training loss: 0.6928487765000335, test loss: 0.6931928726205128 \n",
            "train accuracy: 0.5098743267504489, test accuracy: 0.4357142857142857\n",
            "Iteration 1671 training loss: 0.6928474645545271, test loss: 0.693193065846876 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.4357142857142857\n",
            "Iteration 1672 training loss: 0.6928461469360888, test loss: 0.6931932598807551 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.4357142857142857\n",
            "Iteration 1673 training loss: 0.6928448236212357, test loss: 0.6931934547252259 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.44285714285714284\n",
            "Iteration 1674 training loss: 0.6928434945863947, test loss: 0.6931936503833735 \n",
            "train accuracy: 0.5134649910233393, test accuracy: 0.44285714285714284\n",
            "Iteration 1675 training loss: 0.6928421598079031, test loss: 0.6931938468582933 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.44285714285714284\n",
            "Iteration 1676 training loss: 0.6928408192620081, test loss: 0.6931940441530908 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.44285714285714284\n",
            "Iteration 1677 training loss: 0.6928394729248666, test loss: 0.6931942422708798 \n",
            "train accuracy: 0.5134649910233393, test accuracy: 0.44285714285714284\n",
            "Iteration 1678 training loss: 0.6928381207725444, test loss: 0.6931944412147851 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.44285714285714284\n",
            "Iteration 1679 training loss: 0.6928367627810169, test loss: 0.6931946409879407 \n",
            "train accuracy: 0.5116696588868941, test accuracy: 0.44285714285714284\n",
            "Iteration 1680 training loss: 0.6928353989261676, test loss: 0.6931948415934908 \n",
            "train accuracy: 0.5134649910233393, test accuracy: 0.44285714285714284\n",
            "Iteration 1681 training loss: 0.6928340291837888, test loss: 0.6931950430345889 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.44285714285714284\n",
            "Iteration 1682 training loss: 0.6928326535295809, test loss: 0.6931952453143988 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.44285714285714284\n",
            "Iteration 1683 training loss: 0.6928312719391518, test loss: 0.6931954484360937 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.44285714285714284\n",
            "Iteration 1684 training loss: 0.6928298843880173, test loss: 0.6931956524028574 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.44285714285714284\n",
            "Iteration 1685 training loss: 0.6928284908516001, test loss: 0.6931958572178825 \n",
            "train accuracy: 0.5152603231597845, test accuracy: 0.44285714285714284\n",
            "Iteration 1686 training loss: 0.69282709130523, test loss: 0.6931960628843722 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.44285714285714284\n",
            "Iteration 1687 training loss: 0.6928256857241434, test loss: 0.6931962694055397 \n",
            "train accuracy: 0.5170556552962298, test accuracy: 0.44285714285714284\n",
            "Iteration 1688 training loss: 0.6928242740834829, test loss: 0.6931964767846078 \n",
            "train accuracy: 0.518850987432675, test accuracy: 0.44285714285714284\n",
            "Iteration 1689 training loss: 0.6928228563582973, test loss: 0.6931966850248094 \n",
            "train accuracy: 0.518850987432675, test accuracy: 0.44285714285714284\n",
            "Iteration 1690 training loss: 0.6928214325235408, test loss: 0.6931968941293875 \n",
            "train accuracy: 0.5224416517055656, test accuracy: 0.44285714285714284\n",
            "Iteration 1691 training loss: 0.6928200025540731, test loss: 0.6931971041015947 \n",
            "train accuracy: 0.5242369838420108, test accuracy: 0.44285714285714284\n",
            "Iteration 1692 training loss: 0.6928185664246594, test loss: 0.6931973149446942 \n",
            "train accuracy: 0.5242369838420108, test accuracy: 0.44285714285714284\n",
            "Iteration 1693 training loss: 0.6928171241099692, test loss: 0.6931975266619587 \n",
            "train accuracy: 0.526032315978456, test accuracy: 0.44285714285714284\n",
            "Iteration 1694 training loss: 0.6928156755845766, test loss: 0.6931977392566709 \n",
            "train accuracy: 0.526032315978456, test accuracy: 0.44285714285714284\n",
            "Iteration 1695 training loss: 0.6928142208229597, test loss: 0.6931979527321243 \n",
            "train accuracy: 0.526032315978456, test accuracy: 0.44285714285714284\n",
            "Iteration 1696 training loss: 0.6928127597995013, test loss: 0.6931981670916215 \n",
            "train accuracy: 0.526032315978456, test accuracy: 0.44285714285714284\n",
            "Iteration 1697 training loss: 0.6928112924884867, test loss: 0.6931983823384759 \n",
            "train accuracy: 0.526032315978456, test accuracy: 0.44285714285714284\n",
            "Iteration 1698 training loss: 0.6928098188641048, test loss: 0.693198598476011 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.44285714285714284\n",
            "Iteration 1699 training loss: 0.6928083389004477, test loss: 0.6931988155075599 \n",
            "train accuracy: 0.5296229802513465, test accuracy: 0.44285714285714284\n",
            "Iteration 1700 training loss: 0.69280685257151, test loss: 0.6931990334364664 \n",
            "train accuracy: 0.5296229802513465, test accuracy: 0.44285714285714284\n",
            "Iteration 1701 training loss: 0.6928053598511885, test loss: 0.693199252266084 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1702 training loss: 0.692803860713282, test loss: 0.693199471999777 \n",
            "train accuracy: 0.5296229802513465, test accuracy: 0.4357142857142857\n",
            "Iteration 1703 training loss: 0.6928023551314915, test loss: 0.6931996926409196 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1704 training loss: 0.6928008430794184, test loss: 0.6931999141928957 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1705 training loss: 0.692799324530566, test loss: 0.6932001366591004 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1706 training loss: 0.6927977994583377, test loss: 0.6932003600429383 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1707 training loss: 0.6927962678360381, test loss: 0.6932005843478249 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1708 training loss: 0.6927947296368714, test loss: 0.6932008095771853 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1709 training loss: 0.6927931848339415, test loss: 0.6932010357344556 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1710 training loss: 0.6927916334002522, test loss: 0.6932012628230819 \n",
            "train accuracy: 0.5278276481149012, test accuracy: 0.4357142857142857\n",
            "Iteration 1711 training loss: 0.692790075308706, test loss: 0.6932014908465206 \n",
            "train accuracy: 0.5314183123877917, test accuracy: 0.42857142857142855\n",
            "Iteration 1712 training loss: 0.6927885105321047, test loss: 0.6932017198082385 \n",
            "train accuracy: 0.5314183123877917, test accuracy: 0.42857142857142855\n",
            "Iteration 1713 training loss: 0.6927869390431479, test loss: 0.6932019497117131 \n",
            "train accuracy: 0.5314183123877917, test accuracy: 0.42857142857142855\n",
            "Iteration 1714 training loss: 0.6927853608144344, test loss: 0.6932021805604318 \n",
            "train accuracy: 0.5314183123877917, test accuracy: 0.4357142857142857\n",
            "Iteration 1715 training loss: 0.6927837758184598, test loss: 0.6932024123578927 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.4357142857142857\n",
            "Iteration 1716 training loss: 0.6927821840276183, test loss: 0.6932026451076045 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.42142857142857143\n",
            "Iteration 1717 training loss: 0.6927805854142004, test loss: 0.6932028788130862 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.4142857142857143\n",
            "Iteration 1718 training loss: 0.692778979950394, test loss: 0.693203113477867 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.4142857142857143\n",
            "Iteration 1719 training loss: 0.6927773676082833, test loss: 0.6932033491054871 \n",
            "train accuracy: 0.5368043087971275, test accuracy: 0.4142857142857143\n",
            "Iteration 1720 training loss: 0.692775748359849, test loss: 0.693203585699497 \n",
            "train accuracy: 0.5385996409335727, test accuracy: 0.4142857142857143\n",
            "Iteration 1721 training loss: 0.6927741221769677, test loss: 0.6932038232634576 \n",
            "train accuracy: 0.5385996409335727, test accuracy: 0.4142857142857143\n",
            "Iteration 1722 training loss: 0.6927724890314114, test loss: 0.6932040618009405 \n",
            "train accuracy: 0.5368043087971275, test accuracy: 0.40714285714285714\n",
            "Iteration 1723 training loss: 0.6927708488948475, test loss: 0.693204301315528 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1724 training loss: 0.692769201738838, test loss: 0.6932045418108126 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1725 training loss: 0.6927675475348403, test loss: 0.6932047832903978 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1726 training loss: 0.6927658862542048, test loss: 0.6932050257578977 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.40714285714285714\n",
            "Iteration 1727 training loss: 0.6927642178681773, test loss: 0.6932052692169364 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1728 training loss: 0.6927625423478958, test loss: 0.6932055136711498 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1729 training loss: 0.6927608596643926, test loss: 0.6932057591241835 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.40714285714285714\n",
            "Iteration 1730 training loss: 0.6927591697885924, test loss: 0.6932060055796943 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.4142857142857143\n",
            "Iteration 1731 training loss: 0.6927574726913127, test loss: 0.6932062530413493 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.42142857142857143\n",
            "Iteration 1732 training loss: 0.6927557683432629, test loss: 0.693206501512827 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.42142857142857143\n",
            "Iteration 1733 training loss: 0.6927540567150452, test loss: 0.693206750997816 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.42857142857142855\n",
            "Iteration 1734 training loss: 0.6927523377771521, test loss: 0.6932070015000158 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1735 training loss: 0.6927506114999682, test loss: 0.6932072530231372 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.42857142857142855\n",
            "Iteration 1736 training loss: 0.6927488778537692, test loss: 0.6932075055709013 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1737 training loss: 0.6927471368087202, test loss: 0.6932077591470401 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1738 training loss: 0.6927453883348779, test loss: 0.6932080137552963 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1739 training loss: 0.6927436324021878, test loss: 0.6932082693994238 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1740 training loss: 0.6927418689804858, test loss: 0.6932085260831874 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1741 training loss: 0.6927400980394961, test loss: 0.6932087838103623 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1742 training loss: 0.6927383195488324, test loss: 0.6932090425847353 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1743 training loss: 0.6927365334779966, test loss: 0.6932093024101036 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.42857142857142855\n",
            "Iteration 1744 training loss: 0.6927347397963788, test loss: 0.6932095632902756 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.4357142857142857\n",
            "Iteration 1745 training loss: 0.6927329384732568, test loss: 0.6932098252290705 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.4357142857142857\n",
            "Iteration 1746 training loss: 0.6927311294777958, test loss: 0.6932100882303189 \n",
            "train accuracy: 0.533213644524237, test accuracy: 0.4357142857142857\n",
            "Iteration 1747 training loss: 0.6927293127790486, test loss: 0.6932103522978617 \n",
            "train accuracy: 0.5350089766606823, test accuracy: 0.4357142857142857\n",
            "Iteration 1748 training loss: 0.6927274883459539, test loss: 0.6932106174355516 \n",
            "train accuracy: 0.5385996409335727, test accuracy: 0.44285714285714284\n",
            "Iteration 1749 training loss: 0.6927256561473376, test loss: 0.6932108836472517 \n",
            "train accuracy: 0.5403949730700179, test accuracy: 0.44285714285714284\n",
            "Iteration 1750 training loss: 0.6927238161519111, test loss: 0.6932111509368367 \n",
            "train accuracy: 0.5403949730700179, test accuracy: 0.45\n",
            "Iteration 1751 training loss: 0.6927219683282716, test loss: 0.693211419308192 \n",
            "train accuracy: 0.5439856373429084, test accuracy: 0.45\n",
            "Iteration 1752 training loss: 0.6927201126449017, test loss: 0.6932116887652141 \n",
            "train accuracy: 0.5439856373429084, test accuracy: 0.45\n",
            "Iteration 1753 training loss: 0.692718249070169, test loss: 0.693211959311811 \n",
            "train accuracy: 0.5457809694793537, test accuracy: 0.45\n",
            "Iteration 1754 training loss: 0.6927163775723255, test loss: 0.6932122309519017 \n",
            "train accuracy: 0.5439856373429084, test accuracy: 0.45\n",
            "Iteration 1755 training loss: 0.6927144981195076, test loss: 0.6932125036894158 \n",
            "train accuracy: 0.5457809694793537, test accuracy: 0.44285714285714284\n",
            "Iteration 1756 training loss: 0.6927126106797359, test loss: 0.693212777528295 \n",
            "train accuracy: 0.5457809694793537, test accuracy: 0.44285714285714284\n",
            "Iteration 1757 training loss: 0.6927107152209139, test loss: 0.6932130524724917 \n",
            "train accuracy: 0.5439856373429084, test accuracy: 0.45\n",
            "Iteration 1758 training loss: 0.6927088117108285, test loss: 0.6932133285259692 \n",
            "train accuracy: 0.5439856373429084, test accuracy: 0.45\n",
            "Iteration 1759 training loss: 0.6927069001171499, test loss: 0.6932136056927024 \n",
            "train accuracy: 0.547576301615799, test accuracy: 0.45\n",
            "Iteration 1760 training loss: 0.6927049804074301, test loss: 0.6932138839766777 \n",
            "train accuracy: 0.547576301615799, test accuracy: 0.45\n",
            "Iteration 1761 training loss: 0.6927030525491034, test loss: 0.6932141633818925 \n",
            "train accuracy: 0.547576301615799, test accuracy: 0.45\n",
            "Iteration 1762 training loss: 0.6927011165094861, test loss: 0.6932144439123551 \n",
            "train accuracy: 0.5457809694793537, test accuracy: 0.45\n",
            "Iteration 1763 training loss: 0.6926991722557754, test loss: 0.6932147255720862 \n",
            "train accuracy: 0.5493716337522442, test accuracy: 0.45\n",
            "Iteration 1764 training loss: 0.6926972197550496, test loss: 0.6932150083651163 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.45714285714285713\n",
            "Iteration 1765 training loss: 0.6926952589742682, test loss: 0.6932152922954888 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.45714285714285713\n",
            "Iteration 1766 training loss: 0.6926932898802699, test loss: 0.6932155773672573 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.4642857142857143\n",
            "Iteration 1767 training loss: 0.6926913124397742, test loss: 0.6932158635844873 \n",
            "train accuracy: 0.5529622980251346, test accuracy: 0.4642857142857143\n",
            "Iteration 1768 training loss: 0.6926893266193797, test loss: 0.6932161509512555 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.4642857142857143\n",
            "Iteration 1769 training loss: 0.6926873323855642, test loss: 0.6932164394716507 \n",
            "train accuracy: 0.5493716337522442, test accuracy: 0.4642857142857143\n",
            "Iteration 1770 training loss: 0.6926853297046842, test loss: 0.6932167291497718 \n",
            "train accuracy: 0.5493716337522442, test accuracy: 0.4642857142857143\n",
            "Iteration 1771 training loss: 0.6926833185429748, test loss: 0.6932170199897308 \n",
            "train accuracy: 0.547576301615799, test accuracy: 0.4642857142857143\n",
            "Iteration 1772 training loss: 0.6926812988665488, test loss: 0.6932173119956498 \n",
            "train accuracy: 0.5493716337522442, test accuracy: 0.4642857142857143\n",
            "Iteration 1773 training loss: 0.692679270641397, test loss: 0.693217605171663 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.4642857142857143\n",
            "Iteration 1774 training loss: 0.6926772338333873, test loss: 0.6932178995219163 \n",
            "train accuracy: 0.5511669658886894, test accuracy: 0.4642857142857143\n",
            "Iteration 1775 training loss: 0.6926751884082644, test loss: 0.6932181950505667 \n",
            "train accuracy: 0.5529622980251346, test accuracy: 0.4642857142857143\n",
            "Iteration 1776 training loss: 0.6926731343316495, test loss: 0.6932184917617831 \n",
            "train accuracy: 0.5547576301615799, test accuracy: 0.4642857142857143\n",
            "Iteration 1777 training loss: 0.6926710715690402, test loss: 0.6932187896597456 \n",
            "train accuracy: 0.5565529622980251, test accuracy: 0.4642857142857143\n",
            "Iteration 1778 training loss: 0.6926690000858096, test loss: 0.6932190887486465 \n",
            "train accuracy: 0.5601436265709157, test accuracy: 0.4642857142857143\n",
            "Iteration 1779 training loss: 0.692666919847206, test loss: 0.6932193890326891 \n",
            "train accuracy: 0.5619389587073609, test accuracy: 0.4642857142857143\n",
            "Iteration 1780 training loss: 0.6926648308183533, test loss: 0.6932196905160888 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1781 training loss: 0.6926627329642493, test loss: 0.6932199932030723 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1782 training loss: 0.6926606262497665, test loss: 0.6932202970978784 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1783 training loss: 0.6926585106396511, test loss: 0.6932206022047571 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1784 training loss: 0.6926563860985226, test loss: 0.6932209085279706 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1785 training loss: 0.6926542525908737, test loss: 0.6932212160717924 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1786 training loss: 0.6926521100810695, test loss: 0.6932215248405083 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1787 training loss: 0.6926499585333478, test loss: 0.6932218348384152 \n",
            "train accuracy: 0.5655296229802513, test accuracy: 0.4642857142857143\n",
            "Iteration 1788 training loss: 0.692647797911818, test loss: 0.6932221460698221 \n",
            "train accuracy: 0.5655296229802513, test accuracy: 0.4642857142857143\n",
            "Iteration 1789 training loss: 0.6926456281804608, test loss: 0.6932224585390504 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.45714285714285713\n",
            "Iteration 1790 training loss: 0.6926434493031286, test loss: 0.6932227722504324 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.45714285714285713\n",
            "Iteration 1791 training loss: 0.692641261243544, test loss: 0.6932230872083129 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.45714285714285713\n",
            "Iteration 1792 training loss: 0.6926390639653, test loss: 0.6932234034170484 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1793 training loss: 0.6926368574318593, test loss: 0.6932237208810071 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1794 training loss: 0.6926346416065547, test loss: 0.6932240396045697 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4642857142857143\n",
            "Iteration 1795 training loss: 0.6926324164525877, test loss: 0.693224359592128 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4714285714285714\n",
            "Iteration 1796 training loss: 0.6926301819330282, test loss: 0.6932246808480865 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4714285714285714\n",
            "Iteration 1797 training loss: 0.6926279380108149, test loss: 0.6932250033768614 \n",
            "train accuracy: 0.5619389587073609, test accuracy: 0.4714285714285714\n",
            "Iteration 1798 training loss: 0.6926256846487545, test loss: 0.6932253271828811 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4714285714285714\n",
            "Iteration 1799 training loss: 0.6926234218095204, test loss: 0.6932256522705856 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4714285714285714\n",
            "Iteration 1800 training loss: 0.6926211494556541, test loss: 0.6932259786444276 \n",
            "train accuracy: 0.5619389587073609, test accuracy: 0.4714285714285714\n",
            "Iteration 1801 training loss: 0.6926188675495633, test loss: 0.6932263063088712 \n",
            "train accuracy: 0.5619389587073609, test accuracy: 0.4714285714285714\n",
            "Iteration 1802 training loss: 0.6926165760535214, test loss: 0.6932266352683931 \n",
            "train accuracy: 0.5601436265709157, test accuracy: 0.4714285714285714\n",
            "Iteration 1803 training loss: 0.6926142749296689, test loss: 0.6932269655274819 \n",
            "train accuracy: 0.5619389587073609, test accuracy: 0.4714285714285714\n",
            "Iteration 1804 training loss: 0.6926119641400107, test loss: 0.6932272970906382 \n",
            "train accuracy: 0.5637342908438061, test accuracy: 0.4714285714285714\n",
            "Iteration 1805 training loss: 0.692609643646417, test loss: 0.6932276299623753 \n",
            "train accuracy: 0.5673249551166966, test accuracy: 0.4714285714285714\n",
            "Iteration 1806 training loss: 0.6926073134106231, test loss: 0.6932279641472183 \n",
            "train accuracy: 0.5691202872531418, test accuracy: 0.4714285714285714\n",
            "Iteration 1807 training loss: 0.692604973394228, test loss: 0.6932282996497042 \n",
            "train accuracy: 0.5691202872531418, test accuracy: 0.4714285714285714\n",
            "Iteration 1808 training loss: 0.6926026235586947, test loss: 0.6932286364743833 \n",
            "train accuracy: 0.5709156193895871, test accuracy: 0.4714285714285714\n",
            "Iteration 1809 training loss: 0.6926002638653495, test loss: 0.6932289746258168 \n",
            "train accuracy: 0.5709156193895871, test accuracy: 0.4714285714285714\n",
            "Iteration 1810 training loss: 0.6925978942753817, test loss: 0.6932293141085791 \n",
            "train accuracy: 0.5709156193895871, test accuracy: 0.4785714285714286\n",
            "Iteration 1811 training loss: 0.692595514749843, test loss: 0.6932296549272566 \n",
            "train accuracy: 0.5727109515260324, test accuracy: 0.4714285714285714\n",
            "Iteration 1812 training loss: 0.6925931252496479, test loss: 0.6932299970864483 \n",
            "train accuracy: 0.5745062836624776, test accuracy: 0.4714285714285714\n",
            "Iteration 1813 training loss: 0.6925907257355715, test loss: 0.693230340590765 \n",
            "train accuracy: 0.5745062836624776, test accuracy: 0.4714285714285714\n",
            "Iteration 1814 training loss: 0.6925883161682509, test loss: 0.6932306854448304 \n",
            "train accuracy: 0.5763016157989228, test accuracy: 0.4714285714285714\n",
            "Iteration 1815 training loss: 0.692585896508184, test loss: 0.6932310316532801 \n",
            "train accuracy: 0.5798922800718133, test accuracy: 0.4714285714285714\n",
            "Iteration 1816 training loss: 0.6925834667157292, test loss: 0.6932313792207628 \n",
            "train accuracy: 0.5798922800718133, test accuracy: 0.4714285714285714\n",
            "Iteration 1817 training loss: 0.6925810267511044, test loss: 0.6932317281519392 \n",
            "train accuracy: 0.578096947935368, test accuracy: 0.4642857142857143\n",
            "Iteration 1818 training loss: 0.6925785765743877, test loss: 0.6932320784514825 \n",
            "train accuracy: 0.5798922800718133, test accuracy: 0.4642857142857143\n",
            "Iteration 1819 training loss: 0.692576116145516, test loss: 0.6932324301240781 \n",
            "train accuracy: 0.578096947935368, test accuracy: 0.4714285714285714\n",
            "Iteration 1820 training loss: 0.6925736454242848, test loss: 0.6932327831744248 \n",
            "train accuracy: 0.5798922800718133, test accuracy: 0.4714285714285714\n",
            "Iteration 1821 training loss: 0.6925711643703484, test loss: 0.693233137607233 \n",
            "train accuracy: 0.578096947935368, test accuracy: 0.4714285714285714\n",
            "Iteration 1822 training loss: 0.6925686729432183, test loss: 0.6932334934272262 \n",
            "train accuracy: 0.5798922800718133, test accuracy: 0.4714285714285714\n",
            "Iteration 1823 training loss: 0.6925661711022639, test loss: 0.6932338506391403 \n",
            "train accuracy: 0.5834829443447038, test accuracy: 0.4714285714285714\n",
            "Iteration 1824 training loss: 0.6925636588067114, test loss: 0.6932342092477238 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1825 training loss: 0.6925611360156435, test loss: 0.6932345692577382 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1826 training loss: 0.6925586026879991, test loss: 0.6932349306739569 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1827 training loss: 0.6925560587825728, test loss: 0.6932352935011669 \n",
            "train accuracy: 0.5834829443447038, test accuracy: 0.4785714285714286\n",
            "Iteration 1828 training loss: 0.6925535042580141, test loss: 0.6932356577441675 \n",
            "train accuracy: 0.5834829443447038, test accuracy: 0.4785714285714286\n",
            "Iteration 1829 training loss: 0.6925509390728277, test loss: 0.6932360234077701 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1830 training loss: 0.6925483631853726, test loss: 0.6932363904968003 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1831 training loss: 0.6925457765538612, test loss: 0.6932367590160953 \n",
            "train accuracy: 0.5834829443447038, test accuracy: 0.4785714285714286\n",
            "Iteration 1832 training loss: 0.69254317913636, test loss: 0.6932371289705055 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4785714285714286\n",
            "Iteration 1833 training loss: 0.6925405708907879, test loss: 0.6932375003648942 \n",
            "train accuracy: 0.5870736086175943, test accuracy: 0.4785714285714286\n",
            "Iteration 1834 training loss: 0.6925379517749168, test loss: 0.6932378732041375 \n",
            "train accuracy: 0.5888689407540395, test accuracy: 0.4785714285714286\n",
            "Iteration 1835 training loss: 0.6925353217463708, test loss: 0.6932382474931248 \n",
            "train accuracy: 0.5888689407540395, test accuracy: 0.4785714285714286\n",
            "Iteration 1836 training loss: 0.6925326807626248, test loss: 0.6932386232367577 \n",
            "train accuracy: 0.5870736086175943, test accuracy: 0.4785714285714286\n",
            "Iteration 1837 training loss: 0.6925300287810058, test loss: 0.6932390004399512 \n",
            "train accuracy: 0.5870736086175943, test accuracy: 0.4785714285714286\n",
            "Iteration 1838 training loss: 0.692527365758691, test loss: 0.6932393791076334 \n",
            "train accuracy: 0.5870736086175943, test accuracy: 0.4857142857142857\n",
            "Iteration 1839 training loss: 0.6925246916527084, test loss: 0.6932397592447448 \n",
            "train accuracy: 0.5888689407540395, test accuracy: 0.4857142857142857\n",
            "Iteration 1840 training loss: 0.692522006419935, test loss: 0.6932401408562401 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4857142857142857\n",
            "Iteration 1841 training loss: 0.692519310017098, test loss: 0.6932405239470855 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4857142857142857\n",
            "Iteration 1842 training loss: 0.692516602400773, test loss: 0.6932409085222618 \n",
            "train accuracy: 0.585278276481149, test accuracy: 0.4857142857142857\n",
            "Iteration 1843 training loss: 0.692513883527384, test loss: 0.693241294586762 \n",
            "train accuracy: 0.5870736086175943, test accuracy: 0.4857142857142857\n",
            "Iteration 1844 training loss: 0.6925111533532035, test loss: 0.6932416821455925 \n",
            "train accuracy: 0.5888689407540395, test accuracy: 0.4857142857142857\n",
            "Iteration 1845 training loss: 0.6925084118343506, test loss: 0.6932420712037729 \n",
            "train accuracy: 0.5906642728904847, test accuracy: 0.4857142857142857\n",
            "Iteration 1846 training loss: 0.6925056589267925, test loss: 0.693242461766336 \n",
            "train accuracy: 0.5906642728904847, test accuracy: 0.4928571428571429\n",
            "Iteration 1847 training loss: 0.6925028945863422, test loss: 0.6932428538383282 \n",
            "train accuracy: 0.5906642728904847, test accuracy: 0.4928571428571429\n",
            "Iteration 1848 training loss: 0.6925001187686588, test loss: 0.6932432474248083 \n",
            "train accuracy: 0.5906642728904847, test accuracy: 0.4928571428571429\n",
            "Iteration 1849 training loss: 0.6924973314292475, test loss: 0.6932436425308494 \n",
            "train accuracy: 0.5906642728904847, test accuracy: 0.4928571428571429\n",
            "Iteration 1850 training loss: 0.6924945325234585, test loss: 0.6932440391615371 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.4928571428571429\n",
            "Iteration 1851 training loss: 0.6924917220064862, test loss: 0.6932444373219708 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.4928571428571429\n",
            "Iteration 1852 training loss: 0.6924888998333699, test loss: 0.6932448370172635 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5\n",
            "Iteration 1853 training loss: 0.6924860659589923, test loss: 0.6932452382525409 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5\n",
            "Iteration 1854 training loss: 0.6924832203380792, test loss: 0.6932456410329431 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5071428571428571\n",
            "Iteration 1855 training loss: 0.6924803629251992, test loss: 0.6932460453636228 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5071428571428571\n",
            "Iteration 1856 training loss: 0.6924774936747635, test loss: 0.6932464512497466 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5071428571428571\n",
            "Iteration 1857 training loss: 0.6924746125410247, test loss: 0.6932468586964948 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5071428571428571\n",
            "Iteration 1858 training loss: 0.6924717194780772, test loss: 0.6932472677090613 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5071428571428571\n",
            "Iteration 1859 training loss: 0.6924688144398555, test loss: 0.6932476782926532 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5071428571428571\n",
            "Iteration 1860 training loss: 0.6924658973801349, test loss: 0.6932480904524916 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5071428571428571\n",
            "Iteration 1861 training loss: 0.6924629682525307, test loss: 0.6932485041938112 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5071428571428571\n",
            "Iteration 1862 training loss: 0.6924600270104971, test loss: 0.6932489195218602 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5142857142857142\n",
            "Iteration 1863 training loss: 0.6924570736073276, test loss: 0.6932493364419009 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5142857142857142\n",
            "Iteration 1864 training loss: 0.6924541079961536, test loss: 0.6932497549592094 \n",
            "train accuracy: 0.5942549371633752, test accuracy: 0.5142857142857142\n",
            "Iteration 1865 training loss: 0.6924511301299449, test loss: 0.693250175079075 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5142857142857142\n",
            "Iteration 1866 training loss: 0.692448139961508, test loss: 0.6932505968068016 \n",
            "train accuracy: 0.5978456014362658, test accuracy: 0.5142857142857142\n",
            "Iteration 1867 training loss: 0.692445137443487, test loss: 0.6932510201477066 \n",
            "train accuracy: 0.5978456014362658, test accuracy: 0.5142857142857142\n",
            "Iteration 1868 training loss: 0.6924421225283617, test loss: 0.6932514451071214 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5142857142857142\n",
            "Iteration 1869 training loss: 0.6924390951684481, test loss: 0.6932518716903916 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5142857142857142\n",
            "Iteration 1870 training loss: 0.6924360553158977, test loss: 0.6932522999028761 \n",
            "train accuracy: 0.5960502692998204, test accuracy: 0.5142857142857142\n",
            "Iteration 1871 training loss: 0.6924330029226966, test loss: 0.6932527297499484 \n",
            "train accuracy: 0.599640933572711, test accuracy: 0.5142857142857142\n",
            "Iteration 1872 training loss: 0.6924299379406651, test loss: 0.693253161236996 \n",
            "train accuracy: 0.599640933572711, test accuracy: 0.5142857142857142\n",
            "Iteration 1873 training loss: 0.6924268603214577, test loss: 0.6932535943694205 \n",
            "train accuracy: 0.599640933572711, test accuracy: 0.5142857142857142\n",
            "Iteration 1874 training loss: 0.692423770016562, test loss: 0.6932540291526373 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1875 training loss: 0.6924206669772983, test loss: 0.6932544655920769 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1876 training loss: 0.6924175511548195, test loss: 0.6932549036931824 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1877 training loss: 0.6924144225001099, test loss: 0.6932553434614128 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5142857142857142\n",
            "Iteration 1878 training loss: 0.6924112809639855, test loss: 0.6932557849022406 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5142857142857142\n",
            "Iteration 1879 training loss: 0.6924081264970924, test loss: 0.6932562280211526 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5142857142857142\n",
            "Iteration 1880 training loss: 0.6924049590499075, test loss: 0.6932566728236499 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1881 training loss: 0.6924017785727372, test loss: 0.6932571193152486 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1882 training loss: 0.6923985850157168, test loss: 0.6932575675014785 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5142857142857142\n",
            "Iteration 1883 training loss: 0.6923953783288107, test loss: 0.6932580173878843 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5142857142857142\n",
            "Iteration 1884 training loss: 0.692392158461811, test loss: 0.6932584689800253 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5142857142857142\n",
            "Iteration 1885 training loss: 0.6923889253643373, test loss: 0.6932589222834752 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1886 training loss: 0.6923856789858367, test loss: 0.693259377303822 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1887 training loss: 0.6923824192755823, test loss: 0.6932598340466691 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1888 training loss: 0.6923791461826737, test loss: 0.6932602925176341 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1889 training loss: 0.6923758596560352, test loss: 0.6932607527223492 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1890 training loss: 0.6923725596444169, test loss: 0.6932612146664616 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1891 training loss: 0.6923692460963922, test loss: 0.6932616783556333 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1892 training loss: 0.6923659189603593, test loss: 0.6932621437955415 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1893 training loss: 0.6923625781845388, test loss: 0.6932626109918775 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1894 training loss: 0.6923592237169748, test loss: 0.6932630799503482 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1895 training loss: 0.6923558555055327, test loss: 0.6932635506766753 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 1896 training loss: 0.6923524734979003, test loss: 0.6932640231765959 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1897 training loss: 0.6923490776415859, test loss: 0.6932644974558612 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1898 training loss: 0.6923456678839186, test loss: 0.6932649735202387 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1899 training loss: 0.6923422441720474, test loss: 0.6932654513755104 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1900 training loss: 0.6923388064529404, test loss: 0.6932659310274738 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1901 training loss: 0.6923353546733848, test loss: 0.6932664124819413 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1902 training loss: 0.6923318887799862, test loss: 0.6932668957447413 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1903 training loss: 0.6923284087191676, test loss: 0.6932673808217166 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1904 training loss: 0.6923249144371688, test loss: 0.6932678677187265 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1905 training loss: 0.6923214058800471, test loss: 0.6932683564416451 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1906 training loss: 0.692317882993675, test loss: 0.6932688469963622 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1907 training loss: 0.6923143457237408, test loss: 0.693269339388783 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1908 training loss: 0.6923107940157474, test loss: 0.6932698336248285 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1909 training loss: 0.6923072278150119, test loss: 0.6932703297104358 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5142857142857142\n",
            "Iteration 1910 training loss: 0.6923036470666657, test loss: 0.6932708276515566 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1911 training loss: 0.6923000517156526, test loss: 0.6932713274541592 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1912 training loss: 0.6922964417067292, test loss: 0.6932718291242278 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 1913 training loss: 0.6922928169844641, test loss: 0.6932723326677621 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1914 training loss: 0.6922891774932374, test loss: 0.6932728380907781 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1915 training loss: 0.6922855231772393, test loss: 0.6932733453993076 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1916 training loss: 0.6922818539804712, test loss: 0.6932738545993978 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1917 training loss: 0.6922781698467433, test loss: 0.6932743656971135 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1918 training loss: 0.6922744707196751, test loss: 0.6932748786985343 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 1919 training loss: 0.6922707565426944, test loss: 0.6932753936097565 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 1920 training loss: 0.6922670272590369, test loss: 0.6932759104368927 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1921 training loss: 0.6922632828117454, test loss: 0.6932764291860721 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1922 training loss: 0.6922595231436696, test loss: 0.6932769498634396 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1923 training loss: 0.6922557481974647, test loss: 0.6932774724751571 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 1924 training loss: 0.6922519579155918, test loss: 0.6932779970274029 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1925 training loss: 0.6922481522403164, test loss: 0.6932785235263719 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1926 training loss: 0.6922443311137088, test loss: 0.693279051978275 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1927 training loss: 0.6922404944776417, test loss: 0.6932795823893413 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1928 training loss: 0.692236642273792, test loss: 0.6932801147658145 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5214285714285715\n",
            "Iteration 1929 training loss: 0.6922327744436384, test loss: 0.693280649113957 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 1930 training loss: 0.6922288909284613, test loss: 0.6932811854400474 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 1931 training loss: 0.6922249916693421, test loss: 0.693281723750381 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5357142857142857\n",
            "Iteration 1932 training loss: 0.6922210766071633, test loss: 0.6932822640512701 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5357142857142857\n",
            "Iteration 1933 training loss: 0.6922171456826063, test loss: 0.6932828063490447 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5357142857142857\n",
            "Iteration 1934 training loss: 0.6922131988361526, test loss: 0.6932833506500512 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5357142857142857\n",
            "Iteration 1935 training loss: 0.692209236008082, test loss: 0.6932838969606536 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5428571428571428\n",
            "Iteration 1936 training loss: 0.6922052571384721, test loss: 0.6932844452872329 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5428571428571428\n",
            "Iteration 1937 training loss: 0.6922012621671982, test loss: 0.6932849956361877 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5428571428571428\n",
            "Iteration 1938 training loss: 0.6921972510339323, test loss: 0.6932855480139342 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5428571428571428\n",
            "Iteration 1939 training loss: 0.692193223678142, test loss: 0.6932861024269052 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5428571428571428\n",
            "Iteration 1940 training loss: 0.6921891800390908, test loss: 0.693286658881552 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5428571428571428\n",
            "Iteration 1941 training loss: 0.6921851200558372, test loss: 0.6932872173843432 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5428571428571428\n",
            "Iteration 1942 training loss: 0.6921810436672333, test loss: 0.6932877779417645 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 1943 training loss: 0.692176950811925, test loss: 0.6932883405603204 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 1944 training loss: 0.6921728414283513, test loss: 0.6932889052465323 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 1945 training loss: 0.6921687154547432, test loss: 0.6932894720069401 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 1946 training loss: 0.6921645728291235, test loss: 0.6932900408481014 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 1947 training loss: 0.6921604134893055, test loss: 0.6932906117765919 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 1948 training loss: 0.6921562373728932, test loss: 0.6932911847990052 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 1949 training loss: 0.6921520444172804, test loss: 0.6932917599219537 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 1950 training loss: 0.6921478345596492, test loss: 0.6932923371520675 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 1951 training loss: 0.6921436077369707, test loss: 0.693292916495995 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 1952 training loss: 0.6921393638860034, test loss: 0.6932934979604037 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 1953 training loss: 0.6921351029432926, test loss: 0.693294081551979 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 1954 training loss: 0.6921308248451703, test loss: 0.693294667277425 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 1955 training loss: 0.692126529527754, test loss: 0.6932952551434648 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 1956 training loss: 0.6921222169269461, test loss: 0.6932958451568397 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 1957 training loss: 0.6921178869784338, test loss: 0.6932964373243102 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 1958 training loss: 0.6921135396176872, test loss: 0.6932970316526558 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 1959 training loss: 0.69210917477996, test loss: 0.6932976281486749 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 1960 training loss: 0.6921047924002881, test loss: 0.6932982268191846 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1961 training loss: 0.6921003924134889, test loss: 0.6932988276710219 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1962 training loss: 0.6920959747541607, test loss: 0.6932994307110425 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1963 training loss: 0.6920915393566824, test loss: 0.6933000359461216 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1964 training loss: 0.6920870861552122, test loss: 0.6933006433831539 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1965 training loss: 0.6920826150836873, test loss: 0.6933012530290537 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1966 training loss: 0.6920781260758233, test loss: 0.6933018648907545 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1967 training loss: 0.6920736190651128, test loss: 0.6933024789752102 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1968 training loss: 0.6920690939848259, test loss: 0.6933030952893938 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1969 training loss: 0.6920645507680085, test loss: 0.6933037138402989 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1970 training loss: 0.6920599893474821, test loss: 0.6933043346349381 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1971 training loss: 0.6920554096558428, test loss: 0.693304957680345 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1972 training loss: 0.6920508116254609, test loss: 0.6933055829835729 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1973 training loss: 0.69204619518848, test loss: 0.693306210551696 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1974 training loss: 0.6920415602768166, test loss: 0.6933068403918077 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1975 training loss: 0.6920369068221586, test loss: 0.6933074725110229 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 1976 training loss: 0.6920322347559658, test loss: 0.6933081069164765 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1977 training loss: 0.692027544009468, test loss: 0.6933087436153247 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1978 training loss: 0.6920228345136652, test loss: 0.6933093826147436 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1979 training loss: 0.6920181061993269, test loss: 0.693310023921931 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1980 training loss: 0.69201335899699, test loss: 0.6933106675441051 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1981 training loss: 0.6920085928369599, test loss: 0.6933113134885055 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1982 training loss: 0.6920038076493088, test loss: 0.6933119617623926 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 1983 training loss: 0.6919990033638751, test loss: 0.6933126123730486 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 1984 training loss: 0.6919941799102628, test loss: 0.693313265327777 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 1985 training loss: 0.691989337217841, test loss: 0.6933139206339026 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 1986 training loss: 0.6919844752157424, test loss: 0.6933145782987717 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 1987 training loss: 0.6919795938328633, test loss: 0.6933152383297527 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1988 training loss: 0.6919746929978632, test loss: 0.6933159007342357 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1989 training loss: 0.6919697726391627, test loss: 0.6933165655196326 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1990 training loss: 0.6919648326849444, test loss: 0.6933172326933776 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1991 training loss: 0.6919598730631504, test loss: 0.6933179022629271 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1992 training loss: 0.6919548937014838, test loss: 0.6933185742357596 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1993 training loss: 0.6919498945274057, test loss: 0.6933192486193761 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1994 training loss: 0.691944875468136, test loss: 0.6933199254213003 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1995 training loss: 0.6919398364506523, test loss: 0.6933206046490783 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1996 training loss: 0.6919347774016883, test loss: 0.6933212863102793 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1997 training loss: 0.6919296982477344, test loss: 0.693321970412495 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1998 training loss: 0.6919245989150367, test loss: 0.6933226569633406 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 1999 training loss: 0.6919194793295946, test loss: 0.693323345970454 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 2000 training loss: 0.6919143394171628, test loss: 0.6933240374414967 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2001 training loss: 0.6919091791032482, test loss: 0.6933247313841536 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2002 training loss: 0.6919039983131106, test loss: 0.6933254278061329 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2003 training loss: 0.6918987969717608, test loss: 0.6933261267151668 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2004 training loss: 0.6918935750039612, test loss: 0.6933268281190111 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2005 training loss: 0.6918883323342238, test loss: 0.6933275320254454 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2006 training loss: 0.6918830688868105, test loss: 0.6933282384422738 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2007 training loss: 0.6918777845857311, test loss: 0.6933289473773241 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2008 training loss: 0.6918724793547442, test loss: 0.6933296588384488 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2009 training loss: 0.6918671531173547, test loss: 0.6933303728335248 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2010 training loss: 0.6918618057968143, test loss: 0.6933310893704535 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2011 training loss: 0.6918564373161206, test loss: 0.6933318084571614 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2012 training loss: 0.691851047598015, test loss: 0.6933325301015996 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2013 training loss: 0.6918456365649842, test loss: 0.6933332543117441 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2014 training loss: 0.6918402041392577, test loss: 0.6933339810955965 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2015 training loss: 0.6918347502428077, test loss: 0.6933347104611834 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5357142857142857\n",
            "Iteration 2016 training loss: 0.691829274797348, test loss: 0.6933354424165573 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5357142857142857\n",
            "Iteration 2017 training loss: 0.6918237777243342, test loss: 0.6933361769697959 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5357142857142857\n",
            "Iteration 2018 training loss: 0.691818258944961, test loss: 0.6933369141290029 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5357142857142857\n",
            "Iteration 2019 training loss: 0.6918127183801637, test loss: 0.6933376539023083 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5285714285714286\n",
            "Iteration 2020 training loss: 0.6918071559506159, test loss: 0.6933383962978673 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2021 training loss: 0.6918015715767295, test loss: 0.6933391413238622 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2022 training loss: 0.6917959651786534, test loss: 0.6933398889885013 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5285714285714286\n",
            "Iteration 2023 training loss: 0.6917903366762732, test loss: 0.6933406393000198 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2024 training loss: 0.6917846859892101, test loss: 0.6933413922666791 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2025 training loss: 0.69177901303682, test loss: 0.6933421478967681 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2026 training loss: 0.6917733177381938, test loss: 0.6933429061986024 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2027 training loss: 0.6917676000121551, test loss: 0.6933436671805249 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2028 training loss: 0.6917618597772602, test loss: 0.6933444308509058 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2029 training loss: 0.6917560969517977, test loss: 0.6933451972181432 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2030 training loss: 0.6917503114537871, test loss: 0.6933459662906626 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2031 training loss: 0.6917445032009781, test loss: 0.6933467380769176 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2032 training loss: 0.6917386721108504, test loss: 0.6933475125853897 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2033 training loss: 0.6917328181006122, test loss: 0.6933482898245888 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2034 training loss: 0.6917269410871998, test loss: 0.6933490698030533 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2035 training loss: 0.6917210409872767, test loss: 0.69334985252935 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2036 training loss: 0.6917151177172333, test loss: 0.6933506380120745 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2037 training loss: 0.6917091711931852, test loss: 0.6933514262598519 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2038 training loss: 0.6917032013309735, test loss: 0.6933522172813358 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2039 training loss: 0.6916972080461631, test loss: 0.6933530110852093 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2040 training loss: 0.6916911912540425, test loss: 0.6933538076801854 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2041 training loss: 0.6916851508696231, test loss: 0.6933546070750065 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2042 training loss: 0.6916790868076375, test loss: 0.6933554092784452 \n",
            "train accuracy: 0.6050269299820467, test accuracy: 0.5214285714285715\n",
            "Iteration 2043 training loss: 0.6916729989825403, test loss: 0.6933562142993036 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2044 training loss: 0.6916668873085059, test loss: 0.6933570221464148 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2045 training loss: 0.6916607516994283, test loss: 0.6933578328286424 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5214285714285715\n",
            "Iteration 2046 training loss: 0.6916545920689208, test loss: 0.6933586463548802 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2047 training loss: 0.6916484083303142, test loss: 0.6933594627340535 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2048 training loss: 0.6916422003966567, test loss: 0.693360281975118 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2049 training loss: 0.6916359681807135, test loss: 0.6933611040870614 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2050 training loss: 0.6916297115949648, test loss: 0.6933619290789027 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2051 training loss: 0.6916234305516066, test loss: 0.6933627569596926 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5214285714285715\n",
            "Iteration 2052 training loss: 0.6916171249625483, test loss: 0.6933635877385139 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2053 training loss: 0.6916107947394136, test loss: 0.6933644214244814 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2054 training loss: 0.6916044397935384, test loss: 0.6933652580267422 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2055 training loss: 0.6915980600359704, test loss: 0.6933660975544765 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2056 training loss: 0.6915916553774689, test loss: 0.6933669400168968 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2057 training loss: 0.6915852257285037, test loss: 0.693367785423249 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2058 training loss: 0.6915787709992535, test loss: 0.693368633782812 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2059 training loss: 0.6915722910996068, test loss: 0.6933694851048985 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2060 training loss: 0.6915657859391598, test loss: 0.6933703393988547 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2061 training loss: 0.6915592554272162, test loss: 0.6933711966740611 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2062 training loss: 0.6915526994727862, test loss: 0.6933720569399319 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2063 training loss: 0.6915461179845861, test loss: 0.6933729202059161 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2064 training loss: 0.6915395108710372, test loss: 0.6933737864814974 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2065 training loss: 0.6915328780402653, test loss: 0.6933746557761944 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2066 training loss: 0.6915262194000996, test loss: 0.6933755280995606 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2067 training loss: 0.6915195348580728, test loss: 0.6933764034611852 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2068 training loss: 0.691512824321419, test loss: 0.6933772818706934 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2069 training loss: 0.691506087697074, test loss: 0.6933781633377455 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2070 training loss: 0.6914993248916745, test loss: 0.6933790478720383 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2071 training loss: 0.6914925358115569, test loss: 0.6933799354833055 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2072 training loss: 0.6914857203627569, test loss: 0.6933808261813167 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2073 training loss: 0.6914788784510086, test loss: 0.6933817199758794 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5285714285714286\n",
            "Iteration 2074 training loss: 0.6914720099817437, test loss: 0.6933826168768373 \n",
            "train accuracy: 0.6068222621184919, test accuracy: 0.5285714285714286\n",
            "Iteration 2075 training loss: 0.6914651148600914, test loss: 0.6933835168940724 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2076 training loss: 0.6914581929908765, test loss: 0.693384420037504 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2077 training loss: 0.6914512442786197, test loss: 0.6933853263170895 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2078 training loss: 0.6914442686275367, test loss: 0.6933862357428248 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2079 training loss: 0.6914372659415369, test loss: 0.693387148324744 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2080 training loss: 0.6914302361242235, test loss: 0.6933880640729204 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2081 training loss: 0.6914231790788921, test loss: 0.6933889829974664 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2082 training loss: 0.6914160947085303, test loss: 0.6933899051085339 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2083 training loss: 0.6914089829158171, test loss: 0.6933908304163143 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2084 training loss: 0.6914018436031218, test loss: 0.6933917589310391 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2085 training loss: 0.6913946766725038, test loss: 0.6933926906629799 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2086 training loss: 0.6913874820257114, test loss: 0.6933936256224492 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2087 training loss: 0.691380259564182, test loss: 0.6933945638198006 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2088 training loss: 0.6913730091890399, test loss: 0.6933955052654284 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2089 training loss: 0.6913657308010966, test loss: 0.6933964499697686 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2090 training loss: 0.6913584243008505, test loss: 0.6933973979432991 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2091 training loss: 0.6913510895884855, test loss: 0.6933983491965401 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2092 training loss: 0.6913437265638704, test loss: 0.6933993037400538 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2093 training loss: 0.6913363351265582, test loss: 0.6934002615844455 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2094 training loss: 0.691328915175786, test loss: 0.6934012227403635 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2095 training loss: 0.6913214666104736, test loss: 0.6934021872184998 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2096 training loss: 0.691313989329223, test loss: 0.6934031550295896 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2097 training loss: 0.6913064832303186, test loss: 0.6934041261844127 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2098 training loss: 0.691298948211725, test loss: 0.6934051006937929 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2099 training loss: 0.6912913841710877, test loss: 0.693406078568599 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2100 training loss: 0.6912837910057317, test loss: 0.6934070598197449 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2101 training loss: 0.6912761686126612, test loss: 0.6934080444581898 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2102 training loss: 0.6912685168885587, test loss: 0.6934090324949389 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2103 training loss: 0.6912608357297849, test loss: 0.693410023941043 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2104 training loss: 0.6912531250323773, test loss: 0.6934110188076005 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2105 training loss: 0.6912453846920502, test loss: 0.6934120171057554 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2106 training loss: 0.691237614604194, test loss: 0.6934130188466996 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2107 training loss: 0.6912298146638737, test loss: 0.6934140240416725 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2108 training loss: 0.6912219847658302, test loss: 0.6934150327019613 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2109 training loss: 0.6912141248044775, test loss: 0.6934160448389017 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2110 training loss: 0.691206234673904, test loss: 0.693417060463878 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2111 training loss: 0.69119831426787, test loss: 0.6934180795883236 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2112 training loss: 0.6911903634798096, test loss: 0.6934191022237214 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2113 training loss: 0.6911823822028275, test loss: 0.6934201283816038 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2114 training loss: 0.6911743703296996, test loss: 0.6934211580735542 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2115 training loss: 0.6911663277528735, test loss: 0.6934221913112059 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2116 training loss: 0.6911582543644659, test loss: 0.6934232281062434 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2117 training loss: 0.6911501500562636, test loss: 0.6934242684704027 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2118 training loss: 0.6911420147197219, test loss: 0.6934253124154721 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2119 training loss: 0.6911338482459649, test loss: 0.6934263599532914 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2120 training loss: 0.6911256505257847, test loss: 0.6934274110957535 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2121 training loss: 0.6911174214496403, test loss: 0.6934284658548039 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2122 training loss: 0.6911091609076578, test loss: 0.6934295242424425 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2123 training loss: 0.6911008687896298, test loss: 0.6934305862707224 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2124 training loss: 0.6910925449850148, test loss: 0.6934316519517512 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2125 training loss: 0.691084189382936, test loss: 0.6934327212976914 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2126 training loss: 0.6910758018721819, test loss: 0.6934337943207605 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2127 training loss: 0.6910673823412055, test loss: 0.6934348710332322 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2128 training loss: 0.6910589306781232, test loss: 0.6934359514474355 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2129 training loss: 0.6910504467707151, test loss: 0.6934370355757571 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2130 training loss: 0.6910419305064243, test loss: 0.6934381234306396 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2131 training loss: 0.691033381772356, test loss: 0.6934392150245836 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2132 training loss: 0.6910248004552776, test loss: 0.6934403103701476 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2133 training loss: 0.6910161864416183, test loss: 0.6934414094799485 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2134 training loss: 0.6910075396174682, test loss: 0.6934425123666624 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2135 training loss: 0.690998859868578, test loss: 0.693443619043024 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2136 training loss: 0.690990147080359, test loss: 0.6934447295218287 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2137 training loss: 0.6909814011378819, test loss: 0.6934458438159317 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2138 training loss: 0.6909726219258777, test loss: 0.6934469619382492 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2139 training loss: 0.6909638093287356, test loss: 0.6934480839017584 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2140 training loss: 0.6909549632305043, test loss: 0.6934492097194989 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2141 training loss: 0.6909460835148902, test loss: 0.693450339404572 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2142 training loss: 0.690937170065258, test loss: 0.6934514729701423 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2143 training loss: 0.6909282227646301, test loss: 0.693452610429438 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2144 training loss: 0.6909192414956857, test loss: 0.69345375179575 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2145 training loss: 0.6909102261407615, test loss: 0.6934548970824346 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2146 training loss: 0.6909011765818507, test loss: 0.6934560463029127 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2147 training loss: 0.6908920927006023, test loss: 0.6934571994706706 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2148 training loss: 0.690882974378322, test loss: 0.6934583565992607 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2149 training loss: 0.6908738214959707, test loss: 0.6934595177023017 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2150 training loss: 0.6908646339341646, test loss: 0.6934606827934796 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2151 training loss: 0.6908554115731755, test loss: 0.693461851886548 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2152 training loss: 0.6908461542929297, test loss: 0.6934630249953283 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2153 training loss: 0.6908368619730083, test loss: 0.693464202133711 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2154 training loss: 0.6908275344926463, test loss: 0.6934653833156558 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2155 training loss: 0.6908181717307336, test loss: 0.6934665685551923 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2156 training loss: 0.6908087735658129, test loss: 0.6934677578664203 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2157 training loss: 0.6907993398760817, test loss: 0.6934689512635113 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2158 training loss: 0.69078987053939, test loss: 0.6934701487607076 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2159 training loss: 0.6907803654332418, test loss: 0.6934713503723239 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2160 training loss: 0.6907708244347939, test loss: 0.693472556112748 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2161 training loss: 0.6907612474208558, test loss: 0.6934737659964411 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2162 training loss: 0.6907516342678901, test loss: 0.6934749800379378 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2163 training loss: 0.6907419848520117, test loss: 0.6934761982518485 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2164 training loss: 0.6907322990489883, test loss: 0.6934774206528574 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2165 training loss: 0.6907225767342398, test loss: 0.693478647255726 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2166 training loss: 0.6907128177828383, test loss: 0.6934798780752909 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2167 training loss: 0.6907030220695081, test loss: 0.6934811131264669 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2168 training loss: 0.6906931894686253, test loss: 0.693482352424246 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2169 training loss: 0.6906833198542187, test loss: 0.6934835959836988 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2170 training loss: 0.6906734130999678, test loss: 0.6934848438199751 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2171 training loss: 0.6906634690792053, test loss: 0.693486095948304 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2172 training loss: 0.6906534876649147, test loss: 0.6934873523839956 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2173 training loss: 0.6906434687297321, test loss: 0.6934886131424403 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2174 training loss: 0.6906334121459445, test loss: 0.6934898782391108 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2175 training loss: 0.6906233177854918, test loss: 0.6934911476895621 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2176 training loss: 0.6906131855199644, test loss: 0.6934924215094321 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5428571428571428\n",
            "Iteration 2177 training loss: 0.6906030152206057, test loss: 0.6934936997144423 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5428571428571428\n",
            "Iteration 2178 training loss: 0.6905928067583106, test loss: 0.6934949823203993 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2179 training loss: 0.6905825600036254, test loss: 0.6934962693431945 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2180 training loss: 0.6905722748267489, test loss: 0.693497560798805 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2181 training loss: 0.6905619510975322, test loss: 0.6934988567032946 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2182 training loss: 0.6905515886854781, test loss: 0.6935001570728148 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5357142857142857\n",
            "Iteration 2183 training loss: 0.6905411874597418, test loss: 0.6935014619236042 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2184 training loss: 0.6905307472891311, test loss: 0.6935027712719913 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2185 training loss: 0.6905202680421064, test loss: 0.6935040851343934 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2186 training loss: 0.6905097495867802, test loss: 0.6935054035273183 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2187 training loss: 0.6904991917909186, test loss: 0.6935067264673644 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2188 training loss: 0.6904885945219407, test loss: 0.6935080539712222 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2189 training loss: 0.6904779576469181, test loss: 0.6935093860556748 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2190 training loss: 0.6904672810325767, test loss: 0.6935107227375981 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5357142857142857\n",
            "Iteration 2191 training loss: 0.6904565645452957, test loss: 0.6935120640339624 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5357142857142857\n",
            "Iteration 2192 training loss: 0.6904458080511082, test loss: 0.693513409961833 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5357142857142857\n",
            "Iteration 2193 training loss: 0.6904350114157017, test loss: 0.69351476053837 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5357142857142857\n",
            "Iteration 2194 training loss: 0.6904241745044182, test loss: 0.6935161157808307 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2195 training loss: 0.6904132971822539, test loss: 0.6935174757065693 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5357142857142857\n",
            "Iteration 2196 training loss: 0.6904023793138606, test loss: 0.693518840333038 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 2197 training loss: 0.6903914207635456, test loss: 0.6935202096777875 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2198 training loss: 0.6903804213952712, test loss: 0.6935215837584685 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2199 training loss: 0.6903693810726566, test loss: 0.6935229625928321 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5428571428571428\n",
            "Iteration 2200 training loss: 0.6903582996589765, test loss: 0.6935243461987303 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5428571428571428\n",
            "Iteration 2201 training loss: 0.6903471770171633, test loss: 0.6935257345941176 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2202 training loss: 0.690336013009806, test loss: 0.6935271277970508 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2203 training loss: 0.6903248074991515, test loss: 0.6935285258256914 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2204 training loss: 0.6903135603471047, test loss: 0.6935299286983045 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2205 training loss: 0.6903022714152289, test loss: 0.6935313364332617 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2206 training loss: 0.6902909405647466, test loss: 0.6935327490490398 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2207 training loss: 0.6902795676565394, test loss: 0.6935341665642235 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2208 training loss: 0.6902681525511493, test loss: 0.6935355889975059 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2209 training loss: 0.690256695108778, test loss: 0.693537016367688 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2210 training loss: 0.6902451951892892, test loss: 0.6935384486936815 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2211 training loss: 0.690233652652207, test loss: 0.6935398859945087 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5428571428571428\n",
            "Iteration 2212 training loss: 0.6902220673567191, test loss: 0.6935413282893031 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2213 training loss: 0.6902104391616741, test loss: 0.6935427755973113 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2214 training loss: 0.6901987679255854, test loss: 0.6935442279378928 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2215 training loss: 0.6901870535066295, test loss: 0.693545685330522 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5428571428571428\n",
            "Iteration 2216 training loss: 0.6901752957626478, test loss: 0.6935471477947881 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2217 training loss: 0.6901634945511465, test loss: 0.6935486153503969 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2218 training loss: 0.6901516497292981, test loss: 0.6935500880171713 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2219 training loss: 0.6901397611539416, test loss: 0.693551565815052 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2220 training loss: 0.6901278286815828, test loss: 0.6935530487640991 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2221 training loss: 0.6901158521683963, test loss: 0.6935545368844923 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.55\n",
            "Iteration 2222 training loss: 0.6901038314702248, test loss: 0.6935560301965327 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2223 training loss: 0.6900917664425806, test loss: 0.6935575287206431 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2224 training loss: 0.6900796569406467, test loss: 0.6935590324773693 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2225 training loss: 0.6900675028192764, test loss: 0.6935605414873809 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2226 training loss: 0.6900553039329957, test loss: 0.6935620557714722 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2227 training loss: 0.6900430601360028, test loss: 0.6935635753505636 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2228 training loss: 0.6900307712821699, test loss: 0.6935651002457023 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.55\n",
            "Iteration 2229 training loss: 0.6900184372250433, test loss: 0.6935666304780633 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 2230 training loss: 0.6900060578178449, test loss: 0.6935681660689504 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 2231 training loss: 0.6899936329134725, test loss: 0.6935697070397977 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5428571428571428\n",
            "Iteration 2232 training loss: 0.6899811623645017, test loss: 0.6935712534121693 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2233 training loss: 0.6899686460231855, test loss: 0.6935728052077622 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2234 training loss: 0.6899560837414568, test loss: 0.6935743624484058 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2235 training loss: 0.6899434753709281, test loss: 0.6935759251560635 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2236 training loss: 0.689930820762893, test loss: 0.693577493352834 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2237 training loss: 0.6899181197683275, test loss: 0.6935790670609518 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2238 training loss: 0.6899053722378904, test loss: 0.6935806463027887 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5428571428571428\n",
            "Iteration 2239 training loss: 0.6898925780219249, test loss: 0.6935822311008549 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2240 training loss: 0.6898797369704601, test loss: 0.6935838214777994 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2241 training loss: 0.6898668489332105, test loss: 0.6935854174564119 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2242 training loss: 0.6898539137595787, test loss: 0.6935870190596234 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2243 training loss: 0.689840931298656, test loss: 0.6935886263105073 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2244 training loss: 0.6898279013992236, test loss: 0.693590239232281 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2245 training loss: 0.6898148239097537, test loss: 0.6935918578483058 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2246 training loss: 0.6898016986784103, test loss: 0.6935934821820897 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2247 training loss: 0.6897885255530517, test loss: 0.6935951122572872 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5357142857142857\n",
            "Iteration 2248 training loss: 0.68977530438123, test loss: 0.6935967480977004 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2249 training loss: 0.6897620350101941, test loss: 0.6935983897272812 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2250 training loss: 0.6897487172868895, test loss: 0.6936000371701312 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2251 training loss: 0.6897353510579605, test loss: 0.6936016904505037 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2252 training loss: 0.6897219361697512, test loss: 0.6936033495928046 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2253 training loss: 0.6897084724683074, test loss: 0.6936050146215931 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2254 training loss: 0.6896949597993765, test loss: 0.6936066855615831 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2255 training loss: 0.6896813980084107, test loss: 0.693608362437645 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2256 training loss: 0.6896677869405673, test loss: 0.6936100452748057 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2257 training loss: 0.6896541264407102, test loss: 0.6936117340982507 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2258 training loss: 0.6896404163534119, test loss: 0.6936134289333248 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2259 training loss: 0.6896266565229542, test loss: 0.6936151298055332 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2260 training loss: 0.6896128467933303, test loss: 0.6936168367405428 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2261 training loss: 0.6895989870082461, test loss: 0.6936185497641839 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2262 training loss: 0.6895850770111217, test loss: 0.6936202689024505 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2263 training loss: 0.6895711166450927, test loss: 0.6936219941815018 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2264 training loss: 0.6895571057530124, test loss: 0.6936237256276637 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2265 training loss: 0.6895430441774529, test loss: 0.6936254632674296 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2266 training loss: 0.6895289317607067, test loss: 0.6936272071274618 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2267 training loss: 0.6895147683447884, test loss: 0.693628957234593 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2268 training loss: 0.6895005537714365, test loss: 0.6936307136158262 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2269 training loss: 0.6894862878821152, test loss: 0.6936324762983381 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2270 training loss: 0.6894719705180155, test loss: 0.6936342453094784 \n",
            "train accuracy: 0.6086175942549371, test accuracy: 0.5285714285714286\n",
            "Iteration 2271 training loss: 0.6894576015200572, test loss: 0.6936360206767718 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2272 training loss: 0.6894431807288912, test loss: 0.6936378024279191 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2273 training loss: 0.6894287079849004, test loss: 0.6936395905907984 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2274 training loss: 0.6894141831282018, test loss: 0.6936413851934667 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2275 training loss: 0.6893996059986484, test loss: 0.6936431862641607 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2276 training loss: 0.6893849764358311, test loss: 0.6936449938312977 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5285714285714286\n",
            "Iteration 2277 training loss: 0.6893702942790799, test loss: 0.693646807923478 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5285714285714286\n",
            "Iteration 2278 training loss: 0.689355559367467, test loss: 0.6936486285694851 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2279 training loss: 0.6893407715398071, test loss: 0.6936504557982872 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2280 training loss: 0.6893259306346605, test loss: 0.6936522896390382 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2281 training loss: 0.6893110364903351, test loss: 0.6936541301210806 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2282 training loss: 0.6892960889448868, test loss: 0.6936559772739439 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2283 training loss: 0.6892810878361233, test loss: 0.6936578311273482 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5285714285714286\n",
            "Iteration 2284 training loss: 0.6892660330016052, test loss: 0.6936596917112046 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2285 training loss: 0.689250924278648, test loss: 0.6936615590556162 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2286 training loss: 0.6892357615043243, test loss: 0.6936634331908802 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 2287 training loss: 0.6892205445154658, test loss: 0.6936653141474882 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 2288 training loss: 0.6892052731486651, test loss: 0.693667201956128 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 2289 training loss: 0.6891899472402785, test loss: 0.693669096647685 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2290 training loss: 0.6891745666264273, test loss: 0.6936709982532431 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2291 training loss: 0.6891591311430005, test loss: 0.6936729068040861 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2292 training loss: 0.6891436406256565, test loss: 0.6936748223316993 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2293 training loss: 0.6891280949098256, test loss: 0.6936767448677703 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2294 training loss: 0.6891124938307122, test loss: 0.6936786744441902 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2295 training loss: 0.6890968372232963, test loss: 0.6936806110930558 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2296 training loss: 0.6890811249223375, test loss: 0.6936825548466696 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2297 training loss: 0.6890653567623747, test loss: 0.6936845057375424 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2298 training loss: 0.6890495325777309, test loss: 0.6936864637983933 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2299 training loss: 0.6890336522025138, test loss: 0.6936884290621517 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2300 training loss: 0.6890177154706184, test loss: 0.6936904015619589 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2301 training loss: 0.6890017222157302, test loss: 0.6936923813311687 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2302 training loss: 0.6889856722713268, test loss: 0.6936943684033489 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2303 training loss: 0.6889695654706799, test loss: 0.6936963628122828 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.5214285714285715\n",
            "Iteration 2304 training loss: 0.688953401646859, test loss: 0.6936983645919699 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2305 training loss: 0.6889371806327326, test loss: 0.6937003737766285 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2306 training loss: 0.6889209022609709, test loss: 0.6937023904006955 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2307 training loss: 0.688904566364049, test loss: 0.6937044144988281 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2308 training loss: 0.6888881727742484, test loss: 0.693706446105906 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2309 training loss: 0.6888717213236601, test loss: 0.6937084852570317 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2310 training loss: 0.6888552118441869, test loss: 0.6937105319875314 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5214285714285715\n",
            "Iteration 2311 training loss: 0.6888386441675458, test loss: 0.693712586332958 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5214285714285715\n",
            "Iteration 2312 training loss: 0.6888220181252709, test loss: 0.6937146483290907 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2313 training loss: 0.6888053335487162, test loss: 0.6937167180119371 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2314 training loss: 0.6887885902690571, test loss: 0.6937187954177343 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2315 training loss: 0.6887717881172942, test loss: 0.6937208805829501 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2316 training loss: 0.6887549269242552, test loss: 0.6937229735442849 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2317 training loss: 0.6887380065205986, test loss: 0.6937250743386715 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2318 training loss: 0.6887210267368147, test loss: 0.6937271830032778 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2319 training loss: 0.6887039874032296, test loss: 0.6937292995755081 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2320 training loss: 0.6886868883500076, test loss: 0.6937314240930029 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2321 training loss: 0.688669729407154, test loss: 0.6937335565936418 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2322 training loss: 0.6886525104045177, test loss: 0.6937356971155442 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2323 training loss: 0.6886352311717939, test loss: 0.6937378456970698 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2324 training loss: 0.6886178915385273, test loss: 0.693740002376821 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2325 training loss: 0.6886004913341143, test loss: 0.6937421671936438 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2326 training loss: 0.6885830303878067, test loss: 0.6937443401866288 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2327 training loss: 0.6885655085287136, test loss: 0.6937465213951124 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2328 training loss: 0.6885479255858052, test loss: 0.6937487108586786 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2329 training loss: 0.6885302813879153, test loss: 0.6937509086171597 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2330 training loss: 0.6885125757637435, test loss: 0.6937531147106377 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2331 training loss: 0.6884948085418596, test loss: 0.6937553291794456 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2332 training loss: 0.6884769795507054, test loss: 0.6937575520641687 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2333 training loss: 0.6884590886185981, test loss: 0.6937597834056457 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2334 training loss: 0.6884411355737335, test loss: 0.6937620232449699 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2335 training loss: 0.6884231202441883, test loss: 0.6937642716234905 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2336 training loss: 0.6884050424579244, test loss: 0.693766528582814 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2337 training loss: 0.6883869020427905, test loss: 0.6937687941648047 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2338 training loss: 0.688368698826526, test loss: 0.6937710684115871 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2339 training loss: 0.6883504326367642, test loss: 0.6937733513655457 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2340 training loss: 0.688332103301035, test loss: 0.6937756430693276 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2341 training loss: 0.6883137106467683, test loss: 0.6937779435658423 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2342 training loss: 0.688295254501297, test loss: 0.6937802528982644 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2343 training loss: 0.6882767346918603, test loss: 0.6937825711100328 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2344 training loss: 0.6882581510456067, test loss: 0.6937848982448545 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2345 training loss: 0.6882395033895977, test loss: 0.6937872343467029 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2346 training loss: 0.6882207915508103, test loss: 0.6937895794598212 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2347 training loss: 0.6882020153561405, test loss: 0.6937919336287223 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2348 training loss: 0.6881831746324073, test loss: 0.6937942968981905 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2349 training loss: 0.6881642692063547, test loss: 0.6937966693132825 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2350 training loss: 0.6881452989046564, test loss: 0.6937990509193286 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2351 training loss: 0.6881262635539176, test loss: 0.6938014417619335 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2352 training loss: 0.6881071629806798, test loss: 0.6938038418869776 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2353 training loss: 0.6880879970114234, test loss: 0.6938062513406185 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2354 training loss: 0.6880687654725706, test loss: 0.6938086701692913 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2355 training loss: 0.6880494681904902, test loss: 0.6938110984197104 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2356 training loss: 0.6880301049914999, test loss: 0.6938135361388703 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2357 training loss: 0.6880106757018698, test loss: 0.6938159833740469 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2358 training loss: 0.6879911801478262, test loss: 0.6938184401727978 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2359 training loss: 0.6879716181555554, test loss: 0.6938209065829645 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2360 training loss: 0.6879519895512063, test loss: 0.6938233826526723 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2361 training loss: 0.6879322941608942, test loss: 0.6938258684303324 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2362 training loss: 0.6879125318107049, test loss: 0.6938283639646423 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2363 training loss: 0.6878927023266976, test loss: 0.6938308693045867 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2364 training loss: 0.6878728055349086, test loss: 0.6938333844994391 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2365 training loss: 0.6878528412613556, test loss: 0.693835909598762 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2366 training loss: 0.6878328093320397, test loss: 0.6938384446524085 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2367 training loss: 0.6878127095729504, test loss: 0.6938409897105234 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2368 training loss: 0.6877925418100691, test loss: 0.6938435448235438 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2369 training loss: 0.6877723058693721, test loss: 0.6938461100421999 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2370 training loss: 0.6877520015768347, test loss: 0.6938486854175162 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2371 training loss: 0.6877316287584346, test loss: 0.6938512710008125 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2372 training loss: 0.6877111872401561, test loss: 0.6938538668437046 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2373 training loss: 0.6876906768479937, test loss: 0.6938564729981059 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2374 training loss: 0.6876700974079549, test loss: 0.693859089516227 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2375 training loss: 0.6876494487460654, test loss: 0.6938617164505779 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2376 training loss: 0.6876287306883719, test loss: 0.6938643538539678 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2377 training loss: 0.6876079430609462, test loss: 0.6938670017795072 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2378 training loss: 0.6875870856898888, test loss: 0.6938696602806069 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2379 training loss: 0.6875661584013333, test loss: 0.6938723294109814 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2380 training loss: 0.6875451610214494, test loss: 0.6938750092246472 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2381 training loss: 0.6875240933764473, test loss: 0.6938776997759248 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2382 training loss: 0.6875029552925812, test loss: 0.69388040111944 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5\n",
            "Iteration 2383 training loss: 0.687481746596154, test loss: 0.6938831133101236 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2384 training loss: 0.6874604671135203, test loss: 0.6938858364032129 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2385 training loss: 0.6874391166710904, test loss: 0.693888570454252 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2386 training loss: 0.6874176950953347, test loss: 0.693891315519093 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2387 training loss: 0.6873962022127875, test loss: 0.6938940716538964 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2388 training loss: 0.6873746378500508, test loss: 0.6938968389151321 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2389 training loss: 0.6873530018337983, test loss: 0.6938996173595793 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2390 training loss: 0.6873312939907799, test loss: 0.6939024070443286 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2391 training loss: 0.6873095141478244, test loss: 0.6939052080267815 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2392 training loss: 0.6872876621318451, test loss: 0.6939080203646515 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2393 training loss: 0.6872657377698431, test loss: 0.6939108441159645 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2394 training loss: 0.6872437408889113, test loss: 0.69391367933906 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2395 training loss: 0.6872216713162385, test loss: 0.6939165260925912 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2396 training loss: 0.6871995288791137, test loss: 0.6939193844355257 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2397 training loss: 0.68717731340493, test loss: 0.693922254427146 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2398 training loss: 0.6871550247211891, test loss: 0.6939251361270502 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2399 training loss: 0.6871326626555045, test loss: 0.6939280295951528 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2400 training loss: 0.6871102270356072, test loss: 0.6939309348916847 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2401 training loss: 0.6870877176893481, test loss: 0.693933852077194 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2402 training loss: 0.6870651344447035, test loss: 0.6939367812125468 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2403 training loss: 0.6870424771297786, test loss: 0.6939397223589272 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2404 training loss: 0.6870197455728123, test loss: 0.6939426755778376 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2405 training loss: 0.6869969396021809, test loss: 0.6939456409311 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2406 training loss: 0.6869740590464019, test loss: 0.6939486184808559 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2407 training loss: 0.6869511037341398, test loss: 0.6939516082895663 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2408 training loss: 0.6869280734942091, test loss: 0.6939546104200129 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2409 training loss: 0.6869049681555786, test loss: 0.6939576249352982 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2410 training loss: 0.6868817875473762, test loss: 0.6939606518988457 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2411 training loss: 0.686858531498893, test loss: 0.6939636913744002 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2412 training loss: 0.6868351998395875, test loss: 0.6939667434260285 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2413 training loss: 0.6868117923990901, test loss: 0.6939698081181194 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.4928571428571429\n",
            "Iteration 2414 training loss: 0.6867883090072076, test loss: 0.6939728855153839 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2415 training loss: 0.6867647494939272, test loss: 0.6939759756828561 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2416 training loss: 0.6867411136894208, test loss: 0.6939790786858926 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2417 training loss: 0.6867174014240499, test loss: 0.6939821945901733 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5\n",
            "Iteration 2418 training loss: 0.6866936125283696, test loss: 0.6939853234617013 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5\n",
            "Iteration 2419 training loss: 0.6866697468331332, test loss: 0.6939884653668035 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5\n",
            "Iteration 2420 training loss: 0.6866458041692963, test loss: 0.6939916203721306 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5\n",
            "Iteration 2421 training loss: 0.6866217843680219, test loss: 0.6939947885446567 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5\n",
            "Iteration 2422 training loss: 0.6865976872606838, test loss: 0.6939979699516805 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2423 training loss: 0.6865735126788723, test loss: 0.6940011646608245 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2424 training loss: 0.6865492604543976, test loss: 0.6940043727400356 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2425 training loss: 0.6865249304192951, test loss: 0.6940075942575851 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2426 training loss: 0.686500522405829, test loss: 0.6940108292820684 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2427 training loss: 0.6864760362464981, test loss: 0.6940140778824059 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2428 training loss: 0.6864514717740385, test loss: 0.6940173401278418 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2429 training loss: 0.6864268288214302, test loss: 0.6940206160879452 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2430 training loss: 0.6864021072219, test loss: 0.6940239058326094 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2431 training loss: 0.6863773068089268, test loss: 0.6940272094320523 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2432 training loss: 0.6863524274162459, test loss: 0.6940305269568162 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2433 training loss: 0.6863274688778542, test loss: 0.6940338584777669 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2434 training loss: 0.6863024310280134, test loss: 0.6940372040660955 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2435 training loss: 0.6862773137012561, test loss: 0.6940405637933159 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2436 training loss: 0.6862521167323896, test loss: 0.6940439377312672 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2437 training loss: 0.6862268399565004, test loss: 0.6940473259521109 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2438 training loss: 0.6862014832089594, test loss: 0.6940507285283329 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2439 training loss: 0.686176046325426, test loss: 0.6940541455327418 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2440 training loss: 0.686150529141853, test loss: 0.6940575770384695 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2441 training loss: 0.6861249314944914, test loss: 0.6940610231189708 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2442 training loss: 0.6860992532198941, test loss: 0.6940644838480229 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2443 training loss: 0.6860734941549222, test loss: 0.6940679592997251 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2444 training loss: 0.6860476541367484, test loss: 0.6940714495484989 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2445 training loss: 0.6860217330028615, test loss: 0.6940749546690869 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2446 training loss: 0.6859957305910727, test loss: 0.6940784747365532 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2447 training loss: 0.6859696467395181, test loss: 0.6940820098262824 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2448 training loss: 0.6859434812866655, test loss: 0.6940855600139798 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2449 training loss: 0.6859172340713174, test loss: 0.6940891253756706 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2450 training loss: 0.6858909049326167, test loss: 0.6940927059876986 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5142857142857142\n",
            "Iteration 2451 training loss: 0.6858644937100513, test loss: 0.6940963019267278 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2452 training loss: 0.6858380002434583, test loss: 0.6940999132697399 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2453 training loss: 0.6858114243730297, test loss: 0.6941035400940346 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2454 training loss: 0.6857847659393158, test loss: 0.6941071824772292 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2455 training loss: 0.6857580247832314, test loss: 0.6941108404972579 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2456 training loss: 0.6857312007460596, test loss: 0.6941145142323704 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2457 training loss: 0.6857042936694568, test loss: 0.6941182037611325 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2458 training loss: 0.6856773033954575, test loss: 0.6941219091624248 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2459 training loss: 0.6856502297664787, test loss: 0.694125630515442 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2460 training loss: 0.6856230726253256, test loss: 0.6941293678996926 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2461 training loss: 0.6855958318151953, test loss: 0.6941331213949974 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2462 training loss: 0.6855685071796825, test loss: 0.6941368910814896 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2463 training loss: 0.6855410985627837, test loss: 0.6941406770396136 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2464 training loss: 0.6855136058089015, test loss: 0.6941444793501242 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2465 training loss: 0.6854860287628509, test loss: 0.6941482980940862 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2466 training loss: 0.6854583672698631, test loss: 0.6941521333528721 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2467 training loss: 0.6854306211755898, test loss: 0.6941559852081639 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2468 training loss: 0.6854027903261091, test loss: 0.6941598537419491 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2469 training loss: 0.6853748745679299, test loss: 0.6941637390365224 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2470 training loss: 0.6853468737479962, test loss: 0.6941676411744827 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2471 training loss: 0.6853187877136927, test loss: 0.6941715602387339 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2472 training loss: 0.6852906163128488, test loss: 0.6941754963124825 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2473 training loss: 0.6852623593937444, test loss: 0.6941794494792374 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2474 training loss: 0.6852340168051135, test loss: 0.6941834198228081 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2475 training loss: 0.6852055883961502, test loss: 0.6941874074273046 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2476 training loss: 0.6851770740165125, test loss: 0.6941914123771358 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2477 training loss: 0.6851484735163279, test loss: 0.6941954347570077 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2478 training loss: 0.6851197867461976, test loss: 0.6941994746519237 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2479 training loss: 0.6850910135572017, test loss: 0.6942035321471819 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2480 training loss: 0.6850621538009039, test loss: 0.6942076073283752 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2481 training loss: 0.6850332073293561, test loss: 0.6942117002813886 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2482 training loss: 0.6850041739951036, test loss: 0.6942158110923998 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2483 training loss: 0.6849750536511897, test loss: 0.6942199398478763 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2484 training loss: 0.6849458461511602, test loss: 0.6942240866345745 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2485 training loss: 0.6849165513490685, test loss: 0.6942282515395389 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2486 training loss: 0.6848871690994809, test loss: 0.6942324346501004 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2487 training loss: 0.6848576992574801, test loss: 0.6942366360538745 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2488 training loss: 0.6848281416786711, test loss: 0.6942408558387604 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2489 training loss: 0.684798496219186, test loss: 0.6942450940929392 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2490 training loss: 0.6847687627356877, test loss: 0.694249350904873 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2491 training loss: 0.6847389410853757, test loss: 0.6942536263633027 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5285714285714286\n",
            "Iteration 2492 training loss: 0.6847090311259906, test loss: 0.6942579205572468 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5285714285714286\n",
            "Iteration 2493 training loss: 0.684679032715819, test loss: 0.6942622335759993 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5285714285714286\n",
            "Iteration 2494 training loss: 0.6846489457136973, test loss: 0.6942665655091295 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2495 training loss: 0.6846187699790179, test loss: 0.6942709164464786 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2496 training loss: 0.6845885053717333, test loss: 0.6942752864781596 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2497 training loss: 0.6845581517523607, test loss: 0.6942796756945544 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2498 training loss: 0.6845277089819863, test loss: 0.694284084186313 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2499 training loss: 0.6844971769222713, test loss: 0.6942885120443515 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2500 training loss: 0.6844665554354554, test loss: 0.6942929593598494 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2501 training loss: 0.6844358443843621, test loss: 0.6942974262242496 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2502 training loss: 0.6844050436324037, test loss: 0.6943019127292552 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2503 training loss: 0.6843741530435846, test loss: 0.6943064189668283 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5285714285714286\n",
            "Iteration 2504 training loss: 0.6843431724825081, test loss: 0.6943109450291879 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2505 training loss: 0.6843121018143792, test loss: 0.6943154910088077 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2506 training loss: 0.6842809409050101, test loss: 0.694320056998415 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2507 training loss: 0.6842496896208251, test loss: 0.6943246430909878 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2508 training loss: 0.6842183478288645, test loss: 0.6943292493797535 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2509 training loss: 0.6841869153967897, test loss: 0.6943338759581863 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2510 training loss: 0.684155392192888, test loss: 0.6943385229200059 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2511 training loss: 0.6841237780860767, test loss: 0.6943431903591746 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2512 training loss: 0.6840920729459079, test loss: 0.6943478783698961 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2513 training loss: 0.6840602766425736, test loss: 0.6943525870466125 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5214285714285715\n",
            "Iteration 2514 training loss: 0.6840283890469093, test loss: 0.6943573164840023 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2515 training loss: 0.6839964100303992, test loss: 0.6943620667769792 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2516 training loss: 0.6839643394651809, test loss: 0.6943668380206883 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2517 training loss: 0.6839321772240493, test loss: 0.694371630310505 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2518 training loss: 0.6838999231804616, test loss: 0.6943764437420323 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2519 training loss: 0.6838675772085417, test loss: 0.6943812784110988 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2520 training loss: 0.6838351391830847, test loss: 0.6943861344137555 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5214285714285715\n",
            "Iteration 2521 training loss: 0.6838026089795611, test loss: 0.6943910118462749 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2522 training loss: 0.683769986474122, test loss: 0.6943959108051471 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2523 training loss: 0.6837372715436023, test loss: 0.6944008313870784 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2524 training loss: 0.6837044640655263, test loss: 0.6944057736889881 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2525 training loss: 0.6836715639181117, test loss: 0.6944107378080068 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2526 training loss: 0.6836385709802738, test loss: 0.6944157238414734 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5214285714285715\n",
            "Iteration 2527 training loss: 0.68360548513163, test loss: 0.6944207318869323 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2528 training loss: 0.6835723062525043, test loss: 0.6944257620421318 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2529 training loss: 0.6835390342239313, test loss: 0.6944308144050201 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2530 training loss: 0.683505668927661, test loss: 0.694435889073744 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2531 training loss: 0.6834722102461624, test loss: 0.6944409861466451 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2532 training loss: 0.6834386580626286, test loss: 0.6944461057222583 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2533 training loss: 0.6834050122609806, test loss: 0.6944512478993078 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2534 training loss: 0.6833712727258713, test loss: 0.6944564127767056 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5142857142857142\n",
            "Iteration 2535 training loss: 0.6833374393426902, test loss: 0.6944616004535478 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2536 training loss: 0.6833035119975673, test loss: 0.6944668110291116 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2537 training loss: 0.6832694905773776, test loss: 0.6944720446028537 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2538 training loss: 0.6832353749697448, test loss: 0.6944773012744064 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2539 training loss: 0.6832011650630455, test loss: 0.6944825811435749 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2540 training loss: 0.6831668607464138, test loss: 0.6944878843103341 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2541 training loss: 0.6831324619097446, test loss: 0.6944932108748264 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2542 training loss: 0.6830979684436985, test loss: 0.6944985609373583 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2543 training loss: 0.6830633802397054, test loss: 0.6945039345983967 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2544 training loss: 0.6830286971899676, test loss: 0.6945093319585671 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2545 training loss: 0.682993919187466, test loss: 0.6945147531186496 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2546 training loss: 0.6829590461259621, test loss: 0.6945201981795761 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2547 training loss: 0.6829240779000028, test loss: 0.6945256672424273 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2548 training loss: 0.6828890144049234, test loss: 0.6945311604084285 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2549 training loss: 0.6828538555368533, test loss: 0.6945366777789486 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2550 training loss: 0.6828186011927178, test loss: 0.6945422194554944 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2551 training loss: 0.6827832512702432, test loss: 0.6945477855397089 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2552 training loss: 0.6827478056679603, test loss: 0.694553376133367 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2553 training loss: 0.6827122642852079, test loss: 0.6945589913383733 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2554 training loss: 0.6826766270221372, test loss: 0.6945646312567576 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2555 training loss: 0.6826408937797142, test loss: 0.6945702959906725 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2556 training loss: 0.682605064459725, test loss: 0.6945759856423891 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2557 training loss: 0.6825691389647787, test loss: 0.6945817003142942 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2558 training loss: 0.6825331171983102, test loss: 0.6945874401088862 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2559 training loss: 0.6824969990645856, test loss: 0.6945932051287727 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2560 training loss: 0.682460784468704, test loss: 0.6945989954766654 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5142857142857142\n",
            "Iteration 2561 training loss: 0.6824244733166022, test loss: 0.6946048112553777 \n",
            "train accuracy: 0.6104129263913824, test accuracy: 0.5071428571428571\n",
            "Iteration 2562 training loss: 0.6823880655150575, test loss: 0.6946106525678204 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2563 training loss: 0.6823515609716918, test loss: 0.6946165195169992 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2564 training loss: 0.6823149595949745, test loss: 0.6946224122060093 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2565 training loss: 0.6822782612942258, test loss: 0.6946283307380333 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2566 training loss: 0.6822414659796204, test loss: 0.6946342752163362 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2567 training loss: 0.6822045735621916, test loss: 0.6946402457442629 \n",
            "train accuracy: 0.6122082585278277, test accuracy: 0.5071428571428571\n",
            "Iteration 2568 training loss: 0.6821675839538326, test loss: 0.6946462424252333 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2569 training loss: 0.6821304970673013, test loss: 0.694652265362739 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2570 training loss: 0.6820933128162238, test loss: 0.6946583146603398 \n",
            "train accuracy: 0.6140035906642729, test accuracy: 0.5071428571428571\n",
            "Iteration 2571 training loss: 0.6820560311150958, test loss: 0.694664390421659 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2572 training loss: 0.6820186518792879, test loss: 0.6946704927503798 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2573 training loss: 0.6819811750250472, test loss: 0.694676621750242 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2574 training loss: 0.6819436004695012, test loss: 0.6946827775250372 \n",
            "train accuracy: 0.6157989228007181, test accuracy: 0.5071428571428571\n",
            "Iteration 2575 training loss: 0.6819059281306601, test loss: 0.6946889601786053 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2576 training loss: 0.6818681579274206, test loss: 0.6946951698148297 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.5071428571428571\n",
            "Iteration 2577 training loss: 0.6818302897795686, test loss: 0.6947014065376352 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2578 training loss: 0.6817923236077819, test loss: 0.6947076704509807 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2579 training loss: 0.6817542593336333, test loss: 0.6947139616588587 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2580 training loss: 0.6817160968795933, test loss: 0.6947202802652882 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2581 training loss: 0.6816778361690333, test loss: 0.6947266263743123 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2582 training loss: 0.6816394771262281, test loss: 0.6947330000899934 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2583 training loss: 0.6816010196763582, test loss: 0.6947394015164088 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2584 training loss: 0.6815624637455135, test loss: 0.6947458307576468 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2585 training loss: 0.6815238092606952, test loss: 0.6947522879178021 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2586 training loss: 0.6814850561498187, test loss: 0.6947587731009719 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2587 training loss: 0.6814462043417159, test loss: 0.6947652864112509 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2588 training loss: 0.6814072537661382, test loss: 0.6947718279527275 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2589 training loss: 0.6813682043537587, test loss: 0.6947783978294795 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2590 training loss: 0.6813290560361743, test loss: 0.6947849961455688 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2591 training loss: 0.6812898087459093, test loss: 0.694791623005038 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2592 training loss: 0.6812504624164162, test loss: 0.6947982785119052 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2593 training loss: 0.6812110169820792, test loss: 0.6948049627701595 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2594 training loss: 0.6811714723782161, test loss: 0.6948116758837574 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2595 training loss: 0.68113182854108, test loss: 0.6948184179566164 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2596 training loss: 0.6810920854078628, test loss: 0.6948251890926124 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2597 training loss: 0.6810522429166957, test loss: 0.6948319893955736 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2598 training loss: 0.681012301006653, test loss: 0.6948388189692766 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2599 training loss: 0.6809722596177525, test loss: 0.6948456779174416 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2600 training loss: 0.6809321186909588, test loss: 0.6948525663437274 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2601 training loss: 0.680891878168185, test loss: 0.6948594843517272 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2602 training loss: 0.6808515379922935, test loss: 0.6948664320449631 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2603 training loss: 0.6808110981070999, test loss: 0.6948734095268817 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2604 training loss: 0.6807705584573728, test loss: 0.6948804169008497 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5071428571428571\n",
            "Iteration 2605 training loss: 0.6807299189888371, test loss: 0.6948874542701482 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2606 training loss: 0.6806891796481744, test loss: 0.6948945217379684 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2607 training loss: 0.6806483403830266, test loss: 0.6949016194074068 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2608 training loss: 0.6806074011419951, test loss: 0.6949087473814597 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2609 training loss: 0.6805663618746445, test loss: 0.6949159057630185 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2610 training loss: 0.6805252225315026, test loss: 0.6949230946548652 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2611 training loss: 0.6804839830640632, test loss: 0.6949303141596671 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2612 training loss: 0.6804426434247866, test loss: 0.694937564379971 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2613 training loss: 0.6804012035671011, test loss: 0.6949448454181996 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2614 training loss: 0.680359663445405, test loss: 0.6949521573766448 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2615 training loss: 0.6803180230150672, test loss: 0.6949595003574645 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2616 training loss: 0.6802762822324286, test loss: 0.6949668744626752 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2617 training loss: 0.6802344410548039, test loss: 0.6949742797941495 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.5\n",
            "Iteration 2618 training loss: 0.6801924994404817, test loss: 0.6949817164536082 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2619 training loss: 0.6801504573487263, test loss: 0.6949891845426169 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2620 training loss: 0.6801083147397784, test loss: 0.6949966841625805 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2621 training loss: 0.680066071574857, test loss: 0.6950042154147374 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2622 training loss: 0.6800237278161586, test loss: 0.6950117784001547 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2623 training loss: 0.6799812834268599, test loss: 0.6950193732197228 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2624 training loss: 0.6799387383711176, test loss: 0.6950269999741499 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2625 training loss: 0.679896092614069, test loss: 0.6950346587639569 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2626 training loss: 0.6798533461218342, test loss: 0.6950423496894717 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2627 training loss: 0.6798104988615146, test loss: 0.6950500728508244 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2628 training loss: 0.6797675508011954, test loss: 0.6950578283479418 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2629 training loss: 0.6797245019099454, test loss: 0.695065616280541 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2630 training loss: 0.6796813521578171, test loss: 0.695073436748125 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2631 training loss: 0.6796381015158484, test loss: 0.6950812898499775 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2632 training loss: 0.6795947499560615, test loss: 0.6950891756851556 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2633 training loss: 0.6795512974514647, test loss: 0.6950970943524867 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2634 training loss: 0.6795077439760517, test loss: 0.6951050459505611 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2635 training loss: 0.6794640895048024, test loss: 0.6951130305777273 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2636 training loss: 0.6794203340136826, test loss: 0.6951210483320868 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2637 training loss: 0.6793764774796447, test loss: 0.6951290993114867 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2638 training loss: 0.6793325198806275, test loss: 0.6951371836135166 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2639 training loss: 0.6792884611955565, test loss: 0.6951453013355013 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2640 training loss: 0.6792443014043432, test loss: 0.6951534525744956 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2641 training loss: 0.679200040487886, test loss: 0.6951616374272784 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2642 training loss: 0.6791556784280691, test loss: 0.6951698559903475 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2643 training loss: 0.6791112152077636, test loss: 0.6951781083599136 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2644 training loss: 0.6790666508108255, test loss: 0.6951863946318948 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2645 training loss: 0.6790219852220971, test loss: 0.6951947149019102 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2646 training loss: 0.6789772184274055, test loss: 0.6952030692652753 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2647 training loss: 0.6789323504135631, test loss: 0.695211457816995 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2648 training loss: 0.6788873811683664, test loss: 0.6952198806517587 \n",
            "train accuracy: 0.6175942549371634, test accuracy: 0.4928571428571429\n",
            "Iteration 2649 training loss: 0.6788423106805958, test loss: 0.695228337863934 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2650 training loss: 0.6787971389400147, test loss: 0.6952368295475614 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2651 training loss: 0.67875186593737, test loss: 0.6952453557963476 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2652 training loss: 0.6787064916643898, test loss: 0.695253916703661 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2653 training loss: 0.678661016113784, test loss: 0.6952625123625242 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2654 training loss: 0.6786154392792428, test loss: 0.6952711428656098 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2655 training loss: 0.6785697611554361, test loss: 0.6952798083052327 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2656 training loss: 0.6785239817380122, test loss: 0.695288508773346 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2657 training loss: 0.6784781010235978, test loss: 0.6952972443615341 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2658 training loss: 0.678432119009796, test loss: 0.6953060151610067 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2659 training loss: 0.6783860356951855, test loss: 0.6953148212625933 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2660 training loss: 0.6783398510793199, test loss: 0.695323662756737 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2661 training loss: 0.678293565162726, test loss: 0.6953325397334887 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2662 training loss: 0.6782471779469028, test loss: 0.6953414522825012 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2663 training loss: 0.6782006894343203, test loss: 0.6953504004930222 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2664 training loss: 0.6781540996284176, test loss: 0.6953593844538902 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2665 training loss: 0.6781074085336022, test loss: 0.695368404253527 \n",
            "train accuracy: 0.6193895870736086, test accuracy: 0.4928571428571429\n",
            "Iteration 2666 training loss: 0.6780606161552484, test loss: 0.6953774599799324 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2667 training loss: 0.6780137224996953, test loss: 0.6953865517206773 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2668 training loss: 0.6779667275742457, test loss: 0.6953956795628988 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2669 training loss: 0.6779196313871644, test loss: 0.6954048435932937 \n",
            "train accuracy: 0.6211849192100538, test accuracy: 0.4928571428571429\n",
            "Iteration 2670 training loss: 0.6778724339476763, test loss: 0.6954140438981121 \n",
            "train accuracy: 0.6229802513464991, test accuracy: 0.4928571428571429\n",
            "Iteration 2671 training loss: 0.6778251352659647, test loss: 0.695423280563152 \n",
            "train accuracy: 0.6229802513464991, test accuracy: 0.4928571428571429\n",
            "Iteration 2672 training loss: 0.6777777353531698, test loss: 0.6954325536737526 \n",
            "train accuracy: 0.6247755834829444, test accuracy: 0.4928571428571429\n",
            "Iteration 2673 training loss: 0.6777302342213865, test loss: 0.695441863314789 \n",
            "train accuracy: 0.6247755834829444, test accuracy: 0.4928571428571429\n",
            "Iteration 2674 training loss: 0.6776826318836625, test loss: 0.6954512095706648 \n",
            "train accuracy: 0.6247755834829444, test accuracy: 0.4928571428571429\n",
            "Iteration 2675 training loss: 0.6776349283539966, test loss: 0.6954605925253078 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2676 training loss: 0.6775871236473368, test loss: 0.6954700122621625 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2677 training loss: 0.6775392177795773, test loss: 0.6954794688641849 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2678 training loss: 0.6774912107675575, test loss: 0.6954889624138354 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2679 training loss: 0.6774431026290592, test loss: 0.6954984929930741 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2680 training loss: 0.6773948933828042, test loss: 0.6955080606833534 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4928571428571429\n",
            "Iteration 2681 training loss: 0.6773465830484529, test loss: 0.6955176655656123 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4857142857142857\n",
            "Iteration 2682 training loss: 0.6772981716466004, test loss: 0.6955273077202709 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4857142857142857\n",
            "Iteration 2683 training loss: 0.6772496591987759, test loss: 0.6955369872272239 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2684 training loss: 0.6772010457274387, test loss: 0.6955467041658336 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2685 training loss: 0.6771523312559767, test loss: 0.6955564586149254 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2686 training loss: 0.6771035158087032, test loss: 0.6955662506527808 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2687 training loss: 0.6770545994108546, test loss: 0.6955760803571309 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2688 training loss: 0.6770055820885874, test loss: 0.695585947805151 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2689 training loss: 0.6769564638689761, test loss: 0.6955958530734545 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2690 training loss: 0.6769072447800095, test loss: 0.6956057962380862 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2691 training loss: 0.6768579248505887, test loss: 0.695615777374517 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2692 training loss: 0.6768085041105234, test loss: 0.6956257965576366 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2693 training loss: 0.67675898259053, test loss: 0.695635853861749 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2694 training loss: 0.6767093603222274, test loss: 0.6956459493605646 \n",
            "train accuracy: 0.6265709156193896, test accuracy: 0.4857142857142857\n",
            "Iteration 2695 training loss: 0.6766596373381345, test loss: 0.695656083127196 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2696 training loss: 0.6766098136716673, test loss: 0.6956662552341504 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2697 training loss: 0.6765598893571356, test loss: 0.695676465753324 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2698 training loss: 0.6765098644297395, test loss: 0.6956867147559962 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2699 training loss: 0.676459738925566, test loss: 0.6956970023128232 \n",
            "train accuracy: 0.6283662477558348, test accuracy: 0.4857142857142857\n",
            "Iteration 2700 training loss: 0.6764095128815865, test loss: 0.6957073284938323 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2701 training loss: 0.6763591863356526, test loss: 0.6957176933684152 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2702 training loss: 0.676308759326493, test loss: 0.6957280970053223 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2703 training loss: 0.67625823189371, test loss: 0.695738539472657 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2704 training loss: 0.676207604077776, test loss: 0.695749020837869 \n",
            "train accuracy: 0.63016157989228, test accuracy: 0.4857142857142857\n",
            "Iteration 2705 training loss: 0.6761568759200296, test loss: 0.6957595411677491 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2706 training loss: 0.6761060474626726, test loss: 0.6957701005284219 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2707 training loss: 0.6760551187487654, test loss: 0.6957806989853411 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2708 training loss: 0.6760040898222245, test loss: 0.6957913366032827 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2709 training loss: 0.6759529607278172, test loss: 0.6958020134463397 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2710 training loss: 0.6759017315111588, test loss: 0.6958127295779152 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2711 training loss: 0.675850402218709, test loss: 0.6958234850607168 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2712 training loss: 0.6757989728977667, test loss: 0.6958342799567514 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2713 training loss: 0.6757474435964671, test loss: 0.695845114327318 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2714 training loss: 0.6756958143637771, test loss: 0.6958559882330025 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2715 training loss: 0.6756440852494917, test loss: 0.6958669017336723 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4928571428571429\n",
            "Iteration 2716 training loss: 0.6755922563042293, test loss: 0.6958778548884684 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4928571428571429\n",
            "Iteration 2717 training loss: 0.6755403275794278, test loss: 0.6958888477558025 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4928571428571429\n",
            "Iteration 2718 training loss: 0.6754882991273408, test loss: 0.695899880393348 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4928571428571429\n",
            "Iteration 2719 training loss: 0.6754361710010319, test loss: 0.6959109528580368 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4928571428571429\n",
            "Iteration 2720 training loss: 0.6753839432543726, test loss: 0.6959220652060517 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2721 training loss: 0.6753316159420357, test loss: 0.6959332174928212 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4928571428571429\n",
            "Iteration 2722 training loss: 0.6752791891194923, test loss: 0.695944409773014 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2723 training loss: 0.6752266628430069, test loss: 0.6959556421005323 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2724 training loss: 0.6751740371696331, test loss: 0.695966914528507 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2725 training loss: 0.675121312157208, test loss: 0.6959782271092918 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2726 training loss: 0.6750684878643497, test loss: 0.695989579894457 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2727 training loss: 0.6750155643504501, test loss: 0.6960009729347837 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2728 training loss: 0.6749625416756727, test loss: 0.696012406280259 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2729 training loss: 0.6749094199009457, test loss: 0.6960238799800692 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2730 training loss: 0.6748561990879588, test loss: 0.6960353940825954 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2731 training loss: 0.6748028792991579, test loss: 0.6960469486354064 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2732 training loss: 0.6747494605977394, test loss: 0.6960585436852545 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2733 training loss: 0.6746959430476468, test loss: 0.6960701792780695 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2734 training loss: 0.6746423267135646, test loss: 0.6960818554589521 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2735 training loss: 0.6745886116609138, test loss: 0.6960935722721698 \n",
            "train accuracy: 0.6319569120287253, test accuracy: 0.4857142857142857\n",
            "Iteration 2736 training loss: 0.6745347979558464, test loss: 0.696105329761151 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2737 training loss: 0.6744808856652412, test loss: 0.6961171279684794 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2738 training loss: 0.6744268748566982, test loss: 0.6961289669358875 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2739 training loss: 0.6743727655985331, test loss: 0.6961408467042537 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2740 training loss: 0.6743185579597725, test loss: 0.6961527673135944 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2741 training loss: 0.6742642520101488, test loss: 0.6961647288030597 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2742 training loss: 0.6742098478200947, test loss: 0.6961767312109283 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2743 training loss: 0.6741553454607379, test loss: 0.6961887745746016 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2744 training loss: 0.6741007450038959, test loss: 0.6962008589305989 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2745 training loss: 0.6740460465220706, test loss: 0.6962129843145517 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2746 training loss: 0.6739912500884424, test loss: 0.6962251507611988 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2747 training loss: 0.6739363557768658, test loss: 0.6962373583043814 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2748 training loss: 0.673881363661863, test loss: 0.6962496069770365 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2749 training loss: 0.6738262738186184, test loss: 0.6962618968111942 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2750 training loss: 0.6737710863229738, test loss: 0.6962742278379701 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2751 training loss: 0.6737158012514218, test loss: 0.6962866000875622 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2752 training loss: 0.673660418681101, test loss: 0.696299013589244 \n",
            "train accuracy: 0.6337522441651705, test accuracy: 0.4857142857142857\n",
            "Iteration 2753 training loss: 0.6736049386897904, test loss: 0.696311468371362 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2754 training loss: 0.6735493613559025, test loss: 0.6963239644613279 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2755 training loss: 0.673493686758479, test loss: 0.6963365018856162 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2756 training loss: 0.6734379149771843, test loss: 0.6963490806697572 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2757 training loss: 0.6733820460922999, test loss: 0.6963617008383338 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2758 training loss: 0.6733260801847185, test loss: 0.6963743624149755 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2759 training loss: 0.6732700173359383, test loss: 0.6963870654223547 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2760 training loss: 0.673213857628057, test loss: 0.6963998098821809 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2761 training loss: 0.6731576011437657, test loss: 0.6964125958151968 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2762 training loss: 0.6731012479663435, test loss: 0.6964254232411731 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2763 training loss: 0.6730447981796511, test loss: 0.6964382921789042 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2764 training loss: 0.6729882518681249, test loss: 0.6964512026462035 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2765 training loss: 0.6729316091167711, test loss: 0.696464154659899 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2766 training loss: 0.6728748700111592, test loss: 0.6964771482358284 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2767 training loss: 0.6728180346374167, test loss: 0.6964901833888351 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2768 training loss: 0.6727611030822226, test loss: 0.6965032601327636 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2769 training loss: 0.6727040754328009, test loss: 0.6965163784804551 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2770 training loss: 0.6726469517769151, test loss: 0.6965295384437431 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2771 training loss: 0.6725897322028613, test loss: 0.6965427400334491 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2772 training loss: 0.672532416799463, test loss: 0.6965559832593786 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2773 training loss: 0.6724750056560641, test loss: 0.6965692681303169 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2774 training loss: 0.6724174988625229, test loss: 0.6965825946540244 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2775 training loss: 0.6723598965092056, test loss: 0.6965959628372329 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2776 training loss: 0.6723021986869802, test loss: 0.6966093726856418 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2777 training loss: 0.6722444054872108, test loss: 0.6966228242039136 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2778 training loss: 0.67218651700175, test loss: 0.6966363173956702 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2779 training loss: 0.6721285333229334, test loss: 0.6966498522634884 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2780 training loss: 0.6720704545435736, test loss: 0.6966634288088973 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2781 training loss: 0.6720122807569524, test loss: 0.6966770470323728 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2782 training loss: 0.671954012056816, test loss: 0.6966907069333356 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2783 training loss: 0.6718956485373678, test loss: 0.6967044085101455 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2784 training loss: 0.6718371902932613, test loss: 0.6967181517600997 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2785 training loss: 0.6717786374195953, test loss: 0.6967319366794276 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2786 training loss: 0.6717199900119062, test loss: 0.6967457632632881 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2787 training loss: 0.6716612481661615, test loss: 0.6967596315057658 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2788 training loss: 0.671602411978754, test loss: 0.6967735413998675 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2789 training loss: 0.6715434815464947, test loss: 0.6967874929375185 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2790 training loss: 0.6714844569666066, test loss: 0.69680148610956 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2791 training loss: 0.671425338336718, test loss: 0.6968155209057451 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2792 training loss: 0.671366125754856, test loss: 0.6968295973147353 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2793 training loss: 0.67130681931944, test loss: 0.6968437153240982 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2794 training loss: 0.6712474191292749, test loss: 0.6968578749203037 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2795 training loss: 0.6711879252835445, test loss: 0.6968720760887208 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2796 training loss: 0.6711283378818058, test loss: 0.6968863188136147 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2797 training loss: 0.6710686570239808, test loss: 0.6969006030781443 \n",
            "train accuracy: 0.6355475763016158, test accuracy: 0.4857142857142857\n",
            "Iteration 2798 training loss: 0.6710088828103513, test loss: 0.6969149288643581 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2799 training loss: 0.6709490153415516, test loss: 0.6969292961531927 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2800 training loss: 0.6708890547185618, test loss: 0.6969437049244686 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2801 training loss: 0.6708290010427018, test loss: 0.6969581551568884 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2802 training loss: 0.6707688544156237, test loss: 0.6969726468280336 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2803 training loss: 0.6707086149393062, test loss: 0.6969871799143623 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2804 training loss: 0.6706482827160472, test loss: 0.6970017543912063 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2805 training loss: 0.6705878578484571, test loss: 0.6970163702327681 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2806 training loss: 0.6705273404394527, test loss: 0.6970310274121195 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2807 training loss: 0.6704667305922505, test loss: 0.6970457259011984 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2808 training loss: 0.6704060284103592, test loss: 0.6970604656708065 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2809 training loss: 0.6703452339975741, test loss: 0.6970752466906072 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2810 training loss: 0.6702843474579695, test loss: 0.6970900689291228 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2811 training loss: 0.6702233688958926, test loss: 0.6971049323537333 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2812 training loss: 0.670162298415957, test loss: 0.6971198369306731 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2813 training loss: 0.6701011361230352, test loss: 0.6971347826250303 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2814 training loss: 0.6700398821222529, test loss: 0.6971497694007427 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2815 training loss: 0.6699785365189813, test loss: 0.6971647972205979 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2816 training loss: 0.6699170994188315, test loss: 0.69717986604623 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2817 training loss: 0.6698555709276474, test loss: 0.6971949758381186 \n",
            "train accuracy: 0.6373429084380611, test accuracy: 0.4857142857142857\n",
            "Iteration 2818 training loss: 0.6697939511514983, test loss: 0.6972101265555866 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2819 training loss: 0.6697322401966733, test loss: 0.6972253181567981 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2820 training loss: 0.6696704381696745, test loss: 0.6972405505987583 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2821 training loss: 0.6696085451772095, test loss: 0.6972558238373096 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2822 training loss: 0.6695465613261858, test loss: 0.6972711378271321 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2823 training loss: 0.6694844867237036, test loss: 0.697286492521741 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2824 training loss: 0.6694223214770489, test loss: 0.6973018878734858 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2825 training loss: 0.6693600656936878, test loss: 0.6973173238335485 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2826 training loss: 0.6692977194812588, test loss: 0.6973328003519423 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2827 training loss: 0.6692352829475667, test loss: 0.6973483173775114 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2828 training loss: 0.6691727562005767, test loss: 0.6973638748579281 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2829 training loss: 0.669110139348406, test loss: 0.6973794727396934 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2830 training loss: 0.6690474324993192, test loss: 0.697395110968135 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2831 training loss: 0.6689846357617203, test loss: 0.6974107894874065 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2832 training loss: 0.6689217492441472, test loss: 0.6974265082404871 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2833 training loss: 0.6688587730552638, test loss: 0.69744226716918 \n",
            "train accuracy: 0.6391382405745063, test accuracy: 0.4857142857142857\n",
            "Iteration 2834 training loss: 0.6687957073038547, test loss: 0.6974580662141118 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2835 training loss: 0.6687325520988187, test loss: 0.6974739053147322 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2836 training loss: 0.668669307549161, test loss: 0.6974897844093133 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2837 training loss: 0.668605973763988, test loss: 0.6975057034349484 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2838 training loss: 0.6685425508525003, test loss: 0.6975216623275526 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2839 training loss: 0.668479038923986, test loss: 0.6975376610218613 \n",
            "train accuracy: 0.6409335727109515, test accuracy: 0.4857142857142857\n",
            "Iteration 2840 training loss: 0.6684154380878148, test loss: 0.6975536994514308 \n",
            "train accuracy: 0.6427289048473968, test accuracy: 0.4857142857142857\n",
            "Iteration 2841 training loss: 0.6683517484534311, test loss: 0.6975697775486367 \n",
            "train accuracy: 0.6427289048473968, test accuracy: 0.4857142857142857\n",
            "Iteration 2842 training loss: 0.6682879701303479, test loss: 0.6975858952446753 \n",
            "train accuracy: 0.6427289048473968, test accuracy: 0.4857142857142857\n",
            "Iteration 2843 training loss: 0.6682241032281399, test loss: 0.697602052469562 \n",
            "train accuracy: 0.6427289048473968, test accuracy: 0.4857142857142857\n",
            "Iteration 2844 training loss: 0.6681601478564375, test loss: 0.6976182491521323 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2845 training loss: 0.6680961041249207, test loss: 0.6976344852200413 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2846 training loss: 0.6680319721433121, test loss: 0.6976507605997634 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2847 training loss: 0.6679677520213708, test loss: 0.6976670752165929 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2848 training loss: 0.6679034438688864, test loss: 0.6976834289946443 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2849 training loss: 0.6678390477956724, test loss: 0.6976998218568516 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2850 training loss: 0.6677745639115596, test loss: 0.6977162537249699 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2851 training loss: 0.6677099923263906, test loss: 0.6977327245195748 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2852 training loss: 0.6676453331500132, test loss: 0.6977492341600635 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2853 training loss: 0.6675805864922738, test loss: 0.6977657825646546 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2854 training loss: 0.6675157524630119, test loss: 0.6977823696503894 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2855 training loss: 0.6674508311720535, test loss: 0.697798995333132 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2856 training loss: 0.6673858227292051, test loss: 0.6978156595275705 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2857 training loss: 0.6673207272442475, test loss: 0.6978323621472171 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2858 training loss: 0.6672555448269297, test loss: 0.6978491031044097 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2859 training loss: 0.6671902755869629, test loss: 0.6978658823103125 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2860 training loss: 0.6671249196340145, test loss: 0.6978826996749167 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2861 training loss: 0.6670594770777017, test loss: 0.697899555107042 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2862 training loss: 0.6669939480275865, test loss: 0.6979164485143377 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2863 training loss: 0.6669283325931683, test loss: 0.6979333798032832 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2864 training loss: 0.6668626308838791, test loss: 0.69795034887919 \n",
            "train accuracy: 0.644524236983842, test accuracy: 0.4857142857142857\n",
            "Iteration 2865 training loss: 0.666796843009077, test loss: 0.6979673556462032 \n",
            "train accuracy: 0.6463195691202872, test accuracy: 0.4857142857142857\n",
            "Iteration 2866 training loss: 0.6667309690780406, test loss: 0.6979844000073019 \n",
            "train accuracy: 0.6463195691202872, test accuracy: 0.4857142857142857\n",
            "Iteration 2867 training loss: 0.6666650091999632, test loss: 0.6980014818643017 \n",
            "train accuracy: 0.6463195691202872, test accuracy: 0.4857142857142857\n",
            "Iteration 2868 training loss: 0.6665989634839465, test loss: 0.6980186011178552 \n",
            "train accuracy: 0.6463195691202872, test accuracy: 0.4857142857142857\n",
            "Iteration 2869 training loss: 0.6665328320389954, test loss: 0.6980357576674547 \n",
            "train accuracy: 0.6463195691202872, test accuracy: 0.4857142857142857\n",
            "Iteration 2870 training loss: 0.6664666149740115, test loss: 0.6980529514114332 \n",
            "train accuracy: 0.6481149012567325, test accuracy: 0.4857142857142857\n",
            "Iteration 2871 training loss: 0.6664003123977886, test loss: 0.698070182246966 \n",
            "train accuracy: 0.6481149012567325, test accuracy: 0.4857142857142857\n",
            "Iteration 2872 training loss: 0.6663339244190055, test loss: 0.698087450070073 \n",
            "train accuracy: 0.6481149012567325, test accuracy: 0.4857142857142857\n",
            "Iteration 2873 training loss: 0.6662674511462213, test loss: 0.6981047547756201 \n",
            "train accuracy: 0.6481149012567325, test accuracy: 0.4857142857142857\n",
            "Iteration 2874 training loss: 0.6662008926878695, test loss: 0.6981220962573214 \n",
            "train accuracy: 0.6481149012567325, test accuracy: 0.4857142857142857\n",
            "Iteration 2875 training loss: 0.6661342491522522, test loss: 0.6981394744077412 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2876 training loss: 0.6660675206475349, test loss: 0.6981568891182954 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2877 training loss: 0.6660007072817407, test loss: 0.6981743402792554 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2878 training loss: 0.6659338091627445, test loss: 0.6981918277797478 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2879 training loss: 0.6658668263982683, test loss: 0.6982093515077588 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2880 training loss: 0.6657997590958751, test loss: 0.6982269113501354 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2881 training loss: 0.6657326073629634, test loss: 0.698244507192588 \n",
            "train accuracy: 0.6499102333931778, test accuracy: 0.4857142857142857\n",
            "Iteration 2882 training loss: 0.6656653713067625, test loss: 0.6982621389196931 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2883 training loss: 0.6655980510343267, test loss: 0.6982798064148956 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2884 training loss: 0.6655306466525297, test loss: 0.6982975095605114 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2885 training loss: 0.6654631582680597, test loss: 0.6983152482377303 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2886 training loss: 0.6653955859874144, test loss: 0.6983330223266183 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2887 training loss: 0.6653279299168957, test loss: 0.6983508317061207 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2888 training loss: 0.6652601901626035, test loss: 0.6983686762540648 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2889 training loss: 0.6651923668304321, test loss: 0.6983865558471634 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4857142857142857\n",
            "Iteration 2890 training loss: 0.6651244600260643, test loss: 0.6984044703610163 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2891 training loss: 0.6650564698549664, test loss: 0.6984224196701155 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2892 training loss: 0.6649883964223829, test loss: 0.698440403647846 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2893 training loss: 0.6649202398333327, test loss: 0.6984584221664908 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2894 training loss: 0.6648520001926025, test loss: 0.6984764750972333 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2895 training loss: 0.664783677604743, test loss: 0.698494562310161 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2896 training loss: 0.6647152721740639, test loss: 0.698512683674268 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2897 training loss: 0.6646467840046287, test loss: 0.6985308390574594 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2898 training loss: 0.6645782132002505, test loss: 0.6985490283265546 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2899 training loss: 0.6645095598644862, test loss: 0.6985672513472906 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2900 training loss: 0.6644408241006333, test loss: 0.6985855079843256 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2901 training loss: 0.6643720060117239, test loss: 0.6986037981012427 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4857142857142857\n",
            "Iteration 2902 training loss: 0.6643031057005205, test loss: 0.6986221215605539 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4857142857142857\n",
            "Iteration 2903 training loss: 0.6642341232695123, test loss: 0.6986404782237041 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4857142857142857\n",
            "Iteration 2904 training loss: 0.664165058820909, test loss: 0.6986588679510738 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2905 training loss: 0.6640959124566377, test loss: 0.6986772906019846 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2906 training loss: 0.6640266842783374, test loss: 0.6986957460347016 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2907 training loss: 0.6639573743873558, test loss: 0.6987142341064393 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2908 training loss: 0.6638879828847436, test loss: 0.6987327546733637 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2909 training loss: 0.663818509871251, test loss: 0.6987513075905981 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2910 training loss: 0.6637489554473233, test loss: 0.6987698927122263 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2911 training loss: 0.6636793197130965, test loss: 0.6987885098912977 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2912 training loss: 0.6636096027683929, test loss: 0.6988071589798306 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2913 training loss: 0.6635398047127177, test loss: 0.6988258398288173 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2914 training loss: 0.6634699256452538, test loss: 0.6988445522882292 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2915 training loss: 0.6633999656648584, test loss: 0.6988632962070196 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2916 training loss: 0.6633299248700593, test loss: 0.6988820714331295 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2917 training loss: 0.6632598033590499, test loss: 0.6989008778134929 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2918 training loss: 0.6631896012296862, test loss: 0.6989197151940395 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2919 training loss: 0.6631193185794821, test loss: 0.6989385834197009 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2920 training loss: 0.6630489555056066, test loss: 0.6989574823344153 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2921 training loss: 0.6629785121048787, test loss: 0.6989764117811317 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2922 training loss: 0.6629079884737646, test loss: 0.6989953716018156 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2923 training loss: 0.6628373847083738, test loss: 0.6990143616374531 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2924 training loss: 0.6627667009044549, test loss: 0.6990333817280571 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2925 training loss: 0.6626959371573926, test loss: 0.6990524317126711 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2926 training loss: 0.6626250935622038, test loss: 0.6990715114293752 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2927 training loss: 0.6625541702135344, test loss: 0.6990906207152909 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2928 training loss: 0.662483167205655, test loss: 0.6991097594065868 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2929 training loss: 0.6624120846324586, test loss: 0.6991289273384831 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2930 training loss: 0.6623409225874557, test loss: 0.6991481243452579 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2931 training loss: 0.6622696811637729, test loss: 0.6991673502602513 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2932 training loss: 0.6621983604541477, test loss: 0.6991866049158726 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2933 training loss: 0.6621269605509263, test loss: 0.6992058881436042 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2934 training loss: 0.6620554815460602, test loss: 0.6992251997740082 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2935 training loss: 0.6619839235311026, test loss: 0.6992445396367312 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2936 training loss: 0.661912286597206, test loss: 0.6992639075605106 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2937 training loss: 0.6618405708351186, test loss: 0.6992833033731801 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2938 training loss: 0.6617687763351813, test loss: 0.6993027269016754 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2939 training loss: 0.6616969031873245, test loss: 0.6993221779720397 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2940 training loss: 0.6616249514810665, test loss: 0.69934165640943 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2941 training loss: 0.6615529213055081, test loss: 0.6993611620381233 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2942 training loss: 0.6614808127493323, test loss: 0.6993806946815212 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2943 training loss: 0.6614086259008, test loss: 0.6994002541621577 \n",
            "train accuracy: 0.6535008976660682, test accuracy: 0.4785714285714286\n",
            "Iteration 2944 training loss: 0.6613363608477476, test loss: 0.6994198403017036 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2945 training loss: 0.6612640176775844, test loss: 0.6994394529209733 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2946 training loss: 0.6611915964772898, test loss: 0.6994590918399313 \n",
            "train accuracy: 0.6552962298025135, test accuracy: 0.4785714285714286\n",
            "Iteration 2947 training loss: 0.6611190973334107, test loss: 0.699478756877698 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2948 training loss: 0.6610465203320592, test loss: 0.6994984478525552 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2949 training loss: 0.6609738655589095, test loss: 0.6995181645819539 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2950 training loss: 0.6609011330991961, test loss: 0.6995379068825189 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2951 training loss: 0.6608283230377104, test loss: 0.6995576745700568 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2952 training loss: 0.6607554354587999, test loss: 0.6995774674595608 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2953 training loss: 0.6606824704463639, test loss: 0.6995972853652185 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2954 training loss: 0.6606094280838528, test loss: 0.6996171281004174 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2955 training loss: 0.6605363084542654, test loss: 0.6996369954777517 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2956 training loss: 0.6604631116401457, test loss: 0.6996568873090293 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2957 training loss: 0.6603898377235825, test loss: 0.6996768034052778 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2958 training loss: 0.6603164867862061, test loss: 0.699696743576751 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2959 training loss: 0.6602430589091862, test loss: 0.6997167076329364 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2960 training loss: 0.6601695541732305, test loss: 0.6997366953825612 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2961 training loss: 0.6600959726585826, test loss: 0.6997567066335991 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2962 training loss: 0.6600223144450192, test loss: 0.6997767411932774 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2963 training loss: 0.6599485796118499, test loss: 0.6997967988680835 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2964 training loss: 0.6598747682379137, test loss: 0.6998168794637722 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2965 training loss: 0.6598008804015782, test loss: 0.6998369827853721 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2966 training loss: 0.6597269161807375, test loss: 0.6998571086371923 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2967 training loss: 0.6596528756528105, test loss: 0.6998772568228303 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2968 training loss: 0.6595787588947394, test loss: 0.6998974271451784 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2969 training loss: 0.6595045659829882, test loss: 0.6999176194064306 \n",
            "train accuracy: 0.6570915619389587, test accuracy: 0.4785714285714286\n",
            "Iteration 2970 training loss: 0.6594302969935404, test loss: 0.6999378334080897 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2971 training loss: 0.6593559520018989, test loss: 0.6999580689509749 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2972 training loss: 0.659281531083083, test loss: 0.6999783258352282 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2973 training loss: 0.6592070343116277, test loss: 0.6999986038603225 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2974 training loss: 0.6591324617615828, test loss: 0.7000189028250674 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2975 training loss: 0.6590578135065105, test loss: 0.7000392225276176 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2976 training loss: 0.6589830896194848, test loss: 0.70005956276548 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2977 training loss: 0.6589082901730903, test loss: 0.7000799233355202 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2978 training loss: 0.6588334152394206, test loss: 0.7001003040339706 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2979 training loss: 0.6587584648900775, test loss: 0.7001207046564377 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2980 training loss: 0.6586834391961697, test loss: 0.7001411249979089 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2981 training loss: 0.6586083382283118, test loss: 0.7001615648527604 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2982 training loss: 0.6585331620566229, test loss: 0.7001820240147641 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2983 training loss: 0.6584579107507268, test loss: 0.7002025022770959 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2984 training loss: 0.6583825843797494, test loss: 0.7002229994323422 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2985 training loss: 0.6583071830123194, test loss: 0.7002435152725082 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2986 training loss: 0.6582317067165657, test loss: 0.7002640495890244 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2987 training loss: 0.6581561555601187, test loss: 0.7002846021727553 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2988 training loss: 0.6580805296101079, test loss: 0.7003051728140067 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4714285714285714\n",
            "Iteration 2989 training loss: 0.6580048289331614, test loss: 0.7003257613025323 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2990 training loss: 0.657929053595406, test loss: 0.7003463674275423 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2991 training loss: 0.6578532036624661, test loss: 0.7003669909777112 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2992 training loss: 0.6577772791994629, test loss: 0.7003876317411841 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2993 training loss: 0.6577012802710143, test loss: 0.7004082895055864 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2994 training loss: 0.6576252069412335, test loss: 0.7004289640580296 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2995 training loss: 0.6575490592737302, test loss: 0.7004496551851196 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 2996 training loss: 0.6574728373316083, test loss: 0.7004703626729653 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2997 training loss: 0.6573965411774667, test loss: 0.7004910863071853 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2998 training loss: 0.6573201708733987, test loss: 0.7005118258729158 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 2999 training loss: 0.6572437264809914, test loss: 0.700532581154819 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 3000 training loss: 0.6571672080613257, test loss: 0.7005533519370902 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 3001 training loss: 0.6570906156749763, test loss: 0.700574138003466 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 3002 training loss: 0.6570139493820111, test loss: 0.7005949391372324 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 3003 training loss: 0.6569372092419908, test loss: 0.7006157551212315 \n",
            "train accuracy: 0.6588868940754039, test accuracy: 0.4785714285714286\n",
            "Iteration 3004 training loss: 0.6568603953139699, test loss: 0.7006365857378709 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3005 training loss: 0.6567835076564955, test loss: 0.7006574307691302 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3006 training loss: 0.6567065463276079, test loss: 0.7006782899965701 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3007 training loss: 0.6566295113848406, test loss: 0.7006991632013392 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3008 training loss: 0.6565524028852195, test loss: 0.7007200501641827 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3009 training loss: 0.6564752208852647, test loss: 0.70074095066545 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3010 training loss: 0.6563979654409887, test loss: 0.7007618644851026 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3011 training loss: 0.6563206366078982, test loss: 0.7007827914027221 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3012 training loss: 0.6562432344409931, test loss: 0.7008037311975185 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3013 training loss: 0.6561657589947676, test loss: 0.7008246836483374 \n",
            "train accuracy: 0.6606822262118492, test accuracy: 0.4785714285714286\n",
            "Iteration 3014 training loss: 0.6560882103232099, test loss: 0.7008456485336688 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3015 training loss: 0.6560105884798028, test loss: 0.7008666256316542 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3016 training loss: 0.655932893517524, test loss: 0.7008876147200958 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3017 training loss: 0.6558551254888468, test loss: 0.7009086155764631 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3018 training loss: 0.6557772844457398, test loss: 0.7009296279779023 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3019 training loss: 0.655699370439668, test loss: 0.7009506517012428 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3020 training loss: 0.655621383521593, test loss: 0.7009716865230068 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3021 training loss: 0.6555433237419741, test loss: 0.7009927322194163 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3022 training loss: 0.6554651911507677, test loss: 0.7010137885664011 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3023 training loss: 0.6553869857974292, test loss: 0.7010348553396075 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4785714285714286\n",
            "Iteration 3024 training loss: 0.6553087077309125, test loss: 0.7010559323144059 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3025 training loss: 0.6552303569996722, test loss: 0.7010770192658985 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3026 training loss: 0.6551519336516625, test loss: 0.7010981159689286 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3027 training loss: 0.655073437734339, test loss: 0.7011192221980869 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3028 training loss: 0.6549948692946594, test loss: 0.7011403377277209 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3029 training loss: 0.6549162283790847, test loss: 0.7011614623319425 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3030 training loss: 0.6548375150335783, test loss: 0.7011825957846362 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3031 training loss: 0.6547587293036097, test loss: 0.7012037378594661 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3032 training loss: 0.6546798712341525, test loss: 0.7012248883298862 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3033 training loss: 0.6546009408696877, test loss: 0.7012460469691458 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3034 training loss: 0.6545219382542029, test loss: 0.7012672135503001 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3035 training loss: 0.6544428634311947, test loss: 0.7012883878462157 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3036 training loss: 0.654363716443669, test loss: 0.701309569629581 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4785714285714286\n",
            "Iteration 3037 training loss: 0.6542844973341421, test loss: 0.7013307586729128 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3038 training loss: 0.6542052061446416, test loss: 0.7013519547485643 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3039 training loss: 0.6541258429167087, test loss: 0.7013731576287343 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3040 training loss: 0.6540464076913979, test loss: 0.701394367085474 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3041 training loss: 0.6539669005092786, test loss: 0.7014155828906955 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3042 training loss: 0.6538873214104374, test loss: 0.7014368048161801 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3043 training loss: 0.6538076704344776, test loss: 0.7014580326335861 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3044 training loss: 0.6537279476205223, test loss: 0.7014792661144564 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3045 training loss: 0.6536481530072139, test loss: 0.701500505030227 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3046 training loss: 0.653568286632717, test loss: 0.7015217491522352 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3047 training loss: 0.6534883485347189, test loss: 0.7015429982517266 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3048 training loss: 0.6534083387504311, test loss: 0.7015642520998645 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3049 training loss: 0.6533282573165912, test loss: 0.7015855104677367 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3050 training loss: 0.6532481042694636, test loss: 0.7016067731263637 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3051 training loss: 0.6531678796448418, test loss: 0.7016280398467071 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3052 training loss: 0.6530875834780493, test loss: 0.7016493103996772 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3053 training loss: 0.6530072158039415, test loss: 0.7016705845561412 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3054 training loss: 0.6529267766569068, test loss: 0.7016918620869307 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3055 training loss: 0.6528462660708689, test loss: 0.7017131427628499 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3056 training loss: 0.652765684079288, test loss: 0.7017344263546836 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3057 training loss: 0.6526850307151622, test loss: 0.7017557126332049 \n",
            "train accuracy: 0.6624775583482945, test accuracy: 0.4857142857142857\n",
            "Iteration 3058 training loss: 0.6526043060110296, test loss: 0.7017770013691829 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3059 training loss: 0.6525235099989698, test loss: 0.7017982923333907 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3060 training loss: 0.6524426427106056, test loss: 0.7018195852966145 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3061 training loss: 0.6523617041771053, test loss: 0.7018408800296587 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3062 training loss: 0.6522806944291832, test loss: 0.7018621763033562 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3063 training loss: 0.6521996134971029, test loss: 0.7018834738885754 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3064 training loss: 0.6521184614106775, test loss: 0.7019047725562276 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3065 training loss: 0.6520372381992734, test loss: 0.7019260720772749 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3066 training loss: 0.6519559438918103, test loss: 0.7019473722227388 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3067 training loss: 0.6518745785167639, test loss: 0.7019686727637071 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4785714285714286\n",
            "Iteration 3068 training loss: 0.6517931421021683, test loss: 0.7019899734713417 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3069 training loss: 0.6517116346756171, test loss: 0.7020112741168865 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3070 training loss: 0.6516300562642654, test loss: 0.7020325744716751 \n",
            "train accuracy: 0.6642728904847397, test accuracy: 0.4857142857142857\n",
            "Iteration 3071 training loss: 0.6515484068948325, test loss: 0.7020538743071384 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3072 training loss: 0.6514666865936032, test loss: 0.7020751733948128 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3073 training loss: 0.6513848953864299, test loss: 0.7020964715063468 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3074 training loss: 0.6513030332987351, test loss: 0.7021177684135095 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3075 training loss: 0.6512211003555131, test loss: 0.7021390638881976 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3076 training loss: 0.6511390965813316, test loss: 0.7021603577024436 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3077 training loss: 0.6510570220003353, test loss: 0.7021816496284233 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3078 training loss: 0.6509748766362462, test loss: 0.7022029394384626 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3079 training loss: 0.6508926605123667, test loss: 0.7022242269050457 \n",
            "train accuracy: 0.6660682226211849, test accuracy: 0.4857142857142857\n",
            "Iteration 3080 training loss: 0.6508103736515822, test loss: 0.7022455118008228 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3081 training loss: 0.6507280160763621, test loss: 0.7022667938986172 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3082 training loss: 0.6506455878087628, test loss: 0.7022880729714323 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3083 training loss: 0.6505630888704297, test loss: 0.7023093487924598 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3084 training loss: 0.6504805192825995, test loss: 0.7023306211350873 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3085 training loss: 0.6503978790661026, test loss: 0.702351889772905 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4857142857142857\n",
            "Iteration 3086 training loss: 0.6503151682413646, test loss: 0.7023731544797132 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3087 training loss: 0.6502323868284097, test loss: 0.7023944150295302 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3088 training loss: 0.6501495348468621, test loss: 0.7024156711965988 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.5\n",
            "Iteration 3089 training loss: 0.6500666123159492, test loss: 0.7024369227553944 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3090 training loss: 0.6499836192545027, test loss: 0.7024581694806321 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3091 training loss: 0.6499005556809617, test loss: 0.7024794111472734 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3092 training loss: 0.6498174216133759, test loss: 0.7025006475305341 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3093 training loss: 0.649734217069406, test loss: 0.7025218784058909 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3094 training loss: 0.649650942066328, test loss: 0.7025431035490897 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3095 training loss: 0.6495675966210342, test loss: 0.7025643227361511 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3096 training loss: 0.6494841807500366, test loss: 0.7025855357433792 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3097 training loss: 0.6494006944694689, test loss: 0.7026067423473675 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3098 training loss: 0.6493171377950891, test loss: 0.7026279423250064 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3099 training loss: 0.6492335107422816, test loss: 0.7026491354534906 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3100 training loss: 0.6491498133260606, test loss: 0.7026703215103256 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3101 training loss: 0.6490660455610711, test loss: 0.7026915002733348 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3102 training loss: 0.6489822074615929, test loss: 0.7027126715206666 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3103 training loss: 0.6488982990415424, test loss: 0.702733835030802 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3104 training loss: 0.6488143203144755, test loss: 0.7027549905825597 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3105 training loss: 0.6487302712935892, test loss: 0.7027761379551051 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3106 training loss: 0.6486461519917256, test loss: 0.7027972769279556 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3107 training loss: 0.6485619624213732, test loss: 0.7028184072809884 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3108 training loss: 0.6484777025946703, test loss: 0.7028395287944467 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3109 training loss: 0.6483933725234073, test loss: 0.7028606412489469 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3110 training loss: 0.6483089722190292, test loss: 0.7028817444254851 \n",
            "train accuracy: 0.6678635547576302, test accuracy: 0.5\n",
            "Iteration 3111 training loss: 0.6482245016926382, test loss: 0.7029028381054435 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.5\n",
            "Iteration 3112 training loss: 0.648139960954997, test loss: 0.7029239220705983 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.5\n",
            "Iteration 3113 training loss: 0.6480553500165305, test loss: 0.7029449961031247 \n",
            "train accuracy: 0.6696588868940754, test accuracy: 0.5\n",
            "Iteration 3114 training loss: 0.6479706688873287, test loss: 0.7029660599856048 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3115 training loss: 0.6478859175771498, test loss: 0.7029871135010333 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3116 training loss: 0.6478010960954224, test loss: 0.7030081564328252 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3117 training loss: 0.6477162044512489, test loss: 0.7030291885648209 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3118 training loss: 0.6476312426534068, test loss: 0.703050209681294 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3119 training loss: 0.647546210710353, test loss: 0.7030712195669567 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3120 training loss: 0.6474611086302252, test loss: 0.7030922180069672 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3121 training loss: 0.6473759364208457, test loss: 0.7031132047869355 \n",
            "train accuracy: 0.6714542190305206, test accuracy: 0.5\n",
            "Iteration 3122 training loss: 0.6472906940897228, test loss: 0.7031341796929299 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3123 training loss: 0.6472053816440552, test loss: 0.7031551425114834 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3124 training loss: 0.6471199990907336, test loss: 0.7031760930295997 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3125 training loss: 0.6470345464363433, test loss: 0.7031970310347605 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3126 training loss: 0.646949023687168, test loss: 0.7032179563149303 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3127 training loss: 0.6468634308491913, test loss: 0.7032388686585637 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3128 training loss: 0.6467777679281007, test loss: 0.7032597678546112 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3129 training loss: 0.6466920349292894, test loss: 0.7032806536925251 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3130 training loss: 0.6466062318578595, test loss: 0.7033015259622662 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3131 training loss: 0.6465203587186248, test loss: 0.7033223844543095 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3132 training loss: 0.6464344155161136, test loss: 0.7033432289596502 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3133 training loss: 0.6463484022545715, test loss: 0.7033640592698097 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3134 training loss: 0.6462623189379638, test loss: 0.7033848751768421 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3135 training loss: 0.6461761655699793, test loss: 0.7034056764733395 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3136 training loss: 0.6460899421540318, test loss: 0.7034264629524383 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3137 training loss: 0.6460036486932641, test loss: 0.7034472344078248 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3138 training loss: 0.6459172851905504, test loss: 0.703467990633741 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3139 training loss: 0.6458308516484983, test loss: 0.7034887314249915 \n",
            "train accuracy: 0.6750448833034112, test accuracy: 0.5\n",
            "Iteration 3140 training loss: 0.6457443480694534, test loss: 0.7035094565769472 \n",
            "train accuracy: 0.6750448833034112, test accuracy: 0.5\n",
            "Iteration 3141 training loss: 0.6456577744555003, test loss: 0.703530165885553 \n",
            "train accuracy: 0.6750448833034112, test accuracy: 0.5\n",
            "Iteration 3142 training loss: 0.6455711308084673, test loss: 0.7035508591473325 \n",
            "train accuracy: 0.6750448833034112, test accuracy: 0.5\n",
            "Iteration 3143 training loss: 0.6454844171299272, test loss: 0.7035715361593938 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3144 training loss: 0.6453976334212019, test loss: 0.7035921967194357 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3145 training loss: 0.6453107796833643, test loss: 0.7036128406257522 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3146 training loss: 0.6452238559172413, test loss: 0.7036334676772391 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3147 training loss: 0.645136862123417, test loss: 0.703654077673399 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3148 training loss: 0.6450497983022356, test loss: 0.703674670414347 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3149 training loss: 0.6449626644538036, test loss: 0.7036952457008165 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3150 training loss: 0.6448754605779929, test loss: 0.7037158033341632 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3151 training loss: 0.6447881866744448, test loss: 0.7037363431163725 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3152 training loss: 0.644700842742571, test loss: 0.7037568648500633 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3153 training loss: 0.644613428781558, test loss: 0.7037773683384941 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3154 training loss: 0.6445259447903692, test loss: 0.7037978533855681 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3155 training loss: 0.6444383907677481, test loss: 0.7038183197958385 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3156 training loss: 0.644350766712221, test loss: 0.7038387673745133 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3157 training loss: 0.6442630726221, test loss: 0.7038591959274612 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3158 training loss: 0.6441753084954861, test loss: 0.7038796052612158 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3159 training loss: 0.6440874743302714, test loss: 0.7038999951829819 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3160 training loss: 0.6439995701241427, test loss: 0.7039203655006392 \n",
            "train accuracy: 0.6732495511669659, test accuracy: 0.5\n",
            "Iteration 3161 training loss: 0.6439115958745845, test loss: 0.7039407160227485 \n",
            "train accuracy: 0.6750448833034112, test accuracy: 0.5\n",
            "Iteration 3162 training loss: 0.6438235515788806, test loss: 0.7039610465585563 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3163 training loss: 0.643735437234119, test loss: 0.7039813569179993 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3164 training loss: 0.643647252837193, test loss: 0.7040016469117102 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3165 training loss: 0.6435589983848052, test loss: 0.7040219163510214 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3166 training loss: 0.6434706738734697, test loss: 0.7040421650479711 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3167 training loss: 0.6433822792995155, test loss: 0.7040623928153074 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3168 training loss: 0.6432938146590892, test loss: 0.7040825994664929 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3169 training loss: 0.6432052799481576, test loss: 0.7041027848157104 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3170 training loss: 0.6431166751625115, test loss: 0.7041229486778665 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3171 training loss: 0.6430280002977671, test loss: 0.7041430908685967 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3172 training loss: 0.6429392553493705, test loss: 0.7041632112042702 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3173 training loss: 0.6428504403125996, test loss: 0.7041833095019943 \n",
            "train accuracy: 0.6768402154398564, test accuracy: 0.5\n",
            "Iteration 3174 training loss: 0.6427615551825671, test loss: 0.7042033855796193 \n",
            "train accuracy: 0.6786355475763016, test accuracy: 0.5\n",
            "Iteration 3175 training loss: 0.6426725999542237, test loss: 0.7042234392557418 \n",
            "train accuracy: 0.6786355475763016, test accuracy: 0.5\n",
            "Iteration 3176 training loss: 0.6425835746223608, test loss: 0.7042434703497112 \n",
            "train accuracy: 0.6786355475763016, test accuracy: 0.5\n",
            "Iteration 3177 training loss: 0.6424944791816132, test loss: 0.7042634786816326 \n",
            "train accuracy: 0.6786355475763016, test accuracy: 0.5\n",
            "Iteration 3178 training loss: 0.6424053136264624, test loss: 0.7042834640723712 \n",
            "train accuracy: 0.6786355475763016, test accuracy: 0.5\n",
            "Iteration 3179 training loss: 0.6423160779512392, test loss: 0.7043034263435575 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3180 training loss: 0.6422267721501265, test loss: 0.7043233653175915 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3181 training loss: 0.6421373962171625, test loss: 0.7043432808176466 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3182 training loss: 0.6420479501462434, test loss: 0.7043631726676736 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3183 training loss: 0.6419584339311257, test loss: 0.7043830406924059 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3184 training loss: 0.6418688475654307, test loss: 0.7044028847173625 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3185 training loss: 0.6417791910426452, test loss: 0.7044227045688534 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3186 training loss: 0.6416894643561262, test loss: 0.7044425000739828 \n",
            "train accuracy: 0.6804308797127468, test accuracy: 0.5\n",
            "Iteration 3187 training loss: 0.6415996674991026, test loss: 0.7044622710606533 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3188 training loss: 0.6415098004646785, test loss: 0.7044820173575705 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3189 training loss: 0.6414198632458364, test loss: 0.7045017387942462 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3190 training loss: 0.6413298558354393, test loss: 0.7045214352010029 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3191 training loss: 0.6412397782262339, test loss: 0.7045411064089778 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3192 training loss: 0.6411496304108538, test loss: 0.7045607522501262 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3193 training loss: 0.6410594123818217, test loss: 0.7045803725572259 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3194 training loss: 0.6409691241315527, test loss: 0.7045999671638806 \n",
            "train accuracy: 0.6822262118491921, test accuracy: 0.5\n",
            "Iteration 3195 training loss: 0.6408787656523569, test loss: 0.7046195359045241 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3196 training loss: 0.6407883369364425, test loss: 0.7046390786144234 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3197 training loss: 0.6406978379759183, test loss: 0.7046585951296833 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3198 training loss: 0.6406072687627965, test loss: 0.7046780852872493 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3199 training loss: 0.640516629288996, test loss: 0.7046975489249115 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3200 training loss: 0.6404259195463445, test loss: 0.7047169858813084 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3201 training loss: 0.6403351395265822, test loss: 0.7047363959959301 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3202 training loss: 0.6402442892213636, test loss: 0.7047557791091226 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3203 training loss: 0.640153368622261, test loss: 0.7047751350620898 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3204 training loss: 0.6400623777207674, test loss: 0.7047944636968988 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3205 training loss: 0.6399713165082983, test loss: 0.7048137648564821 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3206 training loss: 0.6398801849761958, test loss: 0.704833038384641 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3207 training loss: 0.6397889831157304, test loss: 0.7048522841260495 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3208 training loss: 0.6396977109181042, test loss: 0.7048715019262574 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3209 training loss: 0.6396063683744538, test loss: 0.7048906916316936 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3210 training loss: 0.6395149554758524, test loss: 0.7049098530896689 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3211 training loss: 0.6394234722133134, test loss: 0.7049289861483804 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3212 training loss: 0.6393319185777926, test loss: 0.704948090656913 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3213 training loss: 0.639240294560191, test loss: 0.7049671664652438 \n",
            "train accuracy: 0.6840215439856373, test accuracy: 0.5\n",
            "Iteration 3214 training loss: 0.6391486001513581, test loss: 0.7049862134242452 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3215 training loss: 0.6390568353420935, test loss: 0.7050052313856868 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3216 training loss: 0.6389650001231508, test loss: 0.7050242202022396 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3217 training loss: 0.6388730944852398, test loss: 0.7050431797274787 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3218 training loss: 0.638781118419029, test loss: 0.7050621098158862 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3219 training loss: 0.6386890719151489, test loss: 0.7050810103228539 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3220 training loss: 0.638596954964194, test loss: 0.7050998811046861 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3221 training loss: 0.6385047675567262, test loss: 0.7051187220186036 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3222 training loss: 0.6384125096832772, test loss: 0.7051375329227451 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3223 training loss: 0.6383201813343504, test loss: 0.7051563136761703 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3224 training loss: 0.6382277825004253, test loss: 0.7051750641388632 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3225 training loss: 0.6381353131719589, test loss: 0.705193784171735 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3226 training loss: 0.6380427733393882, test loss: 0.7052124736366253 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3227 training loss: 0.6379501629931338, test loss: 0.7052311323963066 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3228 training loss: 0.6378574821236019, test loss: 0.7052497603144857 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3229 training loss: 0.6377647307211872, test loss: 0.7052683572558065 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3230 training loss: 0.6376719087762753, test loss: 0.7052869230858532 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3231 training loss: 0.6375790162792458, test loss: 0.7053054576711515 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3232 training loss: 0.637486053220474, test loss: 0.7053239608791728 \n",
            "train accuracy: 0.6858168761220825, test accuracy: 0.5\n",
            "Iteration 3233 training loss: 0.6373930195903349, test loss: 0.7053424325783351 \n",
            "train accuracy: 0.6876122082585279, test accuracy: 0.5\n",
            "Iteration 3234 training loss: 0.6372999153792046, test loss: 0.7053608726380063 \n",
            "train accuracy: 0.6876122082585279, test accuracy: 0.5\n",
            "Iteration 3235 training loss: 0.6372067405774632, test loss: 0.7053792809285059 \n",
            "train accuracy: 0.6876122082585279, test accuracy: 0.5\n",
            "Iteration 3236 training loss: 0.6371134951754975, test loss: 0.7053976573211084 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3237 training loss: 0.6370201791637043, test loss: 0.7054160016880445 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3238 training loss: 0.6369267925324916, test loss: 0.705434313902504 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3239 training loss: 0.6368333352722823, test loss: 0.7054525938386379 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3240 training loss: 0.6367398073735157, test loss: 0.7054708413715604 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3241 training loss: 0.6366462088266511, test loss: 0.7054890563773516 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3242 training loss: 0.6365525396221704, test loss: 0.7055072387330587 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3243 training loss: 0.6364587997505794, test loss: 0.7055253883166996 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3244 training loss: 0.6363649892024112, test loss: 0.7055435050072635 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3245 training loss: 0.6362711079682293, test loss: 0.7055615886847136 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3246 training loss: 0.6361771560386285, test loss: 0.7055796392299889 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3247 training loss: 0.636083133404239, test loss: 0.7055976565250072 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3248 training loss: 0.6359890400557282, test loss: 0.7056156404526649 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3249 training loss: 0.635894875983803, test loss: 0.7056335908968409 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3250 training loss: 0.6358006411792125, test loss: 0.705651507742398 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3251 training loss: 0.6357063356327509, test loss: 0.7056693908751841 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3252 training loss: 0.6356119593352592, test loss: 0.7056872401820349 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3253 training loss: 0.635517512277628, test loss: 0.7057050555507743 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3254 training loss: 0.6354229944508001, test loss: 0.7057228368702183 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3255 training loss: 0.635328405845773, test loss: 0.7057405840301748 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3256 training loss: 0.6352337464536005, test loss: 0.705758296921446 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3257 training loss: 0.6351390162653967, test loss: 0.7057759754358304 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3258 training loss: 0.6350442152723368, test loss: 0.7057936194661238 \n",
            "train accuracy: 0.6894075403949731, test accuracy: 0.5\n",
            "Iteration 3259 training loss: 0.6349493434656601, test loss: 0.7058112289061216 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3260 training loss: 0.634854400836673, test loss: 0.7058288036506193 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3261 training loss: 0.6347593873767504, test loss: 0.7058463435954154 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3262 training loss: 0.6346643030773389, test loss: 0.7058638486373119 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3263 training loss: 0.6345691479299586, test loss: 0.7058813186741159 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3264 training loss: 0.6344739219262057, test loss: 0.7058987536046414 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3265 training loss: 0.6343786250577548, test loss: 0.7059161533287109 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3266 training loss: 0.6342832573163613, test loss: 0.7059335177471557 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3267 training loss: 0.6341878186938635, test loss: 0.7059508467618186 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3268 training loss: 0.6340923091821857, test loss: 0.7059681402755542 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3269 training loss: 0.6339967287733392, test loss: 0.705985398192231 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3270 training loss: 0.6339010774594257, test loss: 0.7060026204167317 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3271 training loss: 0.6338053552326394, test loss: 0.7060198068549555 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3272 training loss: 0.6337095620852684, test loss: 0.7060369574138184 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3273 training loss: 0.6336136980096986, test loss: 0.7060540720012553 \n",
            "train accuracy: 0.6912028725314183, test accuracy: 0.5\n",
            "Iteration 3274 training loss: 0.6335177629984146, test loss: 0.7060711505262198 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3275 training loss: 0.6334217570440023, test loss: 0.7060881928986867 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3276 training loss: 0.6333256801391512, test loss: 0.7061051990296529 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3277 training loss: 0.6332295322766571, test loss: 0.7061221688311369 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3278 training loss: 0.6331333134494238, test loss: 0.7061391022161817 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3279 training loss: 0.6330370236504651, test loss: 0.7061559990988554 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3280 training loss: 0.6329406628729075, test loss: 0.7061728593942513 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3281 training loss: 0.6328442311099923, test loss: 0.7061896830184896 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3282 training loss: 0.6327477283550779, test loss: 0.706206469888718 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3283 training loss: 0.6326511546016413, test loss: 0.7062232199231129 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3284 training loss: 0.6325545098432812, test loss: 0.7062399330408798 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3285 training loss: 0.6324577940737196, test loss: 0.7062566091622546 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3286 training loss: 0.6323610072868039, test loss: 0.7062732482085043 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3287 training loss: 0.6322641494765093, test loss: 0.7062898501019272 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3288 training loss: 0.6321672206369411, test loss: 0.7063064147658543 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3289 training loss: 0.632070220762336, test loss: 0.7063229421246499 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3290 training loss: 0.6319731498470652, test loss: 0.7063394321037118 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3291 training loss: 0.6318760078856358, test loss: 0.706355884629473 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3292 training loss: 0.6317787948726932, test loss: 0.7063722996294012 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3293 training loss: 0.631681510803023, test loss: 0.7063886770320003 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3294 training loss: 0.6315841556715535, test loss: 0.7064050167668098 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3295 training loss: 0.6314867294733568, test loss: 0.706421318764407 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3296 training loss: 0.631389232203652, test loss: 0.7064375829564059 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3297 training loss: 0.6312916638578064, test loss: 0.7064538092754589 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3298 training loss: 0.6311940244313382, test loss: 0.7064699976552569 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3299 training loss: 0.6310963139199172, test loss: 0.7064861480305289 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3300 training loss: 0.6309985323193688, test loss: 0.7065022603370441 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3301 training loss: 0.6309006796256741, test loss: 0.7065183345116105 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3302 training loss: 0.6308027558349732, test loss: 0.7065343704920772 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3303 training loss: 0.6307047609435663, test loss: 0.7065503682173323 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3304 training loss: 0.6306066949479161, test loss: 0.7065663276273054 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3305 training loss: 0.6305085578446497, test loss: 0.706582248662967 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3306 training loss: 0.6304103496305602, test loss: 0.7065981312663289 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3307 training loss: 0.6303120703026095, test loss: 0.7066139753804438 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3308 training loss: 0.630213719857929, test loss: 0.7066297809494064 \n",
            "train accuracy: 0.6929982046678635, test accuracy: 0.5\n",
            "Iteration 3309 training loss: 0.6301152982938224, test loss: 0.7066455479183531 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3310 training loss: 0.630016805607767, test loss: 0.7066612762334629 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3311 training loss: 0.6299182417974164, test loss: 0.7066769658419557 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3312 training loss: 0.6298196068606012, test loss: 0.706692616692095 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3313 training loss: 0.6297209007953322, test loss: 0.7067082287331855 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3314 training loss: 0.6296221235998007, test loss: 0.7067238019155748 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3315 training loss: 0.6295232752723822, test loss: 0.7067393361906529 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3316 training loss: 0.6294243558116359, test loss: 0.7067548315108518 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3317 training loss: 0.6293253652163091, test loss: 0.7067702878296465 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3318 training loss: 0.6292263034853368, test loss: 0.7067857051015536 \n",
            "train accuracy: 0.6947935368043088, test accuracy: 0.5\n",
            "Iteration 3319 training loss: 0.6291271706178447, test loss: 0.7068010832821326 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3320 training loss: 0.6290279666131506, test loss: 0.706816422327985 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3321 training loss: 0.6289286914707665, test loss: 0.7068317221967538 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3322 training loss: 0.6288293451903996, test loss: 0.7068469828471251 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3323 training loss: 0.6287299277719548, test loss: 0.7068622042388256 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3324 training loss: 0.628630439215536, test loss: 0.706877386332624 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3325 training loss: 0.6285308795214479, test loss: 0.7068925290903305 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3326 training loss: 0.6284312486901983, test loss: 0.7069076324747959 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3327 training loss: 0.6283315467224984, test loss: 0.7069226964499127 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3328 training loss: 0.6282317736192661, test loss: 0.7069377209806134 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3329 training loss: 0.6281319293816263, test loss: 0.7069527060328705 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3330 training loss: 0.6280320140109135, test loss: 0.706967651573697 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3331 training loss: 0.6279320275086732, test loss: 0.7069825575711454 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3332 training loss: 0.627831969876663, test loss: 0.7069974239943068 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3333 training loss: 0.6277318411168551, test loss: 0.7070122508133119 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3334 training loss: 0.6276316412314373, test loss: 0.7070270379993294 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3335 training loss: 0.6275313702228148, test loss: 0.7070417855245658 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3336 training loss: 0.6274310280936114, test loss: 0.7070564933622651 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3337 training loss: 0.6273306148466724, test loss: 0.7070711614867086 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3338 training loss: 0.6272301304850638, test loss: 0.7070857898732136 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3339 training loss: 0.6271295750120763, test loss: 0.7071003784981337 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3340 training loss: 0.6270289484312253, test loss: 0.7071149273388576 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3341 training loss: 0.6269282507462531, test loss: 0.7071294363738089 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3342 training loss: 0.6268274819611301, test loss: 0.7071439055824452 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3343 training loss: 0.6267266420800567, test loss: 0.7071583349452579 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3344 training loss: 0.6266257311074637, test loss: 0.7071727244437712 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3345 training loss: 0.6265247490480154, test loss: 0.7071870740605415 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3346 training loss: 0.62642369590661, test loss: 0.7072013837791564 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3347 training loss: 0.6263225716883808, test loss: 0.7072156535842351 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3348 training loss: 0.6262213763986988, test loss: 0.7072298834614262 \n",
            "train accuracy: 0.696588868940754, test accuracy: 0.4928571428571429\n",
            "Iteration 3349 training loss: 0.6261201100431733, test loss: 0.7072440733974078 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3350 training loss: 0.6260187726276532, test loss: 0.707258223379887 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3351 training loss: 0.6259173641582284, test loss: 0.7072723333975978 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3352 training loss: 0.6258158846412323, test loss: 0.7072864034403019 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3353 training loss: 0.6257143340832415, test loss: 0.7073004334987867 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3354 training loss: 0.6256127124910785, test loss: 0.7073144235648648 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3355 training loss: 0.6255110198718122, test loss: 0.7073283736313735 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3356 training loss: 0.6254092562327598, test loss: 0.7073422836921732 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3357 training loss: 0.6253074215814877, test loss: 0.7073561537421469 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3358 training loss: 0.625205515925813, test loss: 0.7073699837771991 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3359 training loss: 0.6251035392738049, test loss: 0.7073837737942551 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3360 training loss: 0.625001491633786, test loss: 0.7073975237912595 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3361 training loss: 0.6248993730143333, test loss: 0.707411233767176 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3362 training loss: 0.6247971834242795, test loss: 0.7074249037219852 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3363 training loss: 0.6246949228727143, test loss: 0.7074385336566847 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3364 training loss: 0.6245925913689863, test loss: 0.7074521235732876 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3365 training loss: 0.6244901889227029, test loss: 0.7074656734748211 \n",
            "train accuracy: 0.6983842010771992, test accuracy: 0.4928571428571429\n",
            "Iteration 3366 training loss: 0.6243877155437326, test loss: 0.7074791833653257 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3367 training loss: 0.624285171242206, test loss: 0.7074926532498541 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3368 training loss: 0.624182556028516, test loss: 0.70750608313447 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3369 training loss: 0.6240798699133209, test loss: 0.707519473026247 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3370 training loss: 0.6239771129075435, test loss: 0.7075328229332669 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3371 training loss: 0.6238742850223736, test loss: 0.7075461328646193 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3372 training loss: 0.6237713862692685, test loss: 0.7075594028304 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3373 training loss: 0.6236684166599546, test loss: 0.7075726328417095 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3374 training loss: 0.6235653762064282, test loss: 0.7075858229106522 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3375 training loss: 0.623462264920956, test loss: 0.7075989730503346 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3376 training loss: 0.6233590828160775, test loss: 0.7076120832748645 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3377 training loss: 0.6232558299046053, test loss: 0.7076251535993495 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3378 training loss: 0.6231525061996257, test loss: 0.7076381840398954 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3379 training loss: 0.6230491117145008, test loss: 0.7076511746136053 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3380 training loss: 0.6229456464628684, test loss: 0.7076641253385775 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3381 training loss: 0.6228421104586442, test loss: 0.7076770362339052 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3382 training loss: 0.6227385037160218, test loss: 0.7076899073196741 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3383 training loss: 0.6226348262494741, test loss: 0.7077027386169612 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3384 training loss: 0.622531078073754, test loss: 0.7077155301478335 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3385 training loss: 0.622427259203896, test loss: 0.7077282819353468 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3386 training loss: 0.6223233696552168, test loss: 0.7077409940035435 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3387 training loss: 0.6222194094433156, test loss: 0.7077536663774514 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3388 training loss: 0.6221153785840761, test loss: 0.7077662990830829 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3389 training loss: 0.6220112770936664, test loss: 0.7077788921474321 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3390 training loss: 0.621907104988541, test loss: 0.7077914455984742 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3391 training loss: 0.6218028622854406, test loss: 0.7078039594651634 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3392 training loss: 0.6216985490013933, test loss: 0.7078164337774319 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3393 training loss: 0.6215941651537161, test loss: 0.7078288685661881 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3394 training loss: 0.6214897107600148, test loss: 0.7078412638633138 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3395 training loss: 0.6213851858381851, test loss: 0.7078536197016647 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3396 training loss: 0.6212805904064141, test loss: 0.707865936115067 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3397 training loss: 0.6211759244831799, test loss: 0.7078782131383158 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3398 training loss: 0.6210711880872534, test loss: 0.7078904508071747 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3399 training loss: 0.6209663812376984, test loss: 0.7079026491583729 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3400 training loss: 0.6208615039538731, test loss: 0.7079148082296034 \n",
            "train accuracy: 0.7001795332136446, test accuracy: 0.4928571428571429\n",
            "Iteration 3401 training loss: 0.6207565562554297, test loss: 0.7079269280595222 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3402 training loss: 0.6206515381623167, test loss: 0.7079390086877457 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3403 training loss: 0.6205464496947776, test loss: 0.707951050154849 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3404 training loss: 0.6204412908733538, test loss: 0.7079630525023647 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3405 training loss: 0.6203360617188836, test loss: 0.7079750157727798 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3406 training loss: 0.6202307622525036, test loss: 0.7079869400095353 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3407 training loss: 0.6201253924956494, test loss: 0.7079988252570236 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3408 training loss: 0.6200199524700558, test loss: 0.7080106715605867 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3409 training loss: 0.6199144421977579, test loss: 0.7080224789665142 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3410 training loss: 0.6198088617010917, test loss: 0.7080342475220411 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3411 training loss: 0.6197032110026943, test loss: 0.7080459772753473 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3412 training loss: 0.6195974901255047, test loss: 0.7080576682755538 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3413 training loss: 0.6194916990927647, test loss: 0.7080693205727218 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3414 training loss: 0.6193858379280192, test loss: 0.7080809342178506 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3415 training loss: 0.619279906655116, test loss: 0.708092509262875 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3416 training loss: 0.6191739052982084, test loss: 0.7081040457606645 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3417 training loss: 0.619067833881753, test loss: 0.7081155437650202 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3418 training loss: 0.6189616924305128, test loss: 0.7081270033306729 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3419 training loss: 0.6188554809695554, test loss: 0.7081384245132814 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3420 training loss: 0.6187491995242558, test loss: 0.7081498073694301 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3421 training loss: 0.6186428481202944, test loss: 0.7081611519566277 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3422 training loss: 0.6185364267836596, test loss: 0.7081724583333036 \n",
            "train accuracy: 0.703770197486535, test accuracy: 0.4928571428571429\n",
            "Iteration 3423 training loss: 0.6184299355406472, test loss: 0.7081837265588071 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3424 training loss: 0.6183233744178608, test loss: 0.7081949566934048 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3425 training loss: 0.6182167434422123, test loss: 0.7082061487982785 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3426 training loss: 0.618110042640923, test loss: 0.7082173029355227 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3427 training loss: 0.6180032720415225, test loss: 0.7082284191681424 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.4928571428571429\n",
            "Iteration 3428 training loss: 0.6178964316718506, test loss: 0.708239497560052 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.5\n",
            "Iteration 3429 training loss: 0.617789521560057, test loss: 0.708250538176072 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.5\n",
            "Iteration 3430 training loss: 0.6176825417346012, test loss: 0.7082615410819264 \n",
            "train accuracy: 0.7019748653500898, test accuracy: 0.5\n",
            "Iteration 3431 training loss: 0.6175754922242541, test loss: 0.7082725063442421 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3432 training loss: 0.6174683730580967, test loss: 0.7082834340305445 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3433 training loss: 0.6173611842655219, test loss: 0.7082943242092571 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3434 training loss: 0.6172539258762334, test loss: 0.7083051769496983 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3435 training loss: 0.6171465979202474, test loss: 0.7083159923220792 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3436 training loss: 0.6170392004278921, test loss: 0.708326770397501 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3437 training loss: 0.6169317334298073, test loss: 0.7083375112479534 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3438 training loss: 0.6168241969569463, test loss: 0.7083482149463118 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3439 training loss: 0.6167165910405746, test loss: 0.7083588815663345 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3440 training loss: 0.616608915712271, test loss: 0.7083695111826616 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3441 training loss: 0.6165011710039272, test loss: 0.7083801038708106 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3442 training loss: 0.6163933569477485, test loss: 0.708390659707176 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3443 training loss: 0.6162854735762539, test loss: 0.7084011787690262 \n",
            "train accuracy: 0.7055655296229802, test accuracy: 0.5\n",
            "Iteration 3444 training loss: 0.6161775209222755, test loss: 0.7084116611345002 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3445 training loss: 0.61606949901896, test loss: 0.708422106882606 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3446 training loss: 0.6159614078997677, test loss: 0.7084325160932183 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3447 training loss: 0.615853247598473, test loss: 0.7084428888470756 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3448 training loss: 0.6157450181491647, test loss: 0.7084532252257775 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3449 training loss: 0.6156367195862458, test loss: 0.7084635253117829 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3450 training loss: 0.6155283519444332, test loss: 0.7084737891884064 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3451 training loss: 0.6154199152587595, test loss: 0.7084840169398174 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3452 training loss: 0.6153114095645703, test loss: 0.7084942086510354 \n",
            "train accuracy: 0.7073608617594255, test accuracy: 0.5\n",
            "Iteration 3453 training loss: 0.6152028348975269, test loss: 0.7085043644079296 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3454 training loss: 0.6150941912936044, test loss: 0.7085144842972145 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3455 training loss: 0.614985478789093, test loss: 0.708524568406449 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3456 training loss: 0.6148766974205974, test loss: 0.7085346168240315 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3457 training loss: 0.6147678472250362, test loss: 0.7085446296392007 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3458 training loss: 0.6146589282396435, test loss: 0.7085546069420289 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3459 training loss: 0.6145499405019673, test loss: 0.7085645488234229 \n",
            "train accuracy: 0.7091561938958707, test accuracy: 0.5\n",
            "Iteration 3460 training loss: 0.6144408840498704, test loss: 0.7085744553751188 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3461 training loss: 0.6143317589215294, test loss: 0.7085843266896811 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3462 training loss: 0.6142225651554358, test loss: 0.7085941628604989 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3463 training loss: 0.614113302790395, test loss: 0.7086039639817839 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3464 training loss: 0.6140039718655265, test loss: 0.7086137301485667 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3465 training loss: 0.613894572420264, test loss: 0.7086234614566956 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3466 training loss: 0.6137851044943549, test loss: 0.7086331580028323 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3467 training loss: 0.6136755681278601, test loss: 0.7086428198844501 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3468 training loss: 0.6135659633611543, test loss: 0.7086524471998311 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3469 training loss: 0.613456290234926, test loss: 0.7086620400480629 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3470 training loss: 0.6133465487901757, test loss: 0.7086715985290358 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3471 training loss: 0.6132367390682181, test loss: 0.708681122743441 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3472 training loss: 0.6131268611106805, test loss: 0.7086906127927668 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3473 training loss: 0.613016914959502, test loss: 0.7087000687792959 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3474 training loss: 0.6129069006569348, test loss: 0.7087094908061029 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3475 training loss: 0.6127968182455432, test loss: 0.708718878977051 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3476 training loss: 0.6126866677682029, test loss: 0.70872823339679 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3477 training loss: 0.6125764492681018, test loss: 0.7087375541707519 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3478 training loss: 0.6124661627887382, test loss: 0.7087468414051497 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3479 training loss: 0.6123558083739225, test loss: 0.7087560952069735 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3480 training loss: 0.6122453860677749, test loss: 0.7087653156839877 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3481 training loss: 0.6121348959147265, test loss: 0.7087745029447281 \n",
            "train accuracy: 0.7109515260323159, test accuracy: 0.5\n",
            "Iteration 3482 training loss: 0.6120243379595182, test loss: 0.7087836570984993 \n",
            "train accuracy: 0.7127468581687613, test accuracy: 0.5\n",
            "Iteration 3483 training loss: 0.6119137122472007, test loss: 0.7087927782553717 \n",
            "train accuracy: 0.7127468581687613, test accuracy: 0.5\n",
            "Iteration 3484 training loss: 0.6118030188231343, test loss: 0.7088018665261777 \n",
            "train accuracy: 0.7127468581687613, test accuracy: 0.5\n",
            "Iteration 3485 training loss: 0.6116922577329879, test loss: 0.70881092202251 \n",
            "train accuracy: 0.7127468581687613, test accuracy: 0.5\n",
            "Iteration 3486 training loss: 0.611581429022739, test loss: 0.7088199448567173 \n",
            "train accuracy: 0.7145421903052065, test accuracy: 0.5\n",
            "Iteration 3487 training loss: 0.6114705327386737, test loss: 0.7088289351419027 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3488 training loss: 0.6113595689273856, test loss: 0.7088378929919196 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3489 training loss: 0.6112485376357761, test loss: 0.7088468185213693 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3490 training loss: 0.611137438911053, test loss: 0.7088557118455973 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3491 training loss: 0.611026272800731, test loss: 0.7088645730806908 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3492 training loss: 0.610915039352631, test loss: 0.7088734023434764 \n",
            "train accuracy: 0.7163375224416517, test accuracy: 0.5\n",
            "Iteration 3493 training loss: 0.6108037386148791, test loss: 0.7088821997515148 \n",
            "train accuracy: 0.718132854578097, test accuracy: 0.5\n",
            "Iteration 3494 training loss: 0.6106923706359072, test loss: 0.7088909654230999 \n",
            "train accuracy: 0.7217235188509874, test accuracy: 0.5\n",
            "Iteration 3495 training loss: 0.6105809354644515, test loss: 0.7088996994772554 \n",
            "train accuracy: 0.7217235188509874, test accuracy: 0.5\n",
            "Iteration 3496 training loss: 0.6104694331495519, test loss: 0.7089084020337296 \n",
            "train accuracy: 0.7217235188509874, test accuracy: 0.5\n",
            "Iteration 3497 training loss: 0.610357863740553, test loss: 0.7089170732129958 \n",
            "train accuracy: 0.7217235188509874, test accuracy: 0.5\n",
            "Iteration 3498 training loss: 0.6102462272871017, test loss: 0.7089257131362461 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3499 training loss: 0.610134523839148, test loss: 0.70893432192539 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3500 training loss: 0.610022753446944, test loss: 0.7089428997030504 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3501 training loss: 0.6099109161610429, test loss: 0.7089514465925609 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3502 training loss: 0.6097990120322992, test loss: 0.7089599627179629 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3503 training loss: 0.609687041111868, test loss: 0.7089684482040013 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3504 training loss: 0.6095750034512036, test loss: 0.7089769031761227 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3505 training loss: 0.6094628991020601, test loss: 0.7089853277604717 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3506 training loss: 0.60935072811649, test loss: 0.7089937220838871 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3507 training loss: 0.6092384905468436, test loss: 0.7090020862738992 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3508 training loss: 0.6091261864457689, test loss: 0.7090104204587269 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3509 training loss: 0.6090138158662102, test loss: 0.7090187247672741 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3510 training loss: 0.6089013788614085, test loss: 0.7090269993291262 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3511 training loss: 0.6087888754848996, test loss: 0.7090352442745473 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3512 training loss: 0.6086763057905142, test loss: 0.7090434597344769 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3513 training loss: 0.6085636698323773, test loss: 0.7090516458405265 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3514 training loss: 0.6084509676649071, test loss: 0.709059802724976 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3515 training loss: 0.6083381993428145, test loss: 0.7090679305207713 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3516 training loss: 0.6082253649211022, test loss: 0.7090760293615203 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.4928571428571429\n",
            "Iteration 3517 training loss: 0.6081124644550643, test loss: 0.7090840993814892 \n",
            "train accuracy: 0.7235188509874326, test accuracy: 0.5\n",
            "Iteration 3518 training loss: 0.6079994980002853, test loss: 0.7090921407156003 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3519 training loss: 0.6078864656126396, test loss: 0.7091001534994281 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3520 training loss: 0.6077733673482903, test loss: 0.7091081378691957 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3521 training loss: 0.6076602032636892, test loss: 0.7091160939617721 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3522 training loss: 0.6075469734155751, test loss: 0.709124021914668 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3523 training loss: 0.6074336778609737, test loss: 0.7091319218660335 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3524 training loss: 0.6073203166571967, test loss: 0.7091397939546537 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3525 training loss: 0.6072068898618406, test loss: 0.709147638319946 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3526 training loss: 0.6070933975327861, test loss: 0.7091554551019563 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3527 training loss: 0.6069798397281982, test loss: 0.709163244441356 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3528 training loss: 0.6068662165065232, test loss: 0.7091710064794386 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3529 training loss: 0.6067525279264905, test loss: 0.7091787413581154 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3530 training loss: 0.6066387740471094, test loss: 0.7091864492199136 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3531 training loss: 0.6065249549276698, test loss: 0.7091941302079714 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3532 training loss: 0.6064110706277407, test loss: 0.7092017844660354 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3533 training loss: 0.6062971212071697, test loss: 0.7092094121384572 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3534 training loss: 0.606183106726081, test loss: 0.7092170133701895 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3535 training loss: 0.6060690272448764, test loss: 0.7092245883067825 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3536 training loss: 0.6059548828242326, test loss: 0.7092321370943816 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5\n",
            "Iteration 3537 training loss: 0.6058406735251015, test loss: 0.7092396598797225 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3538 training loss: 0.6057263994087084, test loss: 0.7092471568101285 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3539 training loss: 0.6056120605365519, test loss: 0.7092546280335069 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3540 training loss: 0.6054976569704023, test loss: 0.709262073698345 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3541 training loss: 0.6053831887723008, test loss: 0.7092694939537081 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3542 training loss: 0.6052686560045589, test loss: 0.7092768889492336 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3543 training loss: 0.605154058729757, test loss: 0.7092842588351295 \n",
            "train accuracy: 0.725314183123878, test accuracy: 0.5071428571428571\n",
            "Iteration 3544 training loss: 0.6050393970107437, test loss: 0.7092916037621704 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3545 training loss: 0.6049246709106347, test loss: 0.7092989238816934 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3546 training loss: 0.6048098804928118, test loss: 0.7093062193455947 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3547 training loss: 0.6046950258209219, test loss: 0.7093134903063267 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3548 training loss: 0.6045801069588763, test loss: 0.7093207369168937 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3549 training loss: 0.6044651239708492, test loss: 0.7093279593308489 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3550 training loss: 0.6043500769212767, test loss: 0.7093351577022904 \n",
            "train accuracy: 0.7271095152603232, test accuracy: 0.5071428571428571\n",
            "Iteration 3551 training loss: 0.6042349658748565, test loss: 0.7093423321858576 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3552 training loss: 0.604119790896546, test loss: 0.7093494829367284 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3553 training loss: 0.6040045520515616, test loss: 0.7093566101106145 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3554 training loss: 0.6038892494053778, test loss: 0.7093637138637583 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3555 training loss: 0.6037738830237258, test loss: 0.7093707943529298 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3556 training loss: 0.6036584529725927, test loss: 0.7093778517354221 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3557 training loss: 0.6035429593182206, test loss: 0.7093848861690485 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3558 training loss: 0.6034274021271049, test loss: 0.7093918978121386 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5071428571428571\n",
            "Iteration 3559 training loss: 0.603311781465994, test loss: 0.7093988868235345 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5142857142857142\n",
            "Iteration 3560 training loss: 0.6031960974018872, test loss: 0.7094058533625872 \n",
            "train accuracy: 0.7289048473967684, test accuracy: 0.5142857142857142\n",
            "Iteration 3561 training loss: 0.6030803500020349, test loss: 0.7094127975891537 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5142857142857142\n",
            "Iteration 3562 training loss: 0.6029645393339359, test loss: 0.709419719663592 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5142857142857142\n",
            "Iteration 3563 training loss: 0.6028486654653384, test loss: 0.709426619746759 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5142857142857142\n",
            "Iteration 3564 training loss: 0.6027327284642366, test loss: 0.7094334980000053 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5142857142857142\n",
            "Iteration 3565 training loss: 0.6026167283988709, test loss: 0.7094403545851732 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5214285714285715\n",
            "Iteration 3566 training loss: 0.6025006653377271, test loss: 0.7094471896645912 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5214285714285715\n",
            "Iteration 3567 training loss: 0.6023845393495333, test loss: 0.709454003401072 \n",
            "train accuracy: 0.7307001795332136, test accuracy: 0.5214285714285715\n",
            "Iteration 3568 training loss: 0.6022683505032616, test loss: 0.7094607959579077 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3569 training loss: 0.6021520988681245, test loss: 0.7094675674988669 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3570 training loss: 0.6020357845135752, test loss: 0.7094743181881904 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3571 training loss: 0.6019194075093055, test loss: 0.7094810481905878 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3572 training loss: 0.6018029679252455, test loss: 0.7094877576712332 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3573 training loss: 0.6016864658315613, test loss: 0.7094944467957635 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3574 training loss: 0.6015699012986554, test loss: 0.7095011157302717 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3575 training loss: 0.6014532743971639, test loss: 0.7095077646413059 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3576 training loss: 0.6013365851979563, test loss: 0.7095143936958638 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3577 training loss: 0.601219833772134, test loss: 0.7095210030613898 \n",
            "train accuracy: 0.7324955116696589, test accuracy: 0.5214285714285715\n",
            "Iteration 3578 training loss: 0.601103020191029, test loss: 0.7095275929057713 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3579 training loss: 0.600986144526203, test loss: 0.7095341633973343 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3580 training loss: 0.6008692068494454, test loss: 0.7095407147048407 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3581 training loss: 0.6007522072327737, test loss: 0.7095472469974838 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3582 training loss: 0.6006351457484301, test loss: 0.7095537604448848 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3583 training loss: 0.6005180224688823, test loss: 0.709560255217089 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3584 training loss: 0.6004008374668205, test loss: 0.7095667314845621 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3585 training loss: 0.6002835908151578, test loss: 0.7095731894181865 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3586 training loss: 0.6001662825870275, test loss: 0.7095796291892573 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3587 training loss: 0.6000489128557831, test loss: 0.7095860509694791 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3588 training loss: 0.599931481694996, test loss: 0.7095924549309619 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3589 training loss: 0.5998139891784549, test loss: 0.7095988412462169 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3590 training loss: 0.5996964353801644, test loss: 0.7096052100881532 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3591 training loss: 0.5995788203743433, test loss: 0.7096115616300747 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3592 training loss: 0.5994611442354242, test loss: 0.7096178960456747 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3593 training loss: 0.5993434070380511, test loss: 0.7096242135090336 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5214285714285715\n",
            "Iteration 3594 training loss: 0.5992256088570792, test loss: 0.7096305141946142 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3595 training loss: 0.5991077497675728, test loss: 0.7096367982772589 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3596 training loss: 0.5989898298448044, test loss: 0.7096430659321845 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3597 training loss: 0.5988718491642537, test loss: 0.7096493173349797 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3598 training loss: 0.5987538078016053, test loss: 0.709655552661601 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3599 training loss: 0.598635705832748, test loss: 0.7096617720883679 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3600 training loss: 0.5985175433337746, test loss: 0.7096679757919607 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3601 training loss: 0.5983993203809783, test loss: 0.7096741639494161 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3602 training loss: 0.598281037050853, test loss: 0.7096803367381225 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3603 training loss: 0.5981626934200915, test loss: 0.7096864943358177 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3604 training loss: 0.5980442895655845, test loss: 0.7096926369205832 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3605 training loss: 0.5979258255644188, test loss: 0.709698764670843 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3606 training loss: 0.5978073014938761, test loss: 0.7097048777653575 \n",
            "train accuracy: 0.7342908438061041, test accuracy: 0.5285714285714286\n",
            "Iteration 3607 training loss: 0.5976887174314318, test loss: 0.7097109763832202 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3608 training loss: 0.5975700734547539, test loss: 0.7097170607038553 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3609 training loss: 0.5974513696417008, test loss: 0.7097231309070116 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3610 training loss: 0.597332606070321, test loss: 0.7097291871727607 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3611 training loss: 0.5972137828188512, test loss: 0.7097352296814918 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3612 training loss: 0.5970948999657149, test loss: 0.7097412586139092 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3613 training loss: 0.596975957589521, test loss: 0.7097472741510268 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3614 training loss: 0.5968569557690632, test loss: 0.7097532764741659 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3615 training loss: 0.5967378945833174, test loss: 0.7097592657649504 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3616 training loss: 0.5966187741114414, test loss: 0.7097652422053035 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3617 training loss: 0.5964995944327731, test loss: 0.7097712059774433 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3618 training loss: 0.5963803556268289, test loss: 0.7097771572638794 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3619 training loss: 0.5962610577733032, test loss: 0.7097830962474093 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3620 training loss: 0.5961417009520655, test loss: 0.7097890231111141 \n",
            "train accuracy: 0.7360861759425493, test accuracy: 0.5285714285714286\n",
            "Iteration 3621 training loss: 0.5960222852431608, test loss: 0.7097949380383547 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3622 training loss: 0.5959028107268072, test loss: 0.7098008412127683 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3623 training loss: 0.5957832774833941, test loss: 0.7098067328182643 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3624 training loss: 0.5956636855934825, test loss: 0.7098126130390207 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3625 training loss: 0.5955440351378014, test loss: 0.7098184820594797 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3626 training loss: 0.5954243261972483, test loss: 0.7098243400643455 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3627 training loss: 0.5953045588528868, test loss: 0.7098301872385775 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3628 training loss: 0.5951847331859458, test loss: 0.7098360237673895 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3629 training loss: 0.595064849277817, test loss: 0.7098418498362447 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3630 training loss: 0.5949449072100554, test loss: 0.7098476656308508 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3631 training loss: 0.594824907064376, test loss: 0.7098534713371581 \n",
            "train accuracy: 0.7378815080789947, test accuracy: 0.5285714285714286\n",
            "Iteration 3632 training loss: 0.5947048489226533, test loss: 0.7098592671413546 \n",
            "train accuracy: 0.7396768402154399, test accuracy: 0.5285714285714286\n",
            "Iteration 3633 training loss: 0.5945847328669202, test loss: 0.7098650532298618 \n",
            "train accuracy: 0.7414721723518851, test accuracy: 0.5285714285714286\n",
            "Iteration 3634 training loss: 0.5944645589793658, test loss: 0.709870829789332 \n",
            "train accuracy: 0.7414721723518851, test accuracy: 0.5285714285714286\n",
            "Iteration 3635 training loss: 0.5943443273423348, test loss: 0.7098765970066434 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5285714285714286\n",
            "Iteration 3636 training loss: 0.5942240380383255, test loss: 0.7098823550688972 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3637 training loss: 0.5941036911499883, test loss: 0.7098881041634127 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3638 training loss: 0.5939832867601256, test loss: 0.7098938444777246 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3639 training loss: 0.5938628249516882, test loss: 0.7098995761995784 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3640 training loss: 0.5937423058077759, test loss: 0.7099052995169274 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3641 training loss: 0.593621729411635, test loss: 0.709911014617927 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3642 training loss: 0.5935010958466574, test loss: 0.7099167216909338 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3643 training loss: 0.5933804051963787, test loss: 0.709922420924499 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3644 training loss: 0.5932596575444773, test loss: 0.7099281125073663 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3645 training loss: 0.5931388529747726, test loss: 0.7099337966284676 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3646 training loss: 0.5930179915712239, test loss: 0.7099394734769189 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3647 training loss: 0.5928970734179287, test loss: 0.7099451432420165 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3648 training loss: 0.5927760985991217, test loss: 0.7099508061132338 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3649 training loss: 0.5926550671991725, test loss: 0.7099564622802172 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5285714285714286\n",
            "Iteration 3650 training loss: 0.5925339793025858, test loss: 0.7099621119327818 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3651 training loss: 0.5924128349939976, test loss: 0.709967755260908 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3652 training loss: 0.5922916343581768, test loss: 0.709973392454738 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3653 training loss: 0.5921703774800208, test loss: 0.7099790237045711 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3654 training loss: 0.5920490644445562, test loss: 0.7099846492008614 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3655 training loss: 0.5919276953369363, test loss: 0.7099902691342119 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3656 training loss: 0.5918062702424403, test loss: 0.7099958836953728 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3657 training loss: 0.591684789246471, test loss: 0.7100014930752363 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3658 training loss: 0.5915632524345551, test loss: 0.7100070974648335 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3659 training loss: 0.5914416598923393, test loss: 0.7100126970553301 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3660 training loss: 0.5913200117055913, test loss: 0.7100182920380232 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3661 training loss: 0.5911983079601966, test loss: 0.7100238826043372 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3662 training loss: 0.5910765487421585, test loss: 0.7100294689458201 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3663 training loss: 0.5909547341375956, test loss: 0.710035051254139 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3664 training loss: 0.5908328642327407, test loss: 0.7100406297210781 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3665 training loss: 0.5907109391139399, test loss: 0.7100462045385327 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3666 training loss: 0.5905889588676502, test loss: 0.7100517758985074 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3667 training loss: 0.590466923580439, test loss: 0.710057343993111 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3668 training loss: 0.5903448333389824, test loss: 0.7100629090145534 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3669 training loss: 0.5902226882300632, test loss: 0.7100684711551413 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3670 training loss: 0.5901004883405709, test loss: 0.7100740306072751 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3671 training loss: 0.5899782337574986, test loss: 0.710079587563445 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3672 training loss: 0.5898559245679427, test loss: 0.7100851422162268 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3673 training loss: 0.5897335608591009, test loss: 0.7100906947582784 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3674 training loss: 0.5896111427182715, test loss: 0.7100962453823361 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3675 training loss: 0.5894886702328511, test loss: 0.7101017942812111 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3676 training loss: 0.589366143490334, test loss: 0.7101073416477856 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3677 training loss: 0.5892435625783102, test loss: 0.7101128876750079 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3678 training loss: 0.5891209275844642, test loss: 0.7101184325558917 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3679 training loss: 0.5889982385965737, test loss: 0.7101239764835083 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3680 training loss: 0.5888754957025081, test loss: 0.7101295196509867 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3681 training loss: 0.5887526989902272, test loss: 0.7101350622515069 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3682 training loss: 0.5886298485477792, test loss: 0.7101406044782986 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3683 training loss: 0.5885069444633007, test loss: 0.7101461465246359 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3684 training loss: 0.5883839868250134, test loss: 0.7101516885838337 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3685 training loss: 0.588260975721224, test loss: 0.7101572308492449 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3686 training loss: 0.5881379112403231, test loss: 0.7101627735142559 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3687 training loss: 0.5880147934707824, test loss: 0.7101683167722831 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3688 training loss: 0.5878916225011547, test loss: 0.7101738608167698 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3689 training loss: 0.587768398420071, test loss: 0.7101794058411814 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3690 training loss: 0.5876451213162409, test loss: 0.7101849520390029 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3691 training loss: 0.5875217912784497, test loss: 0.710190499603734 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3692 training loss: 0.5873984083955585, test loss: 0.710196048728887 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3693 training loss: 0.5872749727565005, test loss: 0.7102015996079812 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3694 training loss: 0.5871514844502821, test loss: 0.7102071524345414 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3695 training loss: 0.5870279435659803, test loss: 0.7102127074020922 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3696 training loss: 0.5869043501927412, test loss: 0.7102182647041557 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3697 training loss: 0.5867807044197787, test loss: 0.7102238245342476 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3698 training loss: 0.5866570063363737, test loss: 0.710229387085873 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3699 training loss: 0.5865332560318721, test loss: 0.7102349525525239 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3700 training loss: 0.5864094535956834, test loss: 0.7102405211276741 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3701 training loss: 0.5862855991172802, test loss: 0.7102460930047767 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3702 training loss: 0.5861616926861952, test loss: 0.7102516683772601 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3703 training loss: 0.5860377343920218, test loss: 0.7102572474385245 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3704 training loss: 0.5859137243244106, test loss: 0.710262830381938 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3705 training loss: 0.58578966257307, test loss: 0.7102684174008339 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3706 training loss: 0.5856655492277636, test loss: 0.7102740086885055 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3707 training loss: 0.5855413843783092, test loss: 0.710279604438204 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3708 training loss: 0.5854171681145776, test loss: 0.7102852048431345 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3709 training loss: 0.5852929005264905, test loss: 0.710290810096452 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3710 training loss: 0.5851685817040205, test loss: 0.7102964203912584 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3711 training loss: 0.5850442117371883, test loss: 0.7103020359205985 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3712 training loss: 0.5849197907160625, test loss: 0.7103076568774567 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3713 training loss: 0.5847953187307569, test loss: 0.7103132834547536 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3714 training loss: 0.5846707958714304, test loss: 0.7103189158453421 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3715 training loss: 0.5845462222282858, test loss: 0.7103245542420039 \n",
            "train accuracy: 0.7432675044883303, test accuracy: 0.5357142857142857\n",
            "Iteration 3716 training loss: 0.5844215978915667, test loss: 0.7103301988374464 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3717 training loss: 0.5842969229515578, test loss: 0.7103358498242986 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3718 training loss: 0.5841721974985834, test loss: 0.7103415073951082 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3719 training loss: 0.584047421623005, test loss: 0.7103471717423375 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3720 training loss: 0.5839225954152213, test loss: 0.71035284305836 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3721 training loss: 0.5837977189656658, test loss: 0.7103585215354579 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3722 training loss: 0.5836727923648058, test loss: 0.7103642073658168 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3723 training loss: 0.5835478157031414, test loss: 0.7103699007415238 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3724 training loss: 0.5834227890712039, test loss: 0.7103756018545635 \n",
            "train accuracy: 0.7450628366247756, test accuracy: 0.5357142857142857\n",
            "Iteration 3725 training loss: 0.5832977125595539, test loss: 0.710381310896814 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3726 training loss: 0.5831725862587812, test loss: 0.7103870280600446 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3727 training loss: 0.5830474102595024, test loss: 0.710392753535911 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3728 training loss: 0.5829221846523601, test loss: 0.7103984875159531 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3729 training loss: 0.5827969095280214, test loss: 0.7104042301915908 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3730 training loss: 0.5826715849771764, test loss: 0.710409981754121 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3731 training loss: 0.5825462110905377, test loss: 0.7104157423947133 \n",
            "train accuracy: 0.7468581687612208, test accuracy: 0.5357142857142857\n",
            "Iteration 3732 training loss: 0.5824207879588377, test loss: 0.7104215123044082 \n",
            "train accuracy: 0.748653500897666, test accuracy: 0.5357142857142857\n",
            "Iteration 3733 training loss: 0.5822953156728286, test loss: 0.7104272916741121 \n",
            "train accuracy: 0.7504488330341114, test accuracy: 0.5357142857142857\n",
            "Iteration 3734 training loss: 0.58216979432328, test loss: 0.7104330806945949 \n",
            "train accuracy: 0.7504488330341114, test accuracy: 0.5357142857142857\n",
            "Iteration 3735 training loss: 0.5820442240009788, test loss: 0.7104388795564861 \n",
            "train accuracy: 0.7522441651705566, test accuracy: 0.5357142857142857\n",
            "Iteration 3736 training loss: 0.5819186047967266, test loss: 0.7104446884502716 \n",
            "train accuracy: 0.7522441651705566, test accuracy: 0.5357142857142857\n",
            "Iteration 3737 training loss: 0.5817929368013394, test loss: 0.7104505075662906 \n",
            "train accuracy: 0.7522441651705566, test accuracy: 0.5357142857142857\n",
            "Iteration 3738 training loss: 0.5816672201056459, test loss: 0.7104563370947315 \n",
            "train accuracy: 0.7522441651705566, test accuracy: 0.5357142857142857\n",
            "Iteration 3739 training loss: 0.5815414548004856, test loss: 0.7104621772256299 \n",
            "train accuracy: 0.7522441651705566, test accuracy: 0.5357142857142857\n",
            "Iteration 3740 training loss: 0.5814156409767091, test loss: 0.7104680281488632 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3741 training loss: 0.5812897787251748, test loss: 0.7104738900541493 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3742 training loss: 0.5811638681367494, test loss: 0.7104797631310422 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3743 training loss: 0.581037909302305, test loss: 0.7104856475689288 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3744 training loss: 0.5809119023127195, test loss: 0.7104915435570256 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3745 training loss: 0.5807858472588738, test loss: 0.7104974512843758 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3746 training loss: 0.5806597442316513, test loss: 0.7105033709398453 \n",
            "train accuracy: 0.7540394973070018, test accuracy: 0.5357142857142857\n",
            "Iteration 3747 training loss: 0.5805335933219367, test loss: 0.7105093027121202 \n",
            "train accuracy: 0.755834829443447, test accuracy: 0.5357142857142857\n",
            "Iteration 3748 training loss: 0.5804073946206139, test loss: 0.7105152467897028 \n",
            "train accuracy: 0.755834829443447, test accuracy: 0.5357142857142857\n",
            "Iteration 3749 training loss: 0.5802811482185659, test loss: 0.7105212033609087 \n",
            "train accuracy: 0.7576301615798923, test accuracy: 0.5357142857142857\n",
            "Iteration 3750 training loss: 0.5801548542066727, test loss: 0.7105271726138636 \n",
            "train accuracy: 0.7576301615798923, test accuracy: 0.5428571428571428\n",
            "Iteration 3751 training loss: 0.5800285126758099, test loss: 0.7105331547364999 \n",
            "train accuracy: 0.7576301615798923, test accuracy: 0.5428571428571428\n",
            "Iteration 3752 training loss: 0.5799021237168486, test loss: 0.7105391499165534 \n",
            "train accuracy: 0.7576301615798923, test accuracy: 0.5428571428571428\n",
            "Iteration 3753 training loss: 0.5797756874206523, test loss: 0.7105451583415606 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3754 training loss: 0.5796492038780774, test loss: 0.7105511801988543 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3755 training loss: 0.579522673179971, test loss: 0.710557215675562 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3756 training loss: 0.5793960954171694, test loss: 0.7105632649586008 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3757 training loss: 0.5792694706804979, test loss: 0.7105693282346762 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3758 training loss: 0.5791427990607685, test loss: 0.7105754056902777 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3759 training loss: 0.5790160806487791, test loss: 0.7105814975116751 \n",
            "train accuracy: 0.7594254937163375, test accuracy: 0.5428571428571428\n",
            "Iteration 3760 training loss: 0.5788893155353121, test loss: 0.7105876038849173 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3761 training loss: 0.5787625038111336, test loss: 0.7105937249958271 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3762 training loss: 0.5786356455669914, test loss: 0.7105998610299994 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3763 training loss: 0.5785087408936144, test loss: 0.710606012172797 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3764 training loss: 0.5783817898817111, test loss: 0.7106121786093483 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3765 training loss: 0.5782547926219682, test loss: 0.7106183605245444 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3766 training loss: 0.5781277492050498, test loss: 0.7106245581030349 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3767 training loss: 0.5780006597215958, test loss: 0.7106307715292253 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3768 training loss: 0.5778735242622206, test loss: 0.7106370009872746 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3769 training loss: 0.5777463429175125, test loss: 0.7106432466610909 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3770 training loss: 0.5776191157780315, test loss: 0.7106495087343299 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3771 training loss: 0.577491842934309, test loss: 0.7106557873903904 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3772 training loss: 0.577364524476846, test loss: 0.7106620828124119 \n",
            "train accuracy: 0.7612208258527827, test accuracy: 0.5428571428571428\n",
            "Iteration 3773 training loss: 0.5772371604961124, test loss: 0.7106683951832719 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3774 training loss: 0.5771097510825448, test loss: 0.710674724685582 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3775 training loss: 0.5769822963265467, test loss: 0.7106810715016857 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3776 training loss: 0.5768547963184861, test loss: 0.7106874358136549 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3777 training loss: 0.5767272511486948, test loss: 0.7106938178032872 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3778 training loss: 0.5765996609074671, test loss: 0.7107002176521028 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3779 training loss: 0.5764720256850588, test loss: 0.7107066355413412 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3780 training loss: 0.5763443455716858, test loss: 0.7107130716519587 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3781 training loss: 0.5762166206575228, test loss: 0.7107195261646257 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3782 training loss: 0.5760888510327025, test loss: 0.7107259992597226 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3783 training loss: 0.5759610367873137, test loss: 0.710732491117338 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3784 training loss: 0.5758331780114013, test loss: 0.7107390019172657 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3785 training loss: 0.5757052747949636, test loss: 0.7107455318390007 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3786 training loss: 0.5755773272279526, test loss: 0.7107520810617377 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3787 training loss: 0.5754493354002717, test loss: 0.7107586497643674 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3788 training loss: 0.5753212994017751, test loss: 0.7107652381254738 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3789 training loss: 0.5751932193222665, test loss: 0.710771846323331 \n",
            "train accuracy: 0.7630161579892281, test accuracy: 0.5428571428571428\n",
            "Iteration 3790 training loss: 0.5750650952514981, test loss: 0.7107784745359015 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3791 training loss: 0.5749369272791687, test loss: 0.7107851229408315 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3792 training loss: 0.5748087154949235, test loss: 0.71079179171545 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3793 training loss: 0.5746804599883532, test loss: 0.7107984810367646 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3794 training loss: 0.5745521608489905, test loss: 0.710805191081459 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3795 training loss: 0.5744238181663122, test loss: 0.710811922025891 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3796 training loss: 0.5742954320297357, test loss: 0.7108186740460886 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3797 training loss: 0.5741670025286187, test loss: 0.7108254473177478 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3798 training loss: 0.5740385297522581, test loss: 0.7108322420162296 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3799 training loss: 0.5739100137898889, test loss: 0.7108390583165579 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3800 training loss: 0.5737814547306824, test loss: 0.7108458963934156 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3801 training loss: 0.5736528526637459, test loss: 0.7108527564211428 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3802 training loss: 0.5735242076781211, test loss: 0.7108596385737338 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5428571428571428\n",
            "Iteration 3803 training loss: 0.5733955198627833, test loss: 0.7108665430248344 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3804 training loss: 0.5732667893066397, test loss: 0.7108734699477394 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3805 training loss: 0.5731380160985289, test loss: 0.710880419515389 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3806 training loss: 0.5730092003272194, test loss: 0.7108873919003679 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3807 training loss: 0.5728803420814087, test loss: 0.7108943872749006 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3808 training loss: 0.5727514414497219, test loss: 0.7109014058108503 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3809 training loss: 0.572622498520711, test loss: 0.7109084476797155 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3810 training loss: 0.5724935133828531, test loss: 0.7109155130526278 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3811 training loss: 0.5723644861245504, test loss: 0.7109226021003487 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3812 training loss: 0.5722354168341277, test loss: 0.7109297149932678 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.55\n",
            "Iteration 3813 training loss: 0.5721063055998328, test loss: 0.7109368519013994 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5571428571428572\n",
            "Iteration 3814 training loss: 0.5719771525098339, test loss: 0.7109440129943806 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5571428571428572\n",
            "Iteration 3815 training loss: 0.57184795765222, test loss: 0.7109511984414684 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5571428571428572\n",
            "Iteration 3816 training loss: 0.5717187211149981, test loss: 0.7109584084115371 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5642857142857143\n",
            "Iteration 3817 training loss: 0.5715894429860942, test loss: 0.710965643073076 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5642857142857143\n",
            "Iteration 3818 training loss: 0.57146012335335, test loss: 0.7109729025941866 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5642857142857143\n",
            "Iteration 3819 training loss: 0.5713307623045233, test loss: 0.7109801871425806 \n",
            "train accuracy: 0.7648114901256733, test accuracy: 0.5642857142857143\n",
            "Iteration 3820 training loss: 0.5712013599272868, test loss: 0.7109874968855765 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5642857142857143\n",
            "Iteration 3821 training loss: 0.5710719163092266, test loss: 0.7109948319900984 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5714285714285714\n",
            "Iteration 3822 training loss: 0.5709424315378409, test loss: 0.7110021926226717 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5714285714285714\n",
            "Iteration 3823 training loss: 0.5708129057005392, test loss: 0.711009578949423 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3824 training loss: 0.5706833388846424, test loss: 0.7110169911360755 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3825 training loss: 0.5705537311773793, test loss: 0.7110244293479481 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3826 training loss: 0.5704240826658876, test loss: 0.7110318937499512 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3827 training loss: 0.5702943934372121, test loss: 0.711039384506587 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3828 training loss: 0.5701646635783035, test loss: 0.7110469017819442 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3829 training loss: 0.5700348931760175, test loss: 0.7110544457396979 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3830 training loss: 0.5699050823171142, test loss: 0.7110620165431059 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3831 training loss: 0.5697752310882559, test loss: 0.7110696143550063 \n",
            "train accuracy: 0.7666068222621185, test accuracy: 0.5785714285714286\n",
            "Iteration 3832 training loss: 0.5696453395760073, test loss: 0.7110772393378163 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3833 training loss: 0.5695154078668336, test loss: 0.711084891653529 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3834 training loss: 0.5693854360471006, test loss: 0.7110925714637107 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3835 training loss: 0.5692554242030717, test loss: 0.7111002789295001 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3836 training loss: 0.5691253724209088, test loss: 0.7111080142116042 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3837 training loss: 0.5689952807866703, test loss: 0.7111157774702974 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3838 training loss: 0.5688651493863105, test loss: 0.7111235688654184 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3839 training loss: 0.568734978305678, test loss: 0.7111313885563686 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3840 training loss: 0.5686047676305153, test loss: 0.7111392367021092 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3841 training loss: 0.5684745174464576, test loss: 0.7111471134611599 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3842 training loss: 0.5683442278390313, test loss: 0.7111550189915952 \n",
            "train accuracy: 0.7684021543985637, test accuracy: 0.5785714285714286\n",
            "Iteration 3843 training loss: 0.568213898893654, test loss: 0.711162953451044 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3844 training loss: 0.5680835306956327, test loss: 0.7111709169966861 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3845 training loss: 0.5679531233301627, test loss: 0.7111789097852507 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3846 training loss: 0.5678226768823275, test loss: 0.7111869319730135 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3847 training loss: 0.5676921914370964, test loss: 0.7111949837157964 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3848 training loss: 0.5675616670793254, test loss: 0.7112030651689623 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3849 training loss: 0.5674311038937541, test loss: 0.7112111764874159 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3850 training loss: 0.5673005019650064, test loss: 0.7112193178256004 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3851 training loss: 0.5671698613775886, test loss: 0.7112274893374954 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3852 training loss: 0.5670391822158891, test loss: 0.7112356911766146 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3853 training loss: 0.5669084645641767, test loss: 0.7112439234960042 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3854 training loss: 0.5667777085065994, test loss: 0.7112521864482411 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3855 training loss: 0.5666469141271852, test loss: 0.7112604801854299 \n",
            "train accuracy: 0.770197486535009, test accuracy: 0.5785714285714286\n",
            "Iteration 3856 training loss: 0.5665160815098391, test loss: 0.711268804859202 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3857 training loss: 0.5663852107383432, test loss: 0.7112771606207129 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3858 training loss: 0.5662543018963554, test loss: 0.7112855476206402 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3859 training loss: 0.5661233550674087, test loss: 0.7112939660091824 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3860 training loss: 0.5659923703349099, test loss: 0.7113024159360557 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3861 training loss: 0.5658613477821393, test loss: 0.7113108975504931 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3862 training loss: 0.5657302874922485, test loss: 0.7113194110012423 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3863 training loss: 0.5655991895482613, test loss: 0.7113279564365633 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3864 training loss: 0.5654680540330711, test loss: 0.711336534004227 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5785714285714286\n",
            "Iteration 3865 training loss: 0.5653368810294404, test loss: 0.7113451438515132 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3866 training loss: 0.5652056706200008, test loss: 0.7113537861252082 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3867 training loss: 0.5650744228872508, test loss: 0.7113624609716039 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3868 training loss: 0.5649431379135555, test loss: 0.7113711685364955 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3869 training loss: 0.564811815781146, test loss: 0.7113799089651792 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3870 training loss: 0.5646804565721176, test loss: 0.7113886824024515 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3871 training loss: 0.5645490603684298, test loss: 0.7113974889926061 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3872 training loss: 0.5644176272519045, test loss: 0.7114063288794333 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3873 training loss: 0.5642861573042263, test loss: 0.7114152022062172 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3874 training loss: 0.5641546506069399, test loss: 0.7114241091157352 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3875 training loss: 0.5640231072414514, test loss: 0.7114330497502545 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3876 training loss: 0.563891527289025, test loss: 0.7114420242515324 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3877 training loss: 0.5637599108307841, test loss: 0.711451032760813 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3878 training loss: 0.5636282579477094, test loss: 0.7114600754188264 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3879 training loss: 0.5634965687206384, test loss: 0.7114691523657866 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3880 training loss: 0.5633648432302636, test loss: 0.7114782637413902 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3881 training loss: 0.5632330815571333, test loss: 0.7114874096848138 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3882 training loss: 0.5631012837816497, test loss: 0.7114965903347142 \n",
            "train accuracy: 0.7719928186714542, test accuracy: 0.5857142857142857\n",
            "Iteration 3883 training loss: 0.5629694499840676, test loss: 0.7115058058292252 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3884 training loss: 0.5628375802444948, test loss: 0.7115150563059561 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3885 training loss: 0.5627056746428899, test loss: 0.7115243419019909 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3886 training loss: 0.5625737332590625, test loss: 0.7115336627538862 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3887 training loss: 0.5624417561726717, test loss: 0.7115430189976703 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3888 training loss: 0.5623097434632257, test loss: 0.7115524107688401 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3889 training loss: 0.5621776952100807, test loss: 0.7115618382023616 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3890 training loss: 0.5620456114924399, test loss: 0.7115713014326671 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3891 training loss: 0.5619134923893531, test loss: 0.7115808005936537 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3892 training loss: 0.5617813379797155, test loss: 0.7115903358186826 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3893 training loss: 0.5616491483422673, test loss: 0.7115999072405766 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3894 training loss: 0.5615169235555922, test loss: 0.7116095149916202 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3895 training loss: 0.5613846636981172, test loss: 0.711619159203556 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3896 training loss: 0.5612523688481114, test loss: 0.7116288400075855 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3897 training loss: 0.5611200390836859, test loss: 0.7116385575343659 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3898 training loss: 0.5609876744827914, test loss: 0.7116483119140101 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3899 training loss: 0.5608552751232195, test loss: 0.7116581032760845 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3900 training loss: 0.5607228410826004, test loss: 0.7116679317496075 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3901 training loss: 0.5605903724384026, test loss: 0.7116777974630493 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3902 training loss: 0.560457869267932, test loss: 0.711687700544329 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3903 training loss: 0.5603253316483311, test loss: 0.7116976411208147 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3904 training loss: 0.5601927596565786, test loss: 0.7117076193193211 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3905 training loss: 0.5600601533694879, test loss: 0.7117176352661093 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3906 training loss: 0.5599275128637075, test loss: 0.7117276890868846 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3907 training loss: 0.5597948382157185, test loss: 0.7117377809067954 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3908 training loss: 0.5596621295018355, test loss: 0.7117479108504327 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3909 training loss: 0.559529386798205, test loss: 0.7117580790418282 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3910 training loss: 0.5593966101808047, test loss: 0.7117682856044528 \n",
            "train accuracy: 0.7737881508078994, test accuracy: 0.5857142857142857\n",
            "Iteration 3911 training loss: 0.559263799725443, test loss: 0.7117785306612167 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3912 training loss: 0.559130955507758, test loss: 0.7117888143344668 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3913 training loss: 0.5589980776032168, test loss: 0.7117991367459863 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3914 training loss: 0.5588651660871152, test loss: 0.7118094980169938 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3915 training loss: 0.558732221034576, test loss: 0.7118198982681412 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3916 training loss: 0.5585992425205494, test loss: 0.7118303376195136 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3917 training loss: 0.5584662306198113, test loss: 0.7118408161906278 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3918 training loss: 0.5583331854069632, test loss: 0.711851334100431 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3919 training loss: 0.5582001069564316, test loss: 0.7118618914673002 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3920 training loss: 0.5580669953424667, test loss: 0.7118724884090409 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3921 training loss: 0.5579338506391418, test loss: 0.7118831250428863 \n",
            "train accuracy: 0.7755834829443446, test accuracy: 0.5857142857142857\n",
            "Iteration 3922 training loss: 0.5578006729203533, test loss: 0.7118938014854953 \n",
            "train accuracy: 0.77737881508079, test accuracy: 0.5857142857142857\n",
            "Iteration 3923 training loss: 0.5576674622598187, test loss: 0.7119045178529534 \n",
            "train accuracy: 0.77737881508079, test accuracy: 0.5857142857142857\n",
            "Iteration 3924 training loss: 0.5575342187310774, test loss: 0.71191527426077 \n",
            "train accuracy: 0.77737881508079, test accuracy: 0.5857142857142857\n",
            "Iteration 3925 training loss: 0.5574009424074893, test loss: 0.7119260708238779 \n",
            "train accuracy: 0.77737881508079, test accuracy: 0.5857142857142857\n",
            "Iteration 3926 training loss: 0.5572676333622334, test loss: 0.7119369076566331 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3927 training loss: 0.5571342916683087, test loss: 0.711947784872813 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3928 training loss: 0.5570009173985322, test loss: 0.7119587025856152 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3929 training loss: 0.5568675106255385, test loss: 0.7119696609076582 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3930 training loss: 0.55673407142178, test loss: 0.7119806599509788 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3931 training loss: 0.5566005998595248, test loss: 0.711991699827032 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3932 training loss: 0.5564670960108573, test loss: 0.7120027806466903 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3933 training loss: 0.556333559947677, test loss: 0.7120139025202421 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3934 training loss: 0.5561999917416978, test loss: 0.712025065557392 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3935 training loss: 0.5560663914644473, test loss: 0.712036269867259 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3936 training loss: 0.5559327591872667, test loss: 0.7120475155583762 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3937 training loss: 0.5557990949813094, test loss: 0.7120588027386899 \n",
            "train accuracy: 0.7791741472172352, test accuracy: 0.5857142857142857\n",
            "Iteration 3938 training loss: 0.5556653989175414, test loss: 0.7120701315155592 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3939 training loss: 0.5555316710667393, test loss: 0.7120815019957542 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3940 training loss: 0.5553979114994908, test loss: 0.7120929142854564 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3941 training loss: 0.5552641202861938, test loss: 0.712104368490258 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3942 training loss: 0.5551302974970553, test loss: 0.7121158647151601 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3943 training loss: 0.5549964432020918, test loss: 0.712127403064573 \n",
            "train accuracy: 0.7809694793536804, test accuracy: 0.5857142857142857\n",
            "Iteration 3944 training loss: 0.5548625574711277, test loss: 0.7121389836423154 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3945 training loss: 0.554728640373795, test loss: 0.7121506065516133 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3946 training loss: 0.5545946919795329, test loss: 0.7121622718951 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3947 training loss: 0.5544607123575878, test loss: 0.7121739797748147 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3948 training loss: 0.554326701577011, test loss: 0.7121857302922026 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3949 training loss: 0.5541926597066598, test loss: 0.7121975235481143 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3950 training loss: 0.5540585868151962, test loss: 0.7122093596428044 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3951 training loss: 0.5539244829710864, test loss: 0.7122212386759322 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3952 training loss: 0.5537903482426003, test loss: 0.7122331607465601 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3953 training loss: 0.553656182697811, test loss: 0.7122451259531536 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3954 training loss: 0.5535219864045943, test loss: 0.7122571343935807 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3955 training loss: 0.5533877594306275, test loss: 0.7122691861651111 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5857142857142857\n",
            "Iteration 3956 training loss: 0.55325350184339, test loss: 0.7122812813644167 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5928571428571429\n",
            "Iteration 3957 training loss: 0.5531192137101619, test loss: 0.7122934200875696 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5928571428571429\n",
            "Iteration 3958 training loss: 0.5529848950980238, test loss: 0.7123056024300429 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5928571428571429\n",
            "Iteration 3959 training loss: 0.5528505460738562, test loss: 0.7123178284867102 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5928571428571429\n",
            "Iteration 3960 training loss: 0.5527161667043391, test loss: 0.7123300983518444 \n",
            "train accuracy: 0.7827648114901257, test accuracy: 0.5928571428571429\n",
            "Iteration 3961 training loss: 0.552581757055951, test loss: 0.7123424121191178 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3962 training loss: 0.5524473171949693, test loss: 0.712354769881602 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3963 training loss: 0.5523128471874689, test loss: 0.7123671717317669 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3964 training loss: 0.5521783470993221, test loss: 0.712379617761481 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3965 training loss: 0.5520438169961983, test loss: 0.7123921080620105 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3966 training loss: 0.5519092569435633, test loss: 0.7124046427240192 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3967 training loss: 0.5517746670066783, test loss: 0.7124172218375685 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3968 training loss: 0.5516400472506007, test loss: 0.7124298454921164 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3969 training loss: 0.5515053977401823, test loss: 0.712442513776518 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3970 training loss: 0.5513707185400694, test loss: 0.7124552267790251 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3971 training loss: 0.5512360097147025, test loss: 0.712467984587285 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3972 training loss: 0.5511012713283161, test loss: 0.712480787288342 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3973 training loss: 0.5509665034449369, test loss: 0.7124936349686353 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3974 training loss: 0.5508317061283852, test loss: 0.7125065277140007 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3975 training loss: 0.5506968794422726, test loss: 0.7125194656096685 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3976 training loss: 0.5505620234500037, test loss: 0.7125324487402651 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3977 training loss: 0.5504271382147732, test loss: 0.7125454771898114 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3978 training loss: 0.5502922237995677, test loss: 0.7125585510417237 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3979 training loss: 0.5501572802671639, test loss: 0.7125716703788132 \n",
            "train accuracy: 0.7845601436265709, test accuracy: 0.5928571428571429\n",
            "Iteration 3980 training loss: 0.5500223076801286, test loss: 0.7125848352832858 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3981 training loss: 0.5498873061008187, test loss: 0.712598045836742 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3982 training loss: 0.54975227559138, test loss: 0.7126113021201773 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3983 training loss: 0.5496172162137473, test loss: 0.7126246042139812 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3984 training loss: 0.549482128029644, test loss: 0.712637952197938 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3985 training loss: 0.5493470111005818, test loss: 0.712651346151227 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3986 training loss: 0.5492118654878597, test loss: 0.7126647861524213 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3987 training loss: 0.5490766912525645, test loss: 0.712678272279489 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3988 training loss: 0.54894148845557, test loss: 0.7126918046097921 \n",
            "train accuracy: 0.7863554757630161, test accuracy: 0.5928571428571429\n",
            "Iteration 3989 training loss: 0.5488062571575363, test loss: 0.7127053832200879 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3990 training loss: 0.5486709974189101, test loss: 0.712719008186528 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3991 training loss: 0.5485357092999241, test loss: 0.7127326795846584 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3992 training loss: 0.5484003928605962, test loss: 0.7127463974894201 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3993 training loss: 0.5482650481607299, test loss: 0.7127601619751491 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3994 training loss: 0.5481296752599134, test loss: 0.7127739731155763 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3995 training loss: 0.5479942742175199, test loss: 0.7127878309838274 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3996 training loss: 0.5478588450927062, test loss: 0.7128017356524234 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3997 training loss: 0.5477233879444137, test loss: 0.7128156871932808 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3998 training loss: 0.5475879028313665, test loss: 0.712829685677712 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 3999 training loss: 0.5474523898120733, test loss: 0.7128437311764242 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 4000 training loss: 0.5473168489448246, test loss: 0.7128578237595213 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 4001 training loss: 0.5471812802876943, test loss: 0.712871963496503 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 4002 training loss: 0.5470456838985388, test loss: 0.7128861504562656 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 4003 training loss: 0.5469100598349961, test loss: 0.7129003847071016 \n",
            "train accuracy: 0.7881508078994613, test accuracy: 0.5928571428571429\n",
            "Iteration 4004 training loss: 0.5467744081544863, test loss: 0.7129146663167008 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4005 training loss: 0.5466387289142113, test loss: 0.7129289953521497 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4006 training loss: 0.546503022171154, test loss: 0.7129433718799325 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4007 training loss: 0.5463672879820785, test loss: 0.7129577959659313 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4008 training loss: 0.5462315264035295, test loss: 0.7129722676754258 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4009 training loss: 0.5460957374918325, test loss: 0.712986787073094 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4010 training loss: 0.5459599213030936, test loss: 0.7130013542230131 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4011 training loss: 0.5458240778931978, test loss: 0.7130159691886594 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4012 training loss: 0.5456882073178112, test loss: 0.7130306320329074 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4013 training loss: 0.5455523096323784, test loss: 0.7130453428180331 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4014 training loss: 0.5454163848921245, test loss: 0.7130601016057118 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4015 training loss: 0.5452804331520527, test loss: 0.7130749084570193 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4016 training loss: 0.5451444544669455, test loss: 0.7130897634324329 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4017 training loss: 0.5450084488913645, test loss: 0.7131046665918311 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4018 training loss: 0.544872416479649, test loss: 0.7131196179944944 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4019 training loss: 0.5447363572859175, test loss: 0.7131346176991061 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4020 training loss: 0.5446002713640663, test loss: 0.7131496657637517 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4021 training loss: 0.5444641587677691, test loss: 0.7131647622459204 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4022 training loss: 0.5443280195504783, test loss: 0.7131799072025059 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4023 training loss: 0.5441918537654232, test loss: 0.7131951006898051 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4024 training loss: 0.5440556614656109, test loss: 0.7132103427635211 \n",
            "train accuracy: 0.7899461400359067, test accuracy: 0.5928571428571429\n",
            "Iteration 4025 training loss: 0.5439194427038256, test loss: 0.7132256334787619 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4026 training loss: 0.5437831975326287, test loss: 0.7132409728900415 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4027 training loss: 0.5436469260043585, test loss: 0.713256361051281 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4028 training loss: 0.5435106281711298, test loss: 0.7132717980158083 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4029 training loss: 0.5433743040848351, test loss: 0.7132872838363596 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4030 training loss: 0.5432379537971422, test loss: 0.7133028185650794 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4031 training loss: 0.543101577359496, test loss: 0.7133184022535211 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4032 training loss: 0.5429651748231177, test loss: 0.7133340349526482 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4033 training loss: 0.5428287462390046, test loss: 0.7133497167128345 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4034 training loss: 0.5426922916579296, test loss: 0.7133654475838648 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4035 training loss: 0.5425558111304426, test loss: 0.7133812276149356 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4036 training loss: 0.5424193047068683, test loss: 0.7133970568546567 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4037 training loss: 0.5422827724373078, test loss: 0.7134129353510499 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4038 training loss: 0.5421462143716377, test loss: 0.7134288631515517 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4039 training loss: 0.5420096305595103, test loss: 0.7134448403030128 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4040 training loss: 0.5418730210503533, test loss: 0.7134608668517 \n",
            "train accuracy: 0.7917414721723519, test accuracy: 0.5928571428571429\n",
            "Iteration 4041 training loss: 0.5417363858933698, test loss: 0.7134769428432952 \n",
            "train accuracy: 0.7935368043087971, test accuracy: 0.5928571428571429\n",
            "Iteration 4042 training loss: 0.5415997251375385, test loss: 0.7134930683228983 \n",
            "train accuracy: 0.7935368043087971, test accuracy: 0.5928571428571429\n",
            "Iteration 4043 training loss: 0.5414630388316135, test loss: 0.7135092433350265 \n",
            "train accuracy: 0.7935368043087971, test accuracy: 0.5928571428571429\n",
            "Iteration 4044 training loss: 0.541326327024124, test loss: 0.7135254679236153 \n",
            "train accuracy: 0.7935368043087971, test accuracy: 0.5928571428571429\n",
            "Iteration 4045 training loss: 0.5411895897633743, test loss: 0.7135417421320198 \n",
            "train accuracy: 0.7935368043087971, test accuracy: 0.5928571428571429\n",
            "Iteration 4046 training loss: 0.541052827097444, test loss: 0.7135580660030154 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4047 training loss: 0.5409160390741881, test loss: 0.7135744395787983 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4048 training loss: 0.5407792257412365, test loss: 0.7135908629009867 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4049 training loss: 0.5406423871459942, test loss: 0.7136073360106213 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4050 training loss: 0.5405055233356415, test loss: 0.7136238589481668 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4051 training loss: 0.5403686343571334, test loss: 0.713640431753512 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4052 training loss: 0.5402317202572005, test loss: 0.7136570544659715 \n",
            "train accuracy: 0.7953321364452424, test accuracy: 0.5928571428571429\n",
            "Iteration 4053 training loss: 0.5400947810823478, test loss: 0.7136737271242857 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4054 training loss: 0.5399578168788557, test loss: 0.7136904497666227 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4055 training loss: 0.5398208276927797, test loss: 0.7137072224305783 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4056 training loss: 0.5396838135699507, test loss: 0.7137240451531781 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4057 training loss: 0.539546774555974, test loss: 0.7137409179708772 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4058 training loss: 0.5394097106962304, test loss: 0.7137578409195621 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4059 training loss: 0.5392726220358761, test loss: 0.7137748140345507 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4060 training loss: 0.5391355086198422, test loss: 0.7137918373505946 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4061 training loss: 0.538998370492835, test loss: 0.7138089109018793 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4062 training loss: 0.5388612076993364, test loss: 0.7138260347220253 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4063 training loss: 0.5387240202836034, test loss: 0.7138432088440892 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4064 training loss: 0.5385868082896685, test loss: 0.7138604333005645 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4065 training loss: 0.5384495717613397, test loss: 0.7138777081233832 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4066 training loss: 0.5383123107422005, test loss: 0.7138950333439164 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4067 training loss: 0.53817502527561, test loss: 0.7139124089929753 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4068 training loss: 0.5380377154047031, test loss: 0.7139298351008129 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4069 training loss: 0.5379003811723904, test loss: 0.7139473116971246 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4070 training loss: 0.5377630226213584, test loss: 0.7139648388110492 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4071 training loss: 0.5376256397940696, test loss: 0.7139824164711703 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.5928571428571429\n",
            "Iteration 4072 training loss: 0.5374882327327628, test loss: 0.7140000447055177 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.6\n",
            "Iteration 4073 training loss: 0.5373508014794527, test loss: 0.7140177235415677 \n",
            "train accuracy: 0.7971274685816876, test accuracy: 0.6\n",
            "Iteration 4074 training loss: 0.5372133460759302, test loss: 0.7140354530062449 \n",
            "train accuracy: 0.7989228007181328, test accuracy: 0.6\n",
            "Iteration 4075 training loss: 0.5370758665637633, test loss: 0.7140532331259233 \n",
            "train accuracy: 0.7989228007181328, test accuracy: 0.6\n",
            "Iteration 4076 training loss: 0.5369383629842958, test loss: 0.7140710639264276 \n",
            "train accuracy: 0.7989228007181328, test accuracy: 0.6\n",
            "Iteration 4077 training loss: 0.5368008353786485, test loss: 0.7140889454330339 \n",
            "train accuracy: 0.800718132854578, test accuracy: 0.6\n",
            "Iteration 4078 training loss: 0.5366632837877192, test loss: 0.7141068776704711 \n",
            "train accuracy: 0.800718132854578, test accuracy: 0.6\n",
            "Iteration 4079 training loss: 0.5365257082521825, test loss: 0.7141248606629221 \n",
            "train accuracy: 0.800718132854578, test accuracy: 0.6\n",
            "Iteration 4080 training loss: 0.5363881088124902, test loss: 0.7141428944340256 \n",
            "train accuracy: 0.800718132854578, test accuracy: 0.6\n",
            "Iteration 4081 training loss: 0.5362504855088712, test loss: 0.7141609790068761 \n",
            "train accuracy: 0.8025134649910234, test accuracy: 0.6\n",
            "Iteration 4082 training loss: 0.5361128383813321, test loss: 0.7141791144040266 \n",
            "train accuracy: 0.8025134649910234, test accuracy: 0.6\n",
            "Iteration 4083 training loss: 0.5359751674696568, test loss: 0.7141973006474884 \n",
            "train accuracy: 0.8025134649910234, test accuracy: 0.6\n",
            "Iteration 4084 training loss: 0.5358374728134072, test loss: 0.7142155377587336 \n",
            "train accuracy: 0.8025134649910234, test accuracy: 0.6\n",
            "Iteration 4085 training loss: 0.5356997544519232, test loss: 0.7142338257586954 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4086 training loss: 0.5355620124243226, test loss: 0.71425216466777 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4087 training loss: 0.5354242467695015, test loss: 0.7142705545058173 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4088 training loss: 0.5352864575261349, test loss: 0.7142889952921635 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4089 training loss: 0.5351486447326761, test loss: 0.7143074870456002 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4090 training loss: 0.5350108084273574, test loss: 0.7143260297843883 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4091 training loss: 0.53487294864819, test loss: 0.7143446235262566 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4092 training loss: 0.534735065432965, test loss: 0.7143632682884057 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4093 training loss: 0.5345971588192526, test loss: 0.7143819640875078 \n",
            "train accuracy: 0.8043087971274686, test accuracy: 0.6\n",
            "Iteration 4094 training loss: 0.5344592288444029, test loss: 0.7144007109397081 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4095 training loss: 0.5343212755455459, test loss: 0.7144195088606272 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4096 training loss: 0.5341832989595916, test loss: 0.714438357865361 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4097 training loss: 0.5340452991232313, test loss: 0.7144572579684834 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4098 training loss: 0.533907276072936, test loss: 0.7144762091840469 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4099 training loss: 0.5337692298449582, test loss: 0.714495211525584 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4100 training loss: 0.5336311604753317, test loss: 0.7145142650061095 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4101 training loss: 0.5334930679998713, test loss: 0.7145333696381209 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4102 training loss: 0.533354952454174, test loss: 0.7145525254335995 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4103 training loss: 0.5332168138736184, test loss: 0.7145717324040138 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4104 training loss: 0.533078652293366, test loss: 0.7145909905603188 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4105 training loss: 0.5329404677483602, test loss: 0.7146102999129583 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4106 training loss: 0.5328022602733278, test loss: 0.7146296604718667 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4107 training loss: 0.5326640299027777, test loss: 0.7146490722464701 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4108 training loss: 0.5325257766710039, test loss: 0.7146685352456873 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4109 training loss: 0.5323875006120827, test loss: 0.7146880494779324 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4110 training loss: 0.532249201759875, test loss: 0.7147076149511155 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4111 training loss: 0.5321108801480261, test loss: 0.7147272316726442 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4112 training loss: 0.5319725358099658, test loss: 0.7147468996494251 \n",
            "train accuracy: 0.8061041292639138, test accuracy: 0.6\n",
            "Iteration 4113 training loss: 0.5318341687789091, test loss: 0.7147666188878659 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4114 training loss: 0.5316957790878559, test loss: 0.7147863893938764 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4115 training loss: 0.5315573667695921, test loss: 0.7148062111728704 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4116 training loss: 0.5314189318566896, test loss: 0.7148260842297661 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4117 training loss: 0.5312804743815064, test loss: 0.7148460085689896 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4118 training loss: 0.5311419943761871, test loss: 0.7148659841944746 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4119 training loss: 0.5310034918726637, test loss: 0.7148860111096653 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4120 training loss: 0.5308649669026552, test loss: 0.7149060893175168 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4121 training loss: 0.5307264194976683, test loss: 0.714926218820498 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4122 training loss: 0.5305878496889982, test loss: 0.7149463996205918 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4123 training loss: 0.5304492575077281, test loss: 0.7149666317192979 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4124 training loss: 0.5303106429847305, test loss: 0.7149869151176333 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4125 training loss: 0.5301720061506665, test loss: 0.7150072498161351 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4126 training loss: 0.5300333470359873, test loss: 0.7150276358148608 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4127 training loss: 0.5298946656709337, test loss: 0.715048073113391 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4128 training loss: 0.5297559620855374, test loss: 0.7150685617108307 \n",
            "train accuracy: 0.8078994614003591, test accuracy: 0.6\n",
            "Iteration 4129 training loss: 0.5296172363096204, test loss: 0.7150891016058103 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4130 training loss: 0.529478488372796, test loss: 0.7151096927964884 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4131 training loss: 0.5293397183044692, test loss: 0.7151303352805526 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4132 training loss: 0.5292009261338368, test loss: 0.7151510290552208 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4133 training loss: 0.5290621118898885, test loss: 0.7151717741172446 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4134 training loss: 0.5289232756014058, test loss: 0.7151925704629087 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4135 training loss: 0.5287844172969647, test loss: 0.7152134180880343 \n",
            "train accuracy: 0.8096947935368043, test accuracy: 0.6\n",
            "Iteration 4136 training loss: 0.5286455370049338, test loss: 0.7152343169879796 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4137 training loss: 0.5285066347534766, test loss: 0.7152552671576424 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4138 training loss: 0.5283677105705509, test loss: 0.7152762685914615 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4139 training loss: 0.5282287644839092, test loss: 0.7152973212834182 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4140 training loss: 0.5280897965211, test loss: 0.7153184252270375 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4141 training loss: 0.5279508067094673, test loss: 0.7153395804153913 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4142 training loss: 0.5278117950761516, test loss: 0.7153607868410987 \n",
            "train accuracy: 0.8114901256732495, test accuracy: 0.6\n",
            "Iteration 4143 training loss: 0.5276727616480902, test loss: 0.7153820444963281 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4144 training loss: 0.5275337064520176, test loss: 0.7154033533727995 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4145 training loss: 0.5273946295144664, test loss: 0.7154247134617855 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4146 training loss: 0.5272555308617667, test loss: 0.7154461247541131 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4147 training loss: 0.5271164105200482, test loss: 0.715467587240166 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4148 training loss: 0.5269772685152391, test loss: 0.7154891009098859 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4149 training loss: 0.5268381048730676, test loss: 0.7155106657527743 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4150 training loss: 0.5266989196190622, test loss: 0.7155322817578942 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4151 training loss: 0.5265597127785517, test loss: 0.7155539489138715 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4152 training loss: 0.5264204843766661, test loss: 0.7155756672088984 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4153 training loss: 0.5262812344383374, test loss: 0.7155974366307327 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4154 training loss: 0.5261419629882995, test loss: 0.7156192571667016 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4155 training loss: 0.526002670051089, test loss: 0.7156411288037025 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.6\n",
            "Iteration 4156 training loss: 0.5258633556510459, test loss: 0.7156630515282046 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4157 training loss: 0.5257240198123138, test loss: 0.7156850253262521 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4158 training loss: 0.5255846625588403, test loss: 0.7157070501834638 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4159 training loss: 0.5254452839143782, test loss: 0.7157291260850369 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4160 training loss: 0.5253058839024854, test loss: 0.7157512530157472 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4161 training loss: 0.5251664625465255, test loss: 0.7157734309599527 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4162 training loss: 0.525027019869669, test loss: 0.7157956599015933 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4163 training loss: 0.5248875558948926, test loss: 0.7158179398241943 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4164 training loss: 0.5247480706449811, test loss: 0.7158402707108676 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4165 training loss: 0.5246085641425269, test loss: 0.7158626525443131 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4166 training loss: 0.5244690364099313, test loss: 0.7158850853068213 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4167 training loss: 0.5243294874694047, test loss: 0.7159075689802747 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4168 training loss: 0.5241899173429668, test loss: 0.7159301035461498 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4169 training loss: 0.5240503260524484, test loss: 0.7159526889855184 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4170 training loss: 0.5239107136194902, test loss: 0.7159753252790503 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4171 training loss: 0.5237710800655448, test loss: 0.7159980124070146 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4172 training loss: 0.5236314254118771, test loss: 0.7160207503492814 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4173 training loss: 0.5234917496795642, test loss: 0.7160435390853241 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4174 training loss: 0.5233520528894962, test loss: 0.7160663785942211 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4175 training loss: 0.5232123350623774, test loss: 0.7160892688546575 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4176 training loss: 0.5230725962187263, test loss: 0.7161122098449271 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4177 training loss: 0.5229328363788762, test loss: 0.7161352015429342 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4178 training loss: 0.5227930555629761, test loss: 0.7161582439261953 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4179 training loss: 0.5226532537909909, test loss: 0.7161813369718413 \n",
            "train accuracy: 0.8132854578096947, test accuracy: 0.5928571428571429\n",
            "Iteration 4180 training loss: 0.522513431082703, test loss: 0.7162044806566192 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4181 training loss: 0.5223735874577113, test loss: 0.716227674956894 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4182 training loss: 0.5222337229354331, test loss: 0.7162509198486504 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4183 training loss: 0.5220938375351043, test loss: 0.716274215307495 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4184 training loss: 0.5219539312757803, test loss: 0.7162975613086581 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4185 training loss: 0.5218140041763356, test loss: 0.716320957826995 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4186 training loss: 0.521674056255466, test loss: 0.7163444048369888 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4187 training loss: 0.521534087531688, test loss: 0.7163679023127516 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4188 training loss: 0.5213940980233398, test loss: 0.716391450228027 \n",
            "train accuracy: 0.8150807899461401, test accuracy: 0.5928571428571429\n",
            "Iteration 4189 training loss: 0.5212540877485823, test loss: 0.7164150485561912 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4190 training loss: 0.5211140567253992, test loss: 0.7164386972702554 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4191 training loss: 0.520974004971598, test loss: 0.7164623963428678 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4192 training loss: 0.5208339325048106, test loss: 0.716486145746315 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4193 training loss: 0.5206938393424934, test loss: 0.7165099454525246 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4194 training loss: 0.5205537255019291, test loss: 0.7165337954330663 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4195 training loss: 0.5204135910002264, test loss: 0.7165576956591545 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.5928571428571429\n",
            "Iteration 4196 training loss: 0.5202734358543211, test loss: 0.7165816461016496 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4197 training loss: 0.520133260080976, test loss: 0.7166056467310605 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4198 training loss: 0.5199930636967834, test loss: 0.7166296975175462 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4199 training loss: 0.5198528467181633, test loss: 0.716653798430917 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4200 training loss: 0.5197126091613661, test loss: 0.7166779494406386 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4201 training loss: 0.5195723510424722, test loss: 0.7167021505158306 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4202 training loss: 0.5194320723773932, test loss: 0.7167264016252721 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4203 training loss: 0.5192917731818723, test loss: 0.7167507027374007 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4204 training loss: 0.5191514534714852, test loss: 0.7167750538203159 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4205 training loss: 0.5190111132616398, test loss: 0.7167994548417811 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4206 training loss: 0.5188707525675793, test loss: 0.7168239057692243 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4207 training loss: 0.5187303714043802, test loss: 0.7168484065697414 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4208 training loss: 0.518589969786954, test loss: 0.7168729572100971 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4209 training loss: 0.518449547730049, test loss: 0.7168975576567277 \n",
            "train accuracy: 0.8186714542190305, test accuracy: 0.6\n",
            "Iteration 4210 training loss: 0.5183091052482494, test loss: 0.716922207875742 \n",
            "train accuracy: 0.8204667863554758, test accuracy: 0.6\n",
            "Iteration 4211 training loss: 0.5181686423559765, test loss: 0.7169469078329241 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4212 training loss: 0.5180281590674901, test loss: 0.7169716574937351 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4213 training loss: 0.5178876553968881, test loss: 0.7169964568233144 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4214 training loss: 0.5177471313581083, test loss: 0.7170213057864826 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4215 training loss: 0.5176065869649282, test loss: 0.7170462043477426 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4216 training loss: 0.5174660222309664, test loss: 0.7170711524712821 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4217 training loss: 0.5173254371696828, test loss: 0.7170961501209749 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4218 training loss: 0.5171848317943798, test loss: 0.7171211972603838 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4219 training loss: 0.5170442061182026, test loss: 0.7171462938527613 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4220 training loss: 0.5169035601541403, test loss: 0.7171714398610526 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4221 training loss: 0.5167628939150264, test loss: 0.7171966352478965 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4222 training loss: 0.5166222074135394, test loss: 0.7172218799756284 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4223 training loss: 0.5164815006622041, test loss: 0.7172471740062812 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4224 training loss: 0.5163407736733916, test loss: 0.7172725173015883 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4225 training loss: 0.5162000264593208, test loss: 0.7172979098229846 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4226 training loss: 0.5160592590320583, test loss: 0.7173233515316083 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4227 training loss: 0.51591847140352, test loss: 0.7173488423883038 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4228 training loss: 0.5157776635854713, test loss: 0.7173743823536228 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4229 training loss: 0.515636835589528, test loss: 0.717399971387827 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4230 training loss: 0.5154959874271571, test loss: 0.7174256094508887 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4231 training loss: 0.5153551191096776, test loss: 0.7174512965024942 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4232 training loss: 0.5152142306482611, test loss: 0.7174770325020443 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4233 training loss: 0.5150733220539323, test loss: 0.7175028174086576 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4234 training loss: 0.5149323933375709, test loss: 0.7175286511811718 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4235 training loss: 0.514791444509911, test loss: 0.7175545337781447 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4236 training loss: 0.5146504755815423, test loss: 0.717580465157858 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4237 training loss: 0.5145094865629115, test loss: 0.7176064452783174 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6\n",
            "Iteration 4238 training loss: 0.5143684774643223, test loss: 0.7176324740972558 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6142857142857143\n",
            "Iteration 4239 training loss: 0.5142274482959366, test loss: 0.7176585515721344 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6142857142857143\n",
            "Iteration 4240 training loss: 0.5140863990677749, test loss: 0.717684677660145 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6142857142857143\n",
            "Iteration 4241 training loss: 0.5139453297897174, test loss: 0.7177108523182117 \n",
            "train accuracy: 0.822262118491921, test accuracy: 0.6142857142857143\n",
            "Iteration 4242 training loss: 0.513804240471505, test loss: 0.717737075502993 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4243 training loss: 0.5136631311227393, test loss: 0.7177633471708839 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4244 training loss: 0.5135220017528844, test loss: 0.7177896672780166 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4245 training loss: 0.5133808523712666, test loss: 0.7178160357802642 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4246 training loss: 0.513239682987076, test loss: 0.7178424526332415 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4247 training loss: 0.5130984936093669, test loss: 0.7178689177923065 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4248 training loss: 0.512957284247059, test loss: 0.717895431212564 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6142857142857143\n",
            "Iteration 4249 training loss: 0.5128160549089377, test loss: 0.7179219928488654 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4250 training loss: 0.5126748056036551, test loss: 0.7179486026558117 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4251 training loss: 0.5125335363397308, test loss: 0.7179752605877558 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4252 training loss: 0.5123922471255529, test loss: 0.7180019665988033 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4253 training loss: 0.5122509379693784, test loss: 0.718028720642815 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4254 training loss: 0.5121096088793343, test loss: 0.7180555226734088 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4255 training loss: 0.5119682598634182, test loss: 0.7180823726439617 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4256 training loss: 0.5118268909294995, test loss: 0.7181092705076109 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4257 training loss: 0.5116855020853198, test loss: 0.718136216217257 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4258 training loss: 0.5115440933384934, test loss: 0.7181632097255642 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4259 training loss: 0.5114026646965094, test loss: 0.7181902509849637 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4260 training loss: 0.5112612161667309, test loss: 0.718217339947655 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4261 training loss: 0.5111197477563971, test loss: 0.718244476565607 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4262 training loss: 0.5109782594726233, test loss: 0.7182716607905616 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4263 training loss: 0.510836751322402, test loss: 0.7182988925740338 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4264 training loss: 0.5106952233126042, test loss: 0.7183261718673143 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4265 training loss: 0.510553675449979, test loss: 0.718353498621472 \n",
            "train accuracy: 0.8240574506283662, test accuracy: 0.6214285714285714\n",
            "Iteration 4266 training loss: 0.5104121077411558, test loss: 0.718380872787354 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6214285714285714\n",
            "Iteration 4267 training loss: 0.5102705201926442, test loss: 0.7184082943155902 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6214285714285714\n",
            "Iteration 4268 training loss: 0.5101289128108352, test loss: 0.7184357631565922 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6214285714285714\n",
            "Iteration 4269 training loss: 0.5099872856020019, test loss: 0.7184632792605573 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6214285714285714\n",
            "Iteration 4270 training loss: 0.5098456385723006, test loss: 0.7184908425774695 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4271 training loss: 0.5097039717277712, test loss: 0.7185184530571013 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4272 training loss: 0.5095622850743382, test loss: 0.7185461106490156 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4273 training loss: 0.5094205786178118, test loss: 0.7185738153025676 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4274 training loss: 0.5092788523638881, test loss: 0.7186015669669069 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4275 training loss: 0.509137106318151, test loss: 0.7186293655909787 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4276 training loss: 0.5089953404860715, test loss: 0.7186572111235263 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4277 training loss: 0.5088535548730103, test loss: 0.7186851035130924 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4278 training loss: 0.5087117494842172, test loss: 0.7187130427080207 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4279 training loss: 0.5085699243248325, test loss: 0.7187410286564594 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4280 training loss: 0.5084280793998882, test loss: 0.7187690613063602 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4281 training loss: 0.5082862147143081, test loss: 0.7187971406054824 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4282 training loss: 0.5081443302729091, test loss: 0.7188252665013941 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4283 training loss: 0.508002426080402, test loss: 0.7188534389414735 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4284 training loss: 0.5078605021413922, test loss: 0.7188816578729115 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4285 training loss: 0.5077185584603809, test loss: 0.7189099232427123 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4286 training loss: 0.5075765950417658, test loss: 0.7189382349976966 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4287 training loss: 0.5074346118898411, test loss: 0.7189665930845023 \n",
            "train accuracy: 0.8258527827648114, test accuracy: 0.6142857142857143\n",
            "Iteration 4288 training loss: 0.5072926090087997, test loss: 0.7189949974495871 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4289 training loss: 0.5071505864027334, test loss: 0.7190234480392295 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4290 training loss: 0.5070085440756334, test loss: 0.7190519447995309 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4291 training loss: 0.5068664820313924, test loss: 0.7190804876764184 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4292 training loss: 0.5067244002738035, test loss: 0.7191090766156442 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4293 training loss: 0.5065822988065628, test loss: 0.7191377115627893 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4294 training loss: 0.5064401776332695, test loss: 0.7191663924632654 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4295 training loss: 0.5062980367574269, test loss: 0.719195119262315 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4296 training loss: 0.5061558761824428, test loss: 0.7192238919050147 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4297 training loss: 0.5060136959116315, test loss: 0.719252710336276 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4298 training loss: 0.5058714959482132, test loss: 0.7192815745008478 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4299 training loss: 0.505729276295316, test loss: 0.719310484343318 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4300 training loss: 0.5055870369559762, test loss: 0.7193394398081139 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4301 training loss: 0.5054447779331394, test loss: 0.7193684408395062 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4302 training loss: 0.5053024992296609, test loss: 0.7193974873816092 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4303 training loss: 0.5051602008483073, test loss: 0.7194265793783826 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4304 training loss: 0.5050178827917569, test loss: 0.7194557167736337 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4305 training loss: 0.5048755450626007, test loss: 0.7194848995110193 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4306 training loss: 0.5047331876633427, test loss: 0.7195141275340463 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4307 training loss: 0.5045908105964021, test loss: 0.7195434007860746 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4308 training loss: 0.5044484138641127, test loss: 0.7195727192103186 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4309 training loss: 0.5043059974687242, test loss: 0.719602082749848 \n",
            "train accuracy: 0.8276481149012568, test accuracy: 0.6142857142857143\n",
            "Iteration 4310 training loss: 0.504163561412404, test loss: 0.7196314913475907 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4311 training loss: 0.504021105697237, test loss: 0.7196609449463335 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4312 training loss: 0.5038786303252263, test loss: 0.7196904434887247 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4313 training loss: 0.503736135298295, test loss: 0.7197199869172749 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4314 training loss: 0.5035936206182868, test loss: 0.7197495751743591 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4315 training loss: 0.5034510862869662, test loss: 0.7197792082022186 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4316 training loss: 0.5033085323060202, test loss: 0.7198088859429621 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4317 training loss: 0.5031659586770585, test loss: 0.7198386083385682 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4318 training loss: 0.503023365401615, test loss: 0.7198683753308857 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4319 training loss: 0.502880752481148, test loss: 0.7198981868616368 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4320 training loss: 0.5027381199170416, test loss: 0.7199280428724177 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4321 training loss: 0.5025954677106066, test loss: 0.719957943304701 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4322 training loss: 0.5024527958630806, test loss: 0.7199878880998364 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4323 training loss: 0.5023101043756298, test loss: 0.7200178771990529 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4324 training loss: 0.5021673932493497, test loss: 0.7200479105434606 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4325 training loss: 0.5020246624852649, test loss: 0.7200779880740522 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4326 training loss: 0.5018819120843319, test loss: 0.7201081097317046 \n",
            "train accuracy: 0.829443447037702, test accuracy: 0.6142857142857143\n",
            "Iteration 4327 training loss: 0.5017391420474377, test loss: 0.7201382754571797 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4328 training loss: 0.5015963523754031, test loss: 0.7201684851911275 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4329 training loss: 0.5014535430689816, test loss: 0.7201987388740869 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4330 training loss: 0.5013107141288607, test loss: 0.7202290364464868 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4331 training loss: 0.5011678655556638, test loss: 0.720259377848649 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4332 training loss: 0.5010249973499501, test loss: 0.7202897630207888 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4333 training loss: 0.5008821095122153, test loss: 0.7203201919030164 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4334 training loss: 0.5007392020428935, test loss: 0.7203506644353393 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4335 training loss: 0.5005962749423567, test loss: 0.7203811805576636 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4336 training loss: 0.5004533282109169, test loss: 0.7204117402097954 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4337 training loss: 0.5003103618488268, test loss: 0.7204423433314417 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4338 training loss: 0.5001673758562794, test loss: 0.7204729898622141 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4339 training loss: 0.5000243702334103, test loss: 0.7205036797416275 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4340 training loss: 0.4998813449802987, test loss: 0.7205344129091039 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4341 training loss: 0.49973830009696646, test loss: 0.7205651893039733 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4342 training loss: 0.49959523558338087, test loss: 0.7205960088654743 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4343 training loss: 0.4994521514394549, test loss: 0.7206268715327567 \n",
            "train accuracy: 0.8312387791741472, test accuracy: 0.6142857142857143\n",
            "Iteration 4344 training loss: 0.49930904766504747, test loss: 0.720657777244883 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6142857142857143\n",
            "Iteration 4345 training loss: 0.4991659242599652, test loss: 0.7206887259408299 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6142857142857143\n",
            "Iteration 4346 training loss: 0.4990227812239629, test loss: 0.7207197175594885 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6142857142857143\n",
            "Iteration 4347 training loss: 0.4988796185567441, test loss: 0.7207507520396679 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6142857142857143\n",
            "Iteration 4348 training loss: 0.49873643625796266, test loss: 0.7207818293200956 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6142857142857143\n",
            "Iteration 4349 training loss: 0.49859323432722297, test loss: 0.7208129493394186 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6214285714285714\n",
            "Iteration 4350 training loss: 0.49845001276408096, test loss: 0.7208441120362056 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6214285714285714\n",
            "Iteration 4351 training loss: 0.49830677156804554, test loss: 0.7208753173489484 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6285714285714286\n",
            "Iteration 4352 training loss: 0.4981635107385786, test loss: 0.7209065652160631 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6285714285714286\n",
            "Iteration 4353 training loss: 0.4980202302750963, test loss: 0.7209378555758923 \n",
            "train accuracy: 0.8330341113105925, test accuracy: 0.6357142857142857\n",
            "Iteration 4354 training loss: 0.49787693017696993, test loss: 0.720969188366705 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6357142857142857\n",
            "Iteration 4355 training loss: 0.4977336104435271, test loss: 0.7210005635267002 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6357142857142857\n",
            "Iteration 4356 training loss: 0.49759027107405174, test loss: 0.7210319809940062 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6357142857142857\n",
            "Iteration 4357 training loss: 0.4974469120677857, test loss: 0.7210634407066842 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6357142857142857\n",
            "Iteration 4358 training loss: 0.49730353342392963, test loss: 0.7210949426027278 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6357142857142857\n",
            "Iteration 4359 training loss: 0.4971601351416432, test loss: 0.7211264866200658 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4360 training loss: 0.4970167172200466, test loss: 0.7211580726965632 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4361 training loss: 0.4968732796582212, test loss: 0.7211897007700221 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4362 training loss: 0.4967298224552102, test loss: 0.7212213707781842 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4363 training loss: 0.4965863456100199, test loss: 0.7212530826587313 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4364 training loss: 0.4964428491216201, test loss: 0.7212848363492873 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4365 training loss: 0.49629933298894524, test loss: 0.7213166317874193 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4366 training loss: 0.4961557972108956, test loss: 0.7213484689106391 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4367 training loss: 0.49601224178633696, test loss: 0.7213803476564047 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4368 training loss: 0.49586866671410307, test loss: 0.7214122679621214 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4369 training loss: 0.4957250719929952, test loss: 0.7214442297651434 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4370 training loss: 0.49558145762178385, test loss: 0.7214762330027759 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4371 training loss: 0.4954378235992089, test loss: 0.7215082776122745 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4372 training loss: 0.4952941699239809, test loss: 0.7215403635308492 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4373 training loss: 0.49515049659478205, test loss: 0.7215724906956631 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4374 training loss: 0.49500680361026655, test loss: 0.7216046590438361 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4375 training loss: 0.49486309096906206, test loss: 0.7216368685124448 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4376 training loss: 0.49471935866976996, test loss: 0.7216691190385242 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4377 training loss: 0.49457560671096656, test loss: 0.7217014105590696 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4378 training loss: 0.4944318350912041, test loss: 0.721733743011037 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4379 training loss: 0.49428804380901087, test loss: 0.7217661163313449 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4380 training loss: 0.49414423286289305, test loss: 0.7217985304568764 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4381 training loss: 0.49400040225133457, test loss: 0.7218309853244788 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4382 training loss: 0.49385655197279893, test loss: 0.7218634808709664 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4383 training loss: 0.4937126820257293, test loss: 0.7218960170331211 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4384 training loss: 0.49356879240854984, test loss: 0.7219285937476941 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4385 training loss: 0.4934248831196659, test loss: 0.7219612109514072 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4386 training loss: 0.49328095415746553, test loss: 0.7219938685809534 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4387 training loss: 0.4931370055203202, test loss: 0.7220265665729989 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4388 training loss: 0.4929930372065856, test loss: 0.7220593048641845 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4389 training loss: 0.492849049214602, test loss: 0.722092083391126 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4390 training loss: 0.4927050415426958, test loss: 0.7221249020904167 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4391 training loss: 0.4925610141891802, test loss: 0.7221577608986274 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4392 training loss: 0.4924169671523553, test loss: 0.7221906597523086 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4393 training loss: 0.49227290043051003, test loss: 0.7222235985879915 \n",
            "train accuracy: 0.8348294434470377, test accuracy: 0.6285714285714286\n",
            "Iteration 4394 training loss: 0.49212881402192266, test loss: 0.7222565773421888 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4395 training loss: 0.49198470792486093, test loss: 0.7222895959513969 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4396 training loss: 0.49184058213758364, test loss: 0.722322654352096 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4397 training loss: 0.4916964366583415, test loss: 0.7223557524807519 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4398 training loss: 0.4915522714853773, test loss: 0.7223888902738178 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4399 training loss: 0.4914080866169274, test loss: 0.7224220676677345 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4400 training loss: 0.49126388205122234, test loss: 0.7224552845989322 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4401 training loss: 0.4911196577864874, test loss: 0.7224885410038311 \n",
            "train accuracy: 0.8366247755834829, test accuracy: 0.6285714285714286\n",
            "Iteration 4402 training loss: 0.4909754138209441, test loss: 0.7225218368188441 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4403 training loss: 0.49083115015281015, test loss: 0.7225551719803759 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4404 training loss: 0.49068686678030105, test loss: 0.7225885464248261 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4405 training loss: 0.4905425637016301, test loss: 0.7226219600885888 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4406 training loss: 0.4903982409150103, test loss: 0.7226554129080551 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4407 training loss: 0.49025389841865424, test loss: 0.7226889048196136 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4408 training loss: 0.49010953621077513, test loss: 0.7227224357596514 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4409 training loss: 0.489965154289588, test loss: 0.7227560056645562 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4410 training loss: 0.4898207526533102, test loss: 0.7227896144707159 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4411 training loss: 0.4896763313001621, test loss: 0.7228232621145212 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4412 training loss: 0.4895318902283681, test loss: 0.7228569485323664 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4413 training loss: 0.4893874294361576, test loss: 0.7228906736606497 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4414 training loss: 0.4892429489217655, test loss: 0.7229244374357757 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4415 training loss: 0.489098448683433, test loss: 0.7229582397941552 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4416 training loss: 0.4889539287194087, test loss: 0.7229920806722075 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4417 training loss: 0.48880938902794924, test loss: 0.7230259600063602 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4418 training loss: 0.48866482960731994, test loss: 0.7230598777330521 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4419 training loss: 0.48852025045579595, test loss: 0.7230938337887324 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4420 training loss: 0.48837565157166285, test loss: 0.7231278281098629 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4421 training loss: 0.48823103295321735, test loss: 0.7231618606329192 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4422 training loss: 0.4880863945987683, test loss: 0.7231959312943909 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4423 training loss: 0.4879417365066374, test loss: 0.723230040030784 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4424 training loss: 0.48779705867515993, test loss: 0.7232641867786204 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4425 training loss: 0.4876523611026857, test loss: 0.7232983714744403 \n",
            "train accuracy: 0.8384201077199281, test accuracy: 0.6285714285714286\n",
            "Iteration 4426 training loss: 0.4875076437875799, test loss: 0.7233325940548027 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4427 training loss: 0.48736290672822313, test loss: 0.7233668544562865 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4428 training loss: 0.4872181499230136, test loss: 0.7234011526154915 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4429 training loss: 0.48707337337036655, test loss: 0.7234354884690398 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4430 training loss: 0.48692857706871595, test loss: 0.7234698619535761 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4431 training loss: 0.48678376101651466, test loss: 0.7235042730057698 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4432 training loss: 0.4866389252122359, test loss: 0.7235387215623149 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4433 training loss: 0.4864940696543732, test loss: 0.7235732075599319 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4434 training loss: 0.4863491943414418, test loss: 0.7236077309353686 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4435 training loss: 0.48620429927197917, test loss: 0.7236422916254006 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4436 training loss: 0.4860593844445461, test loss: 0.7236768895668332 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4437 training loss: 0.4859144498577269, test loss: 0.7237115246965015 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4438 training loss: 0.485769495510131, test loss: 0.7237461969512723 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4439 training loss: 0.4856245214003925, test loss: 0.7237809062680443 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4440 training loss: 0.4854795275271722, test loss: 0.7238156525837495 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4441 training loss: 0.4853345138891576, test loss: 0.7238504358353541 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4442 training loss: 0.4851894804850645, test loss: 0.7238852559598594 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4443 training loss: 0.48504442731363623, test loss: 0.7239201128943031 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4444 training loss: 0.484899354373646, test loss: 0.7239550065757598 \n",
            "train accuracy: 0.8402154398563735, test accuracy: 0.6285714285714286\n",
            "Iteration 4445 training loss: 0.48475426166389696, test loss: 0.7239899369413422 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4446 training loss: 0.4846091491832226, test loss: 0.7240249039282021 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4447 training loss: 0.48446401693048863, test loss: 0.7240599074735313 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4448 training loss: 0.48431886490459247, test loss: 0.7240949475145625 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4449 training loss: 0.48417369310446473, test loss: 0.7241300239885701 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4450 training loss: 0.48402850152906995, test loss: 0.7241651368328715 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4451 training loss: 0.483883290177407, test loss: 0.7242002859848279 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4452 training loss: 0.48373805904851014, test loss: 0.7242354713818451 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4453 training loss: 0.48359280814144956, test loss: 0.7242706929613743 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4454 training loss: 0.4834475374553326, test loss: 0.724305950660913 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4455 training loss: 0.48330224698930374, test loss: 0.7243412444180068 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4456 training loss: 0.4831569367425458, test loss: 0.7243765741702491 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6285714285714286\n",
            "Iteration 4457 training loss: 0.4830116067142806, test loss: 0.7244119398552826 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6357142857142857\n",
            "Iteration 4458 training loss: 0.48286625690376994, test loss: 0.7244473414108 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6357142857142857\n",
            "Iteration 4459 training loss: 0.4827208873103158, test loss: 0.7244827787745447 \n",
            "train accuracy: 0.8420107719928187, test accuracy: 0.6357142857142857\n",
            "Iteration 4460 training loss: 0.4825754979332615, test loss: 0.7245182518843126 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6357142857142857\n",
            "Iteration 4461 training loss: 0.4824300887719924, test loss: 0.7245537606779517 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6357142857142857\n",
            "Iteration 4462 training loss: 0.4822846598259364, test loss: 0.7245893050933644 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4463 training loss: 0.48213921109456487, test loss: 0.7246248850685061 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4464 training loss: 0.4819937425773935, test loss: 0.7246605005413892 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4465 training loss: 0.4818482542739824, test loss: 0.7246961514500808 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4466 training loss: 0.48170274618393794, test loss: 0.7247318377327062 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4467 training loss: 0.481557218306912, test loss: 0.7247675593274476 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4468 training loss: 0.4814116706426041, test loss: 0.7248033161725468 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4469 training loss: 0.48126610319076124, test loss: 0.7248391082063042 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4470 training loss: 0.481120515951179, test loss: 0.7248749353670809 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4471 training loss: 0.48097490892370204, test loss: 0.7249107975932995 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4472 training loss: 0.480829282108225, test loss: 0.724946694823444 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4473 training loss: 0.480683635504693, test loss: 0.724982626996062 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4474 training loss: 0.48053796911310265, test loss: 0.7250185940497639 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4475 training loss: 0.4803922829335024, test loss: 0.7250545959232246 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4476 training loss: 0.4802465769659935, test loss: 0.7250906325551848 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4477 training loss: 0.48010085121073026, test loss: 0.7251267038844507 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4478 training loss: 0.479955105667922, test loss: 0.7251628098498953 \n",
            "train accuracy: 0.8438061041292639, test accuracy: 0.6428571428571429\n",
            "Iteration 4479 training loss: 0.4798093403378318, test loss: 0.7251989503904592 \n",
            "train accuracy: 0.8456014362657092, test accuracy: 0.6428571428571429\n",
            "Iteration 4480 training loss: 0.4796635552207789, test loss: 0.7252351254451516 \n",
            "train accuracy: 0.8456014362657092, test accuracy: 0.6428571428571429\n",
            "Iteration 4481 training loss: 0.47951775031713845, test loss: 0.7252713349530501 \n",
            "train accuracy: 0.8456014362657092, test accuracy: 0.6428571428571429\n",
            "Iteration 4482 training loss: 0.4793719256273428, test loss: 0.7253075788533031 \n",
            "train accuracy: 0.8456014362657092, test accuracy: 0.6428571428571429\n",
            "Iteration 4483 training loss: 0.4792260811518814, test loss: 0.7253438570851289 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4484 training loss: 0.4790802168913025, test loss: 0.7253801695878175 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4485 training loss: 0.47893433284621284, test loss: 0.7254165163007311 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4486 training loss: 0.4787884290172791, test loss: 0.7254528971633044 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4487 training loss: 0.47864250540522796, test loss: 0.7254893121150463 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4488 training loss: 0.47849656201084745, test loss: 0.7255257610955392 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4489 training loss: 0.47835059883498715, test loss: 0.7255622440444416 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4490 training loss: 0.4782046158785587, test loss: 0.7255987609014872 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4491 training loss: 0.4780586131425372, test loss: 0.7256353116064862 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4492 training loss: 0.477912590627961, test loss: 0.7256718960993265 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4493 training loss: 0.47776654833593313, test loss: 0.7257085143199731 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4494 training loss: 0.47762048626762116, test loss: 0.7257451662084707 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4495 training loss: 0.47747440442425915, test loss: 0.7257818517049426 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4496 training loss: 0.4773283028071467, test loss: 0.7258185707495927 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4497 training loss: 0.4771821814176507, test loss: 0.7258553232827052 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4498 training loss: 0.4770360402572057, test loss: 0.7258921092446459 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4499 training loss: 0.47688987932731475, test loss: 0.7259289285758629 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4500 training loss: 0.47674369862954935, test loss: 0.7259657812168866 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4501 training loss: 0.4765974981655509, test loss: 0.7260026671083316 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4502 training loss: 0.4764512779370313, test loss: 0.7260395861908961 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4503 training loss: 0.47630503794577267, test loss: 0.7260765384053633 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4504 training loss: 0.47615877819362945, test loss: 0.7261135236926017 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4505 training loss: 0.4760124986825276, test loss: 0.726150541993566 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4506 training loss: 0.47586619941446623, test loss: 0.726187593249298 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4507 training loss: 0.475719880391518, test loss: 0.7262246774009264 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4508 training loss: 0.47557354161582915, test loss: 0.726261794389668 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4509 training loss: 0.47542718308962106, test loss: 0.7262989441568284 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4510 training loss: 0.4752808048151905, test loss: 0.7263361266438029 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4511 training loss: 0.4751344067949099, test loss: 0.7263733417920761 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4512 training loss: 0.4749879890312283, test loss: 0.7264105895432234 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4513 training loss: 0.47484155152667235, test loss: 0.7264478698389116 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4514 training loss: 0.4746950942838461, test loss: 0.726485182620899 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4515 training loss: 0.4745486173054322, test loss: 0.7265225278310363 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4516 training loss: 0.4744021205941921, test loss: 0.7265599054112675 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4517 training loss: 0.47425560415296764, test loss: 0.7265973153036298 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4518 training loss: 0.47410906798468017, test loss: 0.7266347574502547 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4519 training loss: 0.47396251209233203, test loss: 0.726672231793369 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4520 training loss: 0.4738159364790077, test loss: 0.7267097382752938 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4521 training loss: 0.47366934114787296, test loss: 0.7267472768384475 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4522 training loss: 0.4735227261021768, test loss: 0.7267848474253439 \n",
            "train accuracy: 0.8473967684021544, test accuracy: 0.6357142857142857\n",
            "Iteration 4523 training loss: 0.4733760913452512, test loss: 0.7268224499785942 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4524 training loss: 0.47322943688051217, test loss: 0.7268600844409079 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4525 training loss: 0.4730827627114602, test loss: 0.7268977507550917 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4526 training loss: 0.4729360688416809, test loss: 0.7269354488640516 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4527 training loss: 0.4727893552748453, test loss: 0.7269731787107934 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4528 training loss: 0.47264262201471086, test loss: 0.7270109402384219 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4529 training loss: 0.47249586906512175, test loss: 0.7270487333901431 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4530 training loss: 0.4723490964300099, test loss: 0.7270865581092634 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4531 training loss: 0.47220230411339476, test loss: 0.7271244143391913 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4532 training loss: 0.4720554921193845, test loss: 0.727162302023437 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4533 training loss: 0.47190866045217666, test loss: 0.727200221105613 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4534 training loss: 0.4717618091160579, test loss: 0.7272381715294357 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4535 training loss: 0.4716149381154058, test loss: 0.7272761532387246 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4536 training loss: 0.47146804745468845, test loss: 0.7273141661774034 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4537 training loss: 0.4713211371384652, test loss: 0.7273522102895004 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4538 training loss: 0.4711742071713878, test loss: 0.7273902855191493 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4539 training loss: 0.4710272575582002, test loss: 0.7274283918105894 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4540 training loss: 0.4708802883037393, test loss: 0.7274665291081662 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4541 training loss: 0.4707332994129358, test loss: 0.7275046973563318 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4542 training loss: 0.470586290890815, test loss: 0.7275428964996454 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4543 training loss: 0.4704392627424958, test loss: 0.7275811264827742 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4544 training loss: 0.47029221497319357, test loss: 0.727619387250493 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4545 training loss: 0.47014514758821885, test loss: 0.7276576787476857 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4546 training loss: 0.4699980605929787, test loss: 0.7276960009193453 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4547 training loss: 0.469850953992977, test loss: 0.727734353710574 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4548 training loss: 0.46970382779381503, test loss: 0.7277727370665843 \n",
            "train accuracy: 0.8491921005385996, test accuracy: 0.6357142857142857\n",
            "Iteration 4549 training loss: 0.4695566820011922, test loss: 0.7278111509326991 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4550 training loss: 0.4694095166209061, test loss: 0.7278495952543527 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4551 training loss: 0.4692623316588538, test loss: 0.7278880699770901 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4552 training loss: 0.4691151271210313, test loss: 0.7279265750465687 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4553 training loss: 0.4689679030135353, test loss: 0.7279651104085582 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4554 training loss: 0.4688206593425627, test loss: 0.7280036760089408 \n",
            "train accuracy: 0.8509874326750448, test accuracy: 0.6357142857142857\n",
            "Iteration 4555 training loss: 0.4686733961144113, test loss: 0.728042271793712 \n",
            "train accuracy: 0.8527827648114902, test accuracy: 0.6357142857142857\n",
            "Iteration 4556 training loss: 0.46852611333548105, test loss: 0.7280808977089815 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4557 training loss: 0.46837881101227385, test loss: 0.7281195537009723 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4558 training loss: 0.4682314891513942, test loss: 0.7281582397160223 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4559 training loss: 0.46808414775954915, test loss: 0.7281969557005845 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4560 training loss: 0.46793678684355056, test loss: 0.7282357016012271 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4561 training loss: 0.4677894064103134, test loss: 0.728274477364634 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4562 training loss: 0.46764200646685783, test loss: 0.7283132829376053 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4563 training loss: 0.46749458702030905, test loss: 0.7283521182670583 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4564 training loss: 0.46734714807789784, test loss: 0.7283909833000264 \n",
            "train accuracy: 0.8545780969479354, test accuracy: 0.6357142857142857\n",
            "Iteration 4565 training loss: 0.467199689646961, test loss: 0.7284298779836613 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4566 training loss: 0.46705221173494244, test loss: 0.7284688022652319 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4567 training loss: 0.4669047143493924, test loss: 0.7285077560921258 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4568 training loss: 0.46675719749796934, test loss: 0.7285467394118487 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4569 training loss: 0.46660966118843933, test loss: 0.728585752172026 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4570 training loss: 0.4664621054286775, test loss: 0.7286247943204022 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4571 training loss: 0.46631453022666763, test loss: 0.7286638658048413 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4572 training loss: 0.46616693559050293, test loss: 0.7287029665733282 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4573 training loss: 0.4660193215283868, test loss: 0.7287420965739675 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4574 training loss: 0.4658716880486329, test loss: 0.7287812557549853 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4575 training loss: 0.46572403515966604, test loss: 0.7288204440647288 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4576 training loss: 0.465576362870022, test loss: 0.7288596614516669 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4577 training loss: 0.46542867118834874, test loss: 0.7288989078643906 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4578 training loss: 0.46528096012340625, test loss: 0.7289381832516132 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4579 training loss: 0.4651332296840673, test loss: 0.7289774875621708 \n",
            "train accuracy: 0.8563734290843806, test accuracy: 0.6357142857142857\n",
            "Iteration 4580 training loss: 0.4649854798793179, test loss: 0.7290168207450224 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4581 training loss: 0.4648377107182576, test loss: 0.7290561827492509 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4582 training loss: 0.4646899222101, test loss: 0.7290955735240624 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4583 training loss: 0.4645421143641736, test loss: 0.7291349930187874 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4584 training loss: 0.46439428718992104, test loss: 0.7291744411828813 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4585 training loss: 0.46424644069690124, test loss: 0.7292139179659236 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4586 training loss: 0.46409857489478834, test loss: 0.7292534233176197 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4587 training loss: 0.46395068979337306, test loss: 0.7292929571877993 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4588 training loss: 0.46380278540256265, test loss: 0.7293325195264195 \n",
            "train accuracy: 0.8581687612208259, test accuracy: 0.6357142857142857\n",
            "Iteration 4589 training loss: 0.4636548617323815, test loss: 0.7293721102835623 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4590 training loss: 0.4635069187929717, test loss: 0.7294117294094363 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4591 training loss: 0.46335895659459303, test loss: 0.7294513768543774 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4592 training loss: 0.4632109751476239, test loss: 0.7294910525688483 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4593 training loss: 0.4630629744625612, test loss: 0.7295307565034389 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4594 training loss: 0.4629149545500213, test loss: 0.7295704886088672 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4595 training loss: 0.4627669154207404, test loss: 0.7296102488359788 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4596 training loss: 0.4626188570855742, test loss: 0.7296500371357476 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4597 training loss: 0.4624707795554991, test loss: 0.7296898534592766 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4598 training loss: 0.4623226828416125, test loss: 0.7297296977577974 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4599 training loss: 0.4621745669551327, test loss: 0.7297695699826704 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6357142857142857\n",
            "Iteration 4600 training loss: 0.46202643190739984, test loss: 0.729809470085386 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4601 training loss: 0.4618782777098762, test loss: 0.7298493980175645 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4602 training loss: 0.4617301043741462, test loss: 0.7298893537309555 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4603 training loss: 0.46158191191191705, test loss: 0.7299293371774398 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4604 training loss: 0.46143370033501957, test loss: 0.729969348309028 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4605 training loss: 0.4612854696554079, test loss: 0.7300093870778624 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4606 training loss: 0.46113721988516004, test loss: 0.7300494534362156 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4607 training loss: 0.4609889510364784, test loss: 0.7300895473364926 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4608 training loss: 0.4608406631216902, test loss: 0.7301296687312288 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4609 training loss: 0.4606923561532478, test loss: 0.7301698175730927 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4610 training loss: 0.46054403014372847, test loss: 0.7302099938148846 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4611 training loss: 0.46039568510583634, test loss: 0.7302501974095374 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4612 training loss: 0.4602473210524006, test loss: 0.730290428310116 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4613 training loss: 0.46009893799637774, test loss: 0.7303306864698196 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4614 training loss: 0.4599505359508509, test loss: 0.7303709718419794 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4615 training loss: 0.4598021149290306, test loss: 0.7304112843800605 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4616 training loss: 0.4596536749442547, test loss: 0.730451624037662 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4617 training loss: 0.45950521600998934, test loss: 0.7304919907685163 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4618 training loss: 0.45935673813982875, test loss: 0.7305323845264909 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4619 training loss: 0.4592082413474959, test loss: 0.7305728052655868 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4620 training loss: 0.45905972564684294, test loss: 0.7306132529399401 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4621 training loss: 0.45891119105185096, test loss: 0.7306537275038215 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4622 training loss: 0.45876263757663127, test loss: 0.7306942289116376 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4623 training loss: 0.45861406523542464, test loss: 0.7307347571179292 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4624 training loss: 0.4584654740426025, test loss: 0.7307753120773735 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4625 training loss: 0.4583168640126669, test loss: 0.7308158937447828 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4626 training loss: 0.45816823516025085, test loss: 0.7308565020751063 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4627 training loss: 0.45801958750011873, test loss: 0.7308971370234284 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4628 training loss: 0.45787092104716653, test loss: 0.7309377985449705 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4629 training loss: 0.45772223581642224, test loss: 0.7309784865950905 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4630 training loss: 0.457573531823046, test loss: 0.7310192011292826 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4631 training loss: 0.4574248090823305, test loss: 0.7310599421031793 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4632 training loss: 0.45727606760970185, test loss: 0.7311007094725491 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4633 training loss: 0.4571273074207185, test loss: 0.7311415031932982 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4634 training loss: 0.456978528531073, test loss: 0.7311823232214708 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4635 training loss: 0.4568297309565916, test loss: 0.7312231695132486 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4636 training loss: 0.45668091471323463, test loss: 0.731264042024951 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4637 training loss: 0.45653207981709665, test loss: 0.7313049407130362 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4638 training loss: 0.45638322628440714, test loss: 0.7313458655341006 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4639 training loss: 0.4562343541315305, test loss: 0.731386816444879 \n",
            "train accuracy: 0.8599640933572711, test accuracy: 0.6285714285714286\n",
            "Iteration 4640 training loss: 0.45608546337496625, test loss: 0.7314277934022446 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4641 training loss: 0.45593655403134964, test loss: 0.7314687963632103 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4642 training loss: 0.45578762611745166, test loss: 0.7315098252849274 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4643 training loss: 0.45563867965017946, test loss: 0.7315508801246865 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4644 training loss: 0.45548971464657645, test loss: 0.7315919608399183 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4645 training loss: 0.45534073112382284, test loss: 0.731633067388192 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4646 training loss: 0.4551917290992357, test loss: 0.7316741997272176 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4647 training loss: 0.4550427085902694, test loss: 0.7317153578148443 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4648 training loss: 0.45489366961451544, test loss: 0.7317565416090617 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4649 training loss: 0.45474461218970363, test loss: 0.7317977510679995 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4650 training loss: 0.45459553633370103, test loss: 0.7318389861499278 \n",
            "train accuracy: 0.8617594254937163, test accuracy: 0.6285714285714286\n",
            "Iteration 4651 training loss: 0.4544464420645135, test loss: 0.7318802468132575 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4652 training loss: 0.4542973294002852, test loss: 0.7319215330165396 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4653 training loss: 0.45414819835929904, test loss: 0.7319628447184665 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4654 training loss: 0.45399904895997684, test loss: 0.7320041818778715 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4655 training loss: 0.45384988122087977, test loss: 0.7320455444537286 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4656 training loss: 0.45370069516070843, test loss: 0.7320869324051534 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4657 training loss: 0.4535514907983031, test loss: 0.7321283456914033 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4658 training loss: 0.45340226815264406, test loss: 0.7321697842718763 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4659 training loss: 0.45325302724285166, test loss: 0.7322112481061129 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4660 training loss: 0.453103768088187, test loss: 0.7322527371537948 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4661 training loss: 0.4529544907080514, test loss: 0.7322942513747461 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6285714285714286\n",
            "Iteration 4662 training loss: 0.4528051951219872, test loss: 0.7323357907289331 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4663 training loss: 0.452655881349678, test loss: 0.7323773551764637 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4664 training loss: 0.4525065494109486, test loss: 0.7324189446775882 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4665 training loss: 0.4523571993257652, test loss: 0.7324605591926999 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4666 training loss: 0.452207831114236, test loss: 0.7325021986823342 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4667 training loss: 0.4520584447966109, test loss: 0.7325438631071691 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4668 training loss: 0.4519090403932821, test loss: 0.7325855524280257 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4669 training loss: 0.4517596179247843, test loss: 0.7326272666058679 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4670 training loss: 0.4516101774117945, test loss: 0.7326690056018024 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4671 training loss: 0.45146071887513256, test loss: 0.732710769377079 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4672 training loss: 0.4513112423357616, test loss: 0.7327525578930913 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4673 training loss: 0.4511617478147875, test loss: 0.7327943711113756 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4674 training loss: 0.4510122353334597, test loss: 0.7328362089936119 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4675 training loss: 0.4508627049131712, test loss: 0.7328780715016232 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4676 training loss: 0.45071315657545885, test loss: 0.7329199585973771 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4677 training loss: 0.45056359034200316, test loss: 0.7329618702429843 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4678 training loss: 0.45041400623462885, test loss: 0.7330038064006993 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4679 training loss: 0.45026440427530495, test loss: 0.7330457670329203 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4680 training loss: 0.450114784486145, test loss: 0.7330877521021903 \n",
            "train accuracy: 0.8635547576301615, test accuracy: 0.6214285714285714\n",
            "Iteration 4681 training loss: 0.4499651468894072, test loss: 0.7331297615711955 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4682 training loss: 0.4498154915074943, test loss: 0.7331717954027667 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4683 training loss: 0.44966581836295433, test loss: 0.7332138535598787 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4684 training loss: 0.4495161274784803, test loss: 0.7332559360056509 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4685 training loss: 0.44936641887691053, test loss: 0.733298042703347 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4686 training loss: 0.4492166925812288, test loss: 0.733340173616375 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4687 training loss: 0.4490669486145645, test loss: 0.7333823287082876 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4688 training loss: 0.44891718700019284, test loss: 0.7334245079427822 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4689 training loss: 0.4487674077615349, test loss: 0.7334667112837008 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4690 training loss: 0.4486176109221578, test loss: 0.7335089386950299 \n",
            "train accuracy: 0.8653500897666068, test accuracy: 0.6214285714285714\n",
            "Iteration 4691 training loss: 0.44846779650577495, test loss: 0.7335511901409014 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6214285714285714\n",
            "Iteration 4692 training loss: 0.4483179645362462, test loss: 0.7335934655855915 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6214285714285714\n",
            "Iteration 4693 training loss: 0.4481681150375775, test loss: 0.7336357649935218 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6214285714285714\n",
            "Iteration 4694 training loss: 0.44801824803392193, test loss: 0.7336780883292586 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6214285714285714\n",
            "Iteration 4695 training loss: 0.44786836354957904, test loss: 0.7337204355575135 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4696 training loss: 0.44771846160899537, test loss: 0.7337628066431431 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4697 training loss: 0.44756854223676446, test loss: 0.7338052015511491 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4698 training loss: 0.44741860545762707, test loss: 0.7338476202466786 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4699 training loss: 0.4472686512964712, test loss: 0.7338900626950239 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4700 training loss: 0.4471186797783322, test loss: 0.7339325288616229 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4701 training loss: 0.446968690928393, test loss: 0.7339750187120584 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4702 training loss: 0.44681868477198416, test loss: 0.7340175322120587 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4703 training loss: 0.4466686613345841, test loss: 0.7340600693274981 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4704 training loss: 0.4465186206418187, test loss: 0.7341026300243957 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4705 training loss: 0.4463685627194624, test loss: 0.7341452142689165 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4706 training loss: 0.4462184875934373, test loss: 0.7341878220273713 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4707 training loss: 0.4460683952898137, test loss: 0.7342304532662158 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4708 training loss: 0.4459182858348103, test loss: 0.7342731079520525 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4709 training loss: 0.4457681592547941, test loss: 0.7343157860516284 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4710 training loss: 0.44561801557628067, test loss: 0.7343584875318366 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4711 training loss: 0.44546785482593393, test loss: 0.7344012123597166 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4712 training loss: 0.44531767703056657, test loss: 0.7344439605024526 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4713 training loss: 0.4451674822171402, test loss: 0.7344867319273755 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4714 training loss: 0.4450172704127648, test loss: 0.7345295266019611 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4715 training loss: 0.44486704164469965, test loss: 0.734572344493832 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4716 training loss: 0.4447167959403528, test loss: 0.7346151855707561 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4717 training loss: 0.44456653332728135, test loss: 0.7346580498006471 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4718 training loss: 0.4444162538331916, test loss: 0.7347009371515648 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4719 training loss: 0.44426595748593906, test loss: 0.7347438475917146 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4720 training loss: 0.4441156443135283, test loss: 0.7347867810894485 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4721 training loss: 0.4439653143441135, test loss: 0.7348297376132635 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4722 training loss: 0.443814967605998, test loss: 0.7348727171318032 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4723 training loss: 0.4436646041276346, test loss: 0.7349157196138568 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4724 training loss: 0.44351422393762585, test loss: 0.7349587450283593 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4725 training loss: 0.4433638270647235, test loss: 0.7350017933443922 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4726 training loss: 0.44321341353782906, test loss: 0.7350448645311823 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4727 training loss: 0.4430629833859938, test loss: 0.7350879585581027 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4728 training loss: 0.44291253663841856, test loss: 0.7351310753946722 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4729 training loss: 0.4427620733244539, test loss: 0.7351742150105557 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4730 training loss: 0.4426115934736003, test loss: 0.7352173773755639 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4731 training loss: 0.4424610971155079, test loss: 0.7352605624596533 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4732 training loss: 0.4423105842799768, test loss: 0.7353037702329266 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4733 training loss: 0.44216005499695704, test loss: 0.7353470006656322 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4734 training loss: 0.44200950929654853, test loss: 0.7353902537281642 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4735 training loss: 0.44185894720900093, test loss: 0.735433529391063 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4736 training loss: 0.4417083687647142, test loss: 0.7354768276250141 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4737 training loss: 0.4415577739942382, test loss: 0.7355201484008497 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4738 training loss: 0.44140716292827253, test loss: 0.735563491689547 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4739 training loss: 0.44125653559766725, test loss: 0.7356068574622294 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4740 training loss: 0.4411058920334221, test loss: 0.735650245690166 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4741 training loss: 0.44095523226668726, test loss: 0.7356936563447716 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4742 training loss: 0.4408045563287626, test loss: 0.7357370893976064 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4743 training loss: 0.44065386425109826, test loss: 0.7357805448203766 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4744 training loss: 0.4405031560652945, test loss: 0.735824022584934 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4745 training loss: 0.44035243180310146, test loss: 0.7358675226632757 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4746 training loss: 0.4402016914964195, test loss: 0.7359110450275448 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4747 training loss: 0.4400509351772993, test loss: 0.7359545896500295 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4748 training loss: 0.439900162877941, test loss: 0.7359981565031634 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4749 training loss: 0.4397493746306954, test loss: 0.7360417455595263 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4750 training loss: 0.4395985704680631, test loss: 0.7360853567918426 \n",
            "train accuracy: 0.8671454219030521, test accuracy: 0.6142857142857143\n",
            "Iteration 4751 training loss: 0.4394477504226947, test loss: 0.7361289901729821 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4752 training loss: 0.4392969145273908, test loss: 0.7361726456759602 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4753 training loss: 0.43914606281510216, test loss: 0.7362163232739379 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4754 training loss: 0.43899519531892955, test loss: 0.7362600229402206 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4755 training loss: 0.4388443120721233, test loss: 0.7363037446482592 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4756 training loss: 0.43869341310808413, test loss: 0.7363474883716498 \n",
            "train accuracy: 0.8689407540394973, test accuracy: 0.6142857142857143\n",
            "Iteration 4757 training loss: 0.43854249846036253, test loss: 0.7363912540841336 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4758 training loss: 0.43839156816265884, test loss: 0.7364350417595966 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4759 training loss: 0.438240622248823, test loss: 0.7364788513720695 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4760 training loss: 0.43808966075285516, test loss: 0.7365226828957286 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4761 training loss: 0.43793868370890504, test loss: 0.7365665363048945 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4762 training loss: 0.437787691151272, test loss: 0.7366104115740325 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4763 training loss: 0.4376366831144052, test loss: 0.7366543086777527 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4764 training loss: 0.4374856596329033, test loss: 0.7366982275908102 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4765 training loss: 0.4373346207415147, test loss: 0.7367421682881037 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4766 training loss: 0.4371835664751373, test loss: 0.7367861307446776 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4767 training loss: 0.43703249686881823, test loss: 0.7368301149357196 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4768 training loss: 0.43688141195775443, test loss: 0.7368741208365623 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4769 training loss: 0.436730311777292, test loss: 0.7369181484226827 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4770 training loss: 0.4365791963629262, test loss: 0.7369621976697015 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4771 training loss: 0.4364280657503018, test loss: 0.737006268553384 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4772 training loss: 0.4362769199752125, test loss: 0.7370503610496392 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4773 training loss: 0.43612575907360124, test loss: 0.7370944751345198 \n",
            "train accuracy: 0.8707360861759426, test accuracy: 0.6142857142857143\n",
            "Iteration 4774 training loss: 0.43597458308156006, test loss: 0.737138610784223 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4775 training loss: 0.4358233920353297, test loss: 0.7371827679750897 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4776 training loss: 0.4356721859712999, test loss: 0.737226946683604 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4777 training loss: 0.43552096492600934, test loss: 0.7372711468863936 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4778 training loss: 0.4353697289361453, test loss: 0.7373153685602304 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4779 training loss: 0.43521847803854363, test loss: 0.7373596116820291 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4780 training loss: 0.43506721227018885, test loss: 0.7374038762288476 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4781 training loss: 0.4349159316682139, test loss: 0.7374481621778879 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4782 training loss: 0.43476463626990003, test loss: 0.7374924695064943 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4783 training loss: 0.4346133261126767, test loss: 0.7375367981921542 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4784 training loss: 0.434462001234122, test loss: 0.7375811482124983 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4785 training loss: 0.4343106616719616, test loss: 0.7376255195453 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4786 training loss: 0.43415930746406917, test loss: 0.7376699121684754 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4787 training loss: 0.43400793864846676, test loss: 0.7377143260600831 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4788 training loss: 0.4338565552633235, test loss: 0.7377587611983245 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4789 training loss: 0.4337051573469569, test loss: 0.7378032175615431 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4790 training loss: 0.43355374493783155, test loss: 0.7378476951282246 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4791 training loss: 0.4334023180745596, test loss: 0.7378921938769978 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4792 training loss: 0.4332508767959006, test loss: 0.7379367137866321 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4793 training loss: 0.4330994211407611, test loss: 0.7379812548360402 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4794 training loss: 0.4329479511481953, test loss: 0.7380258170042755 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4795 training loss: 0.43279646685740364, test loss: 0.7380704002705344 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4796 training loss: 0.4326449683077341, test loss: 0.7381150046141537 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4797 training loss: 0.43249345553868085, test loss: 0.7381596300146123 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4798 training loss: 0.43234192858988496, test loss: 0.7382042764515303 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4799 training loss: 0.432190387501134, test loss: 0.7382489439046692 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4800 training loss: 0.43203883231236156, test loss: 0.7382936323539313 \n",
            "train accuracy: 0.8725314183123878, test accuracy: 0.6142857142857143\n",
            "Iteration 4801 training loss: 0.4318872630636479, test loss: 0.7383383417793602 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4802 training loss: 0.43173567979521893, test loss: 0.7383830721611397 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4803 training loss: 0.43158408254744657, test loss: 0.7384278234795953 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4804 training loss: 0.4314324713608489, test loss: 0.7384725957151923 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4805 training loss: 0.43128084627608915, test loss: 0.7385173888485367 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4806 training loss: 0.43112920733397614, test loss: 0.7385622028603749 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4807 training loss: 0.43097755457546433, test loss: 0.7386070377315932 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4808 training loss: 0.43082588804165317, test loss: 0.7386518934432181 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4809 training loss: 0.4306742077737872, test loss: 0.738696769976416 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4810 training loss: 0.43052251381325557, test loss: 0.7387416673124931 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4811 training loss: 0.43037080620159257, test loss: 0.738786585432895 \n",
            "train accuracy: 0.874326750448833, test accuracy: 0.6142857142857143\n",
            "Iteration 4812 training loss: 0.43021908498047695, test loss: 0.738831524319207 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4813 training loss: 0.43006735019173165, test loss: 0.7388764839531532 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4814 training loss: 0.42991560187732414, test loss: 0.7389214643165973 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4815 training loss: 0.4297638400793658, test loss: 0.738966465391542 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4816 training loss: 0.42961206484011183, test loss: 0.7390114871601285 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4817 training loss: 0.42946027620196153, test loss: 0.7390565296046372 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4818 training loss: 0.4293084742074573, test loss: 0.7391015927074863 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4819 training loss: 0.42915665889928534, test loss: 0.739146676451233 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4820 training loss: 0.42900483032027475, test loss: 0.7391917808185726 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4821 training loss: 0.4288529885133979, test loss: 0.7392369057923383 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4822 training loss: 0.42870113352176986, test loss: 0.7392820513555008 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4823 training loss: 0.42854926538864835, test loss: 0.7393272174911696 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4824 training loss: 0.4283973841574337, test loss: 0.7393724041825908 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4825 training loss: 0.4282454898716685, test loss: 0.7394176114131479 \n",
            "train accuracy: 0.8761220825852782, test accuracy: 0.6142857142857143\n",
            "Iteration 4826 training loss: 0.42809358257503727, test loss: 0.739462839166362 \n",
            "train accuracy: 0.8779174147217235, test accuracy: 0.6142857142857143\n",
            "Iteration 4827 training loss: 0.42794166231136677, test loss: 0.7395080874258915 \n",
            "train accuracy: 0.8779174147217235, test accuracy: 0.6142857142857143\n",
            "Iteration 4828 training loss: 0.42778972912462515, test loss: 0.7395533561755309 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4829 training loss: 0.4276377830589224, test loss: 0.7395986453992115 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4830 training loss: 0.42748582415850955, test loss: 0.7396439550810019 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4831 training loss: 0.42733385246777883, test loss: 0.7396892852051062 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4832 training loss: 0.42718186803126357, test loss: 0.739734635755865 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4833 training loss: 0.42702987089363753, test loss: 0.7397800067177547 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4834 training loss: 0.4268778610997152, test loss: 0.7398253980753876 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4835 training loss: 0.4267258386944513, test loss: 0.7398708098135119 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4836 training loss: 0.4265738037229407, test loss: 0.7399162419170104 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4837 training loss: 0.42642175623041795, test loss: 0.739961694370902 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4838 training loss: 0.4262696962622575, test loss: 0.7400071671603403 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4839 training loss: 0.42611762386397306, test loss: 0.7400526602706134 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4840 training loss: 0.4259655390812177, test loss: 0.7400981736871444 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4841 training loss: 0.42581344195978343, test loss: 0.7401437073954905 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4842 training loss: 0.42566133254560123, test loss: 0.7401892613813441 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4843 training loss: 0.4255092108847402, test loss: 0.7402348356305305 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4844 training loss: 0.42535707702340825, test loss: 0.7402804301290093 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4845 training loss: 0.4252049310079511, test loss: 0.7403260448628739 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4846 training loss: 0.42505277288485255, test loss: 0.740371679818351 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4847 training loss: 0.4249006027007337, test loss: 0.7404173349818006 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4848 training loss: 0.42474842050235356, test loss: 0.7404630103397157 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4849 training loss: 0.42459622633660793, test loss: 0.7405087058787219 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4850 training loss: 0.42444402025052946, test loss: 0.7405544215855779 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4851 training loss: 0.42429180229128793, test loss: 0.7406001574471744 \n",
            "train accuracy: 0.8797127468581688, test accuracy: 0.6142857142857143\n",
            "Iteration 4852 training loss: 0.4241395725061892, test loss: 0.7406459134505344 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4853 training loss: 0.4239873309426756, test loss: 0.7406916895828126 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4854 training loss: 0.423835077648325, test loss: 0.7407374858312961 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4855 training loss: 0.4236828126708515, test loss: 0.7407833021834028 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4856 training loss: 0.42353053605810426, test loss: 0.7408291386266826 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4857 training loss: 0.4233782478580679, test loss: 0.7408749951488162 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4858 training loss: 0.4232259481188619, test loss: 0.7409208717376146 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4859 training loss: 0.4230736368887402, test loss: 0.7409667683810205 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4860 training loss: 0.42292131421609186, test loss: 0.7410126850671058 \n",
            "train accuracy: 0.881508078994614, test accuracy: 0.6142857142857143\n",
            "Iteration 4861 training loss: 0.4227689801494394, test loss: 0.7410586217840739 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4862 training loss: 0.4226166347374396, test loss: 0.7411045785202569 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4863 training loss: 0.42246427802888287, test loss: 0.7411505552641178 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4864 training loss: 0.422311910072693, test loss: 0.741196552004248 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4865 training loss: 0.4221595309179269, test loss: 0.7412425687293688 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4866 training loss: 0.42200714061377437, test loss: 0.7412886054283305 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4867 training loss: 0.42185473920955785, test loss: 0.7413346620901115 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4868 training loss: 0.4217023267547319, test loss: 0.7413807387038197 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4869 training loss: 0.4215499032988834, test loss: 0.7414268352586908 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4870 training loss: 0.42139746889173074, test loss: 0.7414729517440882 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4871 training loss: 0.42124502358312405, test loss: 0.7415190881495036 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4872 training loss: 0.4210925674230446, test loss: 0.7415652444645563 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4873 training loss: 0.42094010046160435, test loss: 0.7416114206789928 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4874 training loss: 0.42078762274904635, test loss: 0.741657616782686 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4875 training loss: 0.4206351343357436, test loss: 0.7417038327656367 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4876 training loss: 0.4204826352721996, test loss: 0.7417500686179711 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4877 training loss: 0.42033012560904726, test loss: 0.7417963243299429 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6142857142857143\n",
            "Iteration 4878 training loss: 0.4201776053970491, test loss: 0.7418425998919308 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6214285714285714\n",
            "Iteration 4879 training loss: 0.4200250746870972, test loss: 0.7418888952944398 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6214285714285714\n",
            "Iteration 4880 training loss: 0.419872533530212, test loss: 0.7419352105281002 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6214285714285714\n",
            "Iteration 4881 training loss: 0.4197199819775431, test loss: 0.7419815455836676 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6214285714285714\n",
            "Iteration 4882 training loss: 0.419567420080368, test loss: 0.7420279004520225 \n",
            "train accuracy: 0.8833034111310593, test accuracy: 0.6214285714285714\n",
            "Iteration 4883 training loss: 0.4194148478900926, test loss: 0.7420742751241703 \n",
            "train accuracy: 0.8850987432675045, test accuracy: 0.6214285714285714\n",
            "Iteration 4884 training loss: 0.4192622654582502, test loss: 0.7421206695912409 \n",
            "train accuracy: 0.8850987432675045, test accuracy: 0.6214285714285714\n",
            "Iteration 4885 training loss: 0.41910967283650197, test loss: 0.742167083844488 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4886 training loss: 0.4189570700766358, test loss: 0.7422135178752896 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4887 training loss: 0.41880445723056664, test loss: 0.7422599716751469 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4888 training loss: 0.4186518343503358, test loss: 0.742306445235685 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4889 training loss: 0.4184992014881111, test loss: 0.7423529385486516 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4890 training loss: 0.41834655869618603, test loss: 0.7423994516059177 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4891 training loss: 0.41819390602697976, test loss: 0.7424459843994762 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4892 training loss: 0.41804124353303673, test loss: 0.7424925369214427 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4893 training loss: 0.41788857126702644, test loss: 0.7425391091640546 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4894 training loss: 0.4177358892817428, test loss: 0.7425857011196711 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4895 training loss: 0.4175831976301046, test loss: 0.7426323127807728 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4896 training loss: 0.41743049636515406, test loss: 0.7426789441399612 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4897 training loss: 0.4172777855400575, test loss: 0.7427255951899586 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4898 training loss: 0.4171250652081044, test loss: 0.7427722659236082 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4899 training loss: 0.4169723354227076, test loss: 0.7428189563338732 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4900 training loss: 0.41681959623740233, test loss: 0.7428656664138366 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4901 training loss: 0.4166668477058465, test loss: 0.7429123961567013 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4902 training loss: 0.41651408988182, test loss: 0.7429591455557892 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4903 training loss: 0.4163613228192245, test loss: 0.7430059146045418 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4904 training loss: 0.4162085465720833, test loss: 0.7430527032965187 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4905 training loss: 0.41605576119454024, test loss: 0.7430995116253986 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4906 training loss: 0.41590296674086075, test loss: 0.7431463395849778 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4907 training loss: 0.4157501632654301, test loss: 0.7431931871691708 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4908 training loss: 0.4155973508227536, test loss: 0.7432400543720094 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4909 training loss: 0.4154445294674569, test loss: 0.7432869411876425 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4910 training loss: 0.4152916992542846, test loss: 0.7433338476103363 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4911 training loss: 0.41513886023810065, test loss: 0.7433807736344736 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4912 training loss: 0.4149860124738875, test loss: 0.7434277192545531 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4913 training loss: 0.41483315601674614, test loss: 0.7434746844651897 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4914 training loss: 0.4146802909218957, test loss: 0.7435216692611141 \n",
            "train accuracy: 0.8868940754039497, test accuracy: 0.6214285714285714\n",
            "Iteration 4915 training loss: 0.41452741724467285, test loss: 0.7435686736371717 \n",
            "train accuracy: 0.8886894075403949, test accuracy: 0.6214285714285714\n",
            "Iteration 4916 training loss: 0.41437453504053184, test loss: 0.743615697588324 \n",
            "train accuracy: 0.8886894075403949, test accuracy: 0.6214285714285714\n",
            "Iteration 4917 training loss: 0.4142216443650439, test loss: 0.743662741109646 \n",
            "train accuracy: 0.8886894075403949, test accuracy: 0.6214285714285714\n",
            "Iteration 4918 training loss: 0.4140687452738966, test loss: 0.7437098041963282 \n",
            "train accuracy: 0.8886894075403949, test accuracy: 0.6214285714285714\n",
            "Iteration 4919 training loss: 0.4139158378228945, test loss: 0.7437568868436741 \n",
            "train accuracy: 0.8886894075403949, test accuracy: 0.6214285714285714\n",
            "Iteration 4920 training loss: 0.4137629220679575, test loss: 0.7438039890471018 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4921 training loss: 0.4136099980651214, test loss: 0.7438511108021423 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4922 training loss: 0.41345706587053727, test loss: 0.7438982521044396 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4923 training loss: 0.4133041255404712, test loss: 0.7439454129497509 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4924 training loss: 0.4131511771313035, test loss: 0.7439925933339456 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4925 training loss: 0.4129982206995291, test loss: 0.7440397932530048 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4926 training loss: 0.4128452563017563, test loss: 0.7440870127030219 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4927 training loss: 0.41269228399470737, test loss: 0.7441342516802012 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4928 training loss: 0.4125393038352173, test loss: 0.7441815101808582 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4929 training loss: 0.4123863158802342, test loss: 0.7442287882014197 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4930 training loss: 0.41223332018681835, test loss: 0.7442760857384219 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4931 training loss: 0.41208031681214186, test loss: 0.7443234027885117 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4932 training loss: 0.411927305813489, test loss: 0.7443707393484457 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4933 training loss: 0.411774287248255, test loss: 0.7444180954150893 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4934 training loss: 0.41162126117394604, test loss: 0.7444654709854175 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4935 training loss: 0.411468227648179, test loss: 0.7445128660565136 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4936 training loss: 0.41131518672868067, test loss: 0.7445602806255691 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4937 training loss: 0.411162138473288, test loss: 0.7446077146898842 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4938 training loss: 0.41100908293994715, test loss: 0.7446551682468656 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4939 training loss: 0.4108560201867134, test loss: 0.7447026412940279 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4940 training loss: 0.41070295027175086, test loss: 0.7447501338289922 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6214285714285714\n",
            "Iteration 4941 training loss: 0.4105498732533318, test loss: 0.744797645849487 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4942 training loss: 0.41039678918983635, test loss: 0.7448451773533458 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4943 training loss: 0.4102436981397524, test loss: 0.7448927283385083 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4944 training loss: 0.410090600161675, test loss: 0.7449402988030203 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4945 training loss: 0.40993749531430584, test loss: 0.7449878887450316 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4946 training loss: 0.4097843836564532, test loss: 0.7450354981627976 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4947 training loss: 0.4096312652470313, test loss: 0.7450831270546772 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4948 training loss: 0.40947814014505995, test loss: 0.7451307754191343 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4949 training loss: 0.4093250084096645, test loss: 0.7451784432547353 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4950 training loss: 0.40917187010007505, test loss: 0.7452261305601506 \n",
            "train accuracy: 0.8904847396768402, test accuracy: 0.6142857142857143\n",
            "Iteration 4951 training loss: 0.4090187252756259, test loss: 0.7452738373341535 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4952 training loss: 0.4088655739957558, test loss: 0.7453215635756191 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4953 training loss: 0.4087124163200072, test loss: 0.745369309283525 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4954 training loss: 0.4085592523080259, test loss: 0.7454170744569507 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4955 training loss: 0.4084060820195604, test loss: 0.7454648590950768 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4956 training loss: 0.4082529055144619, test loss: 0.7455126631971852 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4957 training loss: 0.40809972285268387, test loss: 0.7455604867626578 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4958 training loss: 0.40794653409428144, test loss: 0.7456083297909771 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4959 training loss: 0.4077933392994109, test loss: 0.7456561922817254 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4960 training loss: 0.4076401385283301, test loss: 0.7457040742345845 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4961 training loss: 0.4074869318413967, test loss: 0.7457519756493349 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4962 training loss: 0.4073337192990692, test loss: 0.7457998965258564 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4963 training loss: 0.40718050096190544, test loss: 0.7458478368641264 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4964 training loss: 0.4070272768905629, test loss: 0.7458957966642202 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4965 training loss: 0.406874047145798, test loss: 0.7459437759263114 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4966 training loss: 0.4067208117884657, test loss: 0.74599177465067 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4967 training loss: 0.40656757087951917, test loss: 0.7460397928376623 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4968 training loss: 0.40641432448000925, test loss: 0.746087830487752 \n",
            "train accuracy: 0.8922800718132855, test accuracy: 0.6142857142857143\n",
            "Iteration 4969 training loss: 0.40626107265108446, test loss: 0.746135887601498 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6142857142857143\n",
            "Iteration 4970 training loss: 0.40610781545398994, test loss: 0.7461839641795548 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6142857142857143\n",
            "Iteration 4971 training loss: 0.4059545529500676, test loss: 0.7462320602226715 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6142857142857143\n",
            "Iteration 4972 training loss: 0.40580128520075553, test loss: 0.7462801757316935 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6142857142857143\n",
            "Iteration 4973 training loss: 0.40564801226758745, test loss: 0.7463283107075586 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6142857142857143\n",
            "Iteration 4974 training loss: 0.4054947342121926, test loss: 0.7463764651512997 \n",
            "train accuracy: 0.8940754039497307, test accuracy: 0.6071428571428571\n",
            "Iteration 4975 training loss: 0.40534145109629505, test loss: 0.7464246390640425 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4976 training loss: 0.4051881629817134, test loss: 0.7464728324470066 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4977 training loss: 0.40503486993036053, test loss: 0.7465210453015031 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4978 training loss: 0.40488157200424274, test loss: 0.7465692776289367 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4979 training loss: 0.4047282692654601, test loss: 0.7466175294308023 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4980 training loss: 0.40457496177620517, test loss: 0.7466658007086887 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4981 training loss: 0.40442164959876303, test loss: 0.7467140914642727 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4982 training loss: 0.4042683327955111, test loss: 0.7467624016993243 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4983 training loss: 0.40411501142891826, test loss: 0.7468107314157019 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4984 training loss: 0.4039616855615447, test loss: 0.7468590806153556 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4985 training loss: 0.4038083552560413, test loss: 0.7469074493003217 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4986 training loss: 0.4036550205751497, test loss: 0.74695583747273 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4987 training loss: 0.403501681581701, test loss: 0.7470042451347932 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4988 training loss: 0.4033483383386166, test loss: 0.7470526722888184 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6\n",
            "Iteration 4989 training loss: 0.40319499090890637, test loss: 0.7471011189371938 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4990 training loss: 0.40304163935566933, test loss: 0.7471495850824018 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4991 training loss: 0.40288828374209285, test loss: 0.7471980707270038 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4992 training loss: 0.40273492413145195, test loss: 0.7472465758736566 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4993 training loss: 0.4025815605871093, test loss: 0.7472951005250926 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4994 training loss: 0.40242819317251466, test loss: 0.7473436446841416 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4995 training loss: 0.40227482195120445, test loss: 0.7473922083537051 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4996 training loss: 0.4021214469868013, test loss: 0.7474407915367846 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4997 training loss: 0.40196806834301346, test loss: 0.747489394236448 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4998 training loss: 0.4018146860836351, test loss: 0.7475380164558677 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 4999 training loss: 0.40166130027254465, test loss: 0.7475866581982761 \n",
            "train accuracy: 0.895870736086176, test accuracy: 0.6071428571428571\n",
            "Iteration 5000 training loss: 0.40150791097370553, test loss: 0.7476353194670148 \n",
            "train accuracy: 0.8976660682226212, test accuracy: 0.6071428571428571\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f36004ba0b0>,\n",
              " <matplotlib.lines.Line2D at 0x7f36004b96f0>]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def grad(prediction, target, params, feature_0, feature_1):\n",
        "    \"\"\" Gradient function with sigmoid activation\n",
        "    Args:\n",
        "        prediction: model predicted value, a 2d array with shape (# samples, 1)\n",
        "        target: labeled value from data set, a 2d array with shape (# samples, 1)\n",
        "        feature: feature matrix, a 2d array with shape (# samples, # pixels)\n",
        "    Returns:\n",
        "        dL_dw: row vector of BCE loss partial derivatives w.r.t. weights, 2d array with shape (1, # features)\n",
        "        dL_db: scalar of BCE loss partial derivatives w.r.t. bias\n",
        "    \"\"\"\n",
        "    M = target.shape[0]\n",
        "    dL_dw2 = 1 / M * np.dot((prediction - target).T, feature_1)\n",
        "    dL_db2 = np.mean(prediction - target)\n",
        "    dL_dX1 = np.dot((prediction - target), params['w_2'])\n",
        "    dX1_dZ1 = feature_1 * (1 - feature_1)\n",
        "    dL_dW1 = 1 / M * np.dot((dL_dX1 * dX1_dZ1).T, feature_0)\n",
        "    dL_db1 = np.mean(dL_dX1 * dX1_dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {\n",
        "        'dW_1': dL_dW1,\n",
        "        'db_1': dL_db1,\n",
        "        'dw_2': dL_dw2,\n",
        "        'db_2': dL_db2,\n",
        "    }\n",
        "    return grads\n",
        "\n",
        "# Initialize parameters\n",
        "W1 = np.random.normal(loc=0., scale=0.0001, size=(128, feature_train.shape[1]))\n",
        "b1 = np.random.normal(loc=0., scale=0.0001, size=(1, 128))\n",
        "w2 = np.random.normal(loc=0., scale=0.0001, size=(1, 128))\n",
        "b2 = np.random.normal(loc=0., scale=0.0001)\n",
        "params = {'W_1': W1, 'b_1': b1, 'w_2': w2, 'b_2': b2}\n",
        "num_iters = 5000\n",
        "learning_rate = 0.007\n",
        "losses_train, losses_test = [], []\n",
        "# Optimization loop\n",
        "for i in range(num_iters):\n",
        "    # Evaluate model on train and test data\n",
        "    pred_train, feature_1 = forward(feature_train, params)\n",
        "    loss_train = binary_cross_entropy_loss(pred_train, target_train)\n",
        "    pred_test, _ = forward(feature_test, params)\n",
        "    loss_test = binary_cross_entropy_loss(pred_test, target_test)\n",
        "    # Compute gradient\n",
        "    grads = grad(pred_train, target_train, params, feature_train, feature_1)\n",
        "    params['W_1'] = params['W_1'] - learning_rate * grads['dW_1']\n",
        "    params['b_1'] = params['b_1'] - learning_rate * grads['db_1']\n",
        "    params['w_2'] = params['w_2'] - learning_rate * grads['dw_2']\n",
        "    params['b_2'] = params['b_2'] - learning_rate * grads['db_2']\n",
        "    # Stats\n",
        "    category_train = np.ones_like(pred_train)\n",
        "    category_train[pred_train > 0.5] = 1\n",
        "    category_train[pred_train <= 0.5] = 0\n",
        "    acc_train = np.sum(category_train==target_train) / target_train.shape[0]\n",
        "    category_test = np.ones_like(pred_test)\n",
        "    category_test[pred_test > 0.5] = 1\n",
        "    category_test[pred_test <= 0.5] = 0\n",
        "    acc_test = np.sum(category_test==target_test) / target_test.shape[0]\n",
        "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test} \\ntrain accuracy: {acc_train}, test accuracy: {acc_test}\")\n",
        "    losses_train.append(loss_train)\n",
        "    losses_test.append(loss_test)\n",
        "\n",
        "\n",
        "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss keeps descreasing, but test loss even increased a little. This is a sign of over-fitting, which suggests to use a simpler model or employ more data."
      ],
      "metadata": {
        "id": "_tZlKutpYbf1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpkaV1xxB2Nd"
      },
      "source": [
        "## 8. Test\n",
        "Download new images to this folder and try to use your model to classify them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw_M7FJnB2Nd"
      },
      "outputs": [],
      "source": [
        "image_raw = None  # read the raw image from a file\n",
        "image_rgb = None  # Convert BGR to RGB\n",
        "image_resize = None  # Resize image to shape (200, 200, 3)\n",
        "image_flatten = None  # Flatten image array to a row vector with shape (1, 200*200*3)\n",
        "image_rescale = None  # rescale pixel value from 0~255 to 0.~1.\n",
        "dog_likelihood = None  # predict new image with your model\n",
        "\n",
        "is_dog = dog_likelihood > 0.5\n",
        "if is_dog.squeeze():\n",
        "    print(\"It's dog!\")\n",
        "else:\n",
        "    print(\"It's cat!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}