{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6NF77VBb8I2"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7I1YU2blb8I4",
        "outputId": "b456b493-6086-49a4-e403-912baac72d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000,) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_set = datasets.MNIST('./data', train=True, download=True)\n",
        "test_set = datasets.MNIST('./data', train=False, download=True)\n",
        "\n",
        "train_feature_array = train_set.data.numpy()\n",
        "train_target_array = train_set.targets.numpy()\n",
        "test_feature_array = test_set.data.numpy()\n",
        "test_target_array = test_set.targets.numpy()\n",
        "print(train_feature_array.shape, test_feature_array.shape)\n",
        "print(train_target_array.shape, test_target_array.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTUm8rI1b8I6"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rc6Vlpmqb8I6",
        "outputId": "f661810e-aa71-4e2d-fa3a-57f876f56f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784) (60000, 10) (10000, 10)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.60784314 0.99215686 0.99215686 0.74117647 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.99215686 0.98431373\n",
            " 0.98431373 0.7372549  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.99215686 0.98431373 0.98431373 0.7372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.54509804\n",
            " 0.99215686 0.98431373 0.98431373 0.7372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.61960784 0.99215686 0.98431373\n",
            " 0.98431373 0.7372549  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.62352941 1.         0.99215686 0.99215686 0.74117647\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.61960784\n",
            " 0.99215686 0.98431373 0.98431373 0.7372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.1254902  0.80392157 0.99215686 0.98431373\n",
            " 0.98431373 0.7372549  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.25098039 0.98431373 0.99215686 0.98431373 0.98431373 0.7372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.25098039 0.98431373\n",
            " 0.99215686 0.98431373 0.98431373 0.7372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.48627451 0.99215686 1.         0.99215686\n",
            " 0.99215686 0.74117647 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.86666667 0.98431373 0.99215686 0.98431373 0.98431373 0.27058824\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.86666667 0.98431373\n",
            " 0.99215686 0.98431373 0.7372549  0.05882353 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.86666667 0.98431373 0.99215686 0.8745098\n",
            " 0.05882353 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.86666667 0.98431373 0.99215686 0.8627451  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.50196078 0.99215686 0.99215686\n",
            " 1.         0.86666667 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.49803922 0.98431373 0.98431373 0.99215686 0.78431373\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.0627451  0.74117647\n",
            " 0.98431373 0.98431373 0.99215686 0.24313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.67058824 0.98431373 0.98431373 0.98431373\n",
            " 0.50588235 0.03137255 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.35686275 0.98431373 0.98431373 0.98431373 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "num_samples_train = train_target_array.shape[0]\n",
        "num_samples_test = test_target_array.shape[0]\n",
        "# Reshape feature and target arrays\n",
        "flatten_feature_train = train_feature_array.reshape(num_samples_train, -1)\n",
        "flatten_feature_test = test_feature_array.reshape(num_samples_test, -1)\n",
        "flatten_target_train = train_target_array.reshape(num_samples_train, -1)\n",
        "flatten_target_test = test_target_array.reshape(num_samples_test, -1)\n",
        "# Rescale features\n",
        "rescale_feature_train = flatten_feature_train / 255\n",
        "rescale_feature_test = flatten_feature_test / 255\n",
        "\n",
        "# One hot encode targets\n",
        "onehot_target_train = np.zeros((train_target_array.size, len(train_set.classes)))\n",
        "onehot_target_train[np.arange(train_target_array.size), train_target_array] = 1\n",
        "onehot_target_test = np.zeros((test_target_array.size, len(test_set.classes)))\n",
        "onehot_target_test[np.arange(test_target_array.size), test_target_array] = 1\n",
        "# Rename\n",
        "feature_train = rescale_feature_train\n",
        "feature_test = rescale_feature_test\n",
        "target_train = onehot_target_train\n",
        "target_test = onehot_target_test\n",
        "print(feature_train.shape, feature_test.shape, target_train.shape, target_test.shape)\n",
        "print(feature_train[3321])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5923. 6742. 5958. 6131. 5842. 5421. 5918. 6265. 5851. 5949.]\n"
          ]
        }
      ],
      "source": [
        "onehot_target_train = np.zeros((train_target_array.size, 10))\n",
        "onehot_target_train[np.arange(train_target_array.size), train_target_array] = 1\n",
        "print(np.sum(onehot_target_train, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVjux7ib8I7"
      },
      "source": [
        "## Parameters Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IV3RDV0Zb8I7",
        "outputId": "8ffe0545-3103-403b-fafd-4a701452dc44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['layer1', 'layer2', 'layer3'])\n",
            "{'layer1': {'W': array([[-7.34788781e-05, -8.34610878e-05, -5.33914052e-05,\n",
            "         2.63735506e-04,  3.03756719e-05,  1.75001581e-05,\n",
            "         1.95381553e-04],\n",
            "       [-1.86713689e-06,  2.52079527e-05, -1.59334956e-04,\n",
            "        -5.66606105e-05,  5.94732995e-05,  1.06969417e-04,\n",
            "         4.45793467e-05],\n",
            "       [-1.72550044e-04,  1.44560275e-04, -8.26745438e-05,\n",
            "         3.52685214e-05, -2.13800132e-05, -9.82738333e-05,\n",
            "         2.37505398e-05],\n",
            "       [ 6.18552554e-05,  4.74333872e-06, -1.11775943e-05,\n",
            "        -1.92655142e-04, -7.08625019e-06, -1.35814396e-04,\n",
            "         1.50295398e-04],\n",
            "       [ 1.54503715e-04,  7.98753567e-05,  8.58015354e-05,\n",
            "        -7.27494317e-05,  3.21028850e-05,  2.92964435e-06,\n",
            "         4.37460903e-05],\n",
            "       [ 7.01960137e-06,  5.81159172e-05,  2.81404956e-05,\n",
            "         4.97058123e-05,  1.22653648e-04, -4.80677418e-05,\n",
            "         2.56311161e-05]]), 'b': array([[-2.08710855e-04,  4.12886629e-05,  7.68525502e-05,\n",
            "        -2.67914523e-05,  2.41924156e-05,  1.39058060e-04]])}, 'layer2': {'W': array([[ 4.12795592e-05, -7.01383001e-05,  4.74721834e-05,\n",
            "        -5.36307199e-05, -6.14827974e-05, -1.40726707e-04],\n",
            "       [-1.66256000e-04, -2.69246288e-05,  1.08812228e-04,\n",
            "         1.03872470e-04,  2.36942357e-06,  2.03657626e-04],\n",
            "       [ 1.19940323e-04, -1.56214396e-04, -2.39354388e-05,\n",
            "         2.64694454e-05,  1.23391433e-05,  5.74174346e-05],\n",
            "       [ 7.50175523e-05,  1.79538757e-05, -3.70477085e-05,\n",
            "         1.04061346e-05,  1.60374076e-04,  9.08639871e-05],\n",
            "       [-4.60810949e-05, -1.19408440e-04,  1.72002141e-04,\n",
            "         2.92517597e-05,  1.89395906e-04,  1.78854674e-04]]), 'b': array([[ 2.35603851e-04, -2.49561820e-05, -4.40067721e-05,\n",
            "         6.27198324e-06,  1.21176052e-05]])}, 'layer3': {'W': array([[ 1.53349102e-05,  4.14466979e-05, -2.12699401e-05,\n",
            "        -5.35466839e-05, -1.50542805e-04],\n",
            "       [-4.90220178e-05,  4.74866619e-06,  2.58501589e-05,\n",
            "         1.03436536e-04,  1.55022845e-04],\n",
            "       [ 1.15361250e-04,  1.16434859e-04, -5.47431664e-05,\n",
            "        -2.19905406e-05,  1.45478045e-04],\n",
            "       [-1.13134624e-05,  3.00576922e-05,  1.49127438e-04,\n",
            "        -2.58488547e-04, -3.40769353e-05]]), 'b': array([[-1.13828147e-04, -6.13882765e-05,  6.38610074e-05,\n",
            "         6.94833029e-05]])}}\n"
          ]
        }
      ],
      "source": [
        "def init_params(layer_sizes):\n",
        "    params = {}\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        params['layer'+str(i+1)] = {\n",
        "            'W': np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i])),\n",
        "            'b': np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
        "        }\n",
        "\n",
        "    return params\n",
        "# Sanity check\n",
        "dummy_layer_sizes = (7, 6, 5, 4)\n",
        "dummy_params = init_params(dummy_layer_sizes)\n",
        "print(dummy_params.keys())\n",
        "print(dummy_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poCrpRkHb8I8"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rG0TQbqzb8I8",
        "outputId": "d277528f-663a-4e52-9d9e-2173bf324619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]\n",
            " [0.24997416 0.24998727 0.25001859 0.25001999]]\n",
            "dict_keys(['X0', 'Z1', 'X1', 'Z2', 'X2', 'Z3'])\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "def linear(feature, weight, bias):\n",
        "    return np.dot(feature, weight.T) + bias\n",
        "\n",
        "def forward(input_feature, params):\n",
        "    cache = {'X0': input_feature}\n",
        "    X = input_feature\n",
        "    for i in range(len(params) - 1):\n",
        "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['layer' + str(i+1)]['W'], params['layer' + str(i+1)]['b'])\n",
        "        cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
        "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['layer' + str(i+2)]['W'], params['layer' + str(i+2)]['b'])\n",
        "    prediction = softmax(cache['Z' + str(i+2)])\n",
        "\n",
        "    return prediction, cache\n",
        "\n",
        "# Sanity check\n",
        "np.random.seed(3321)\n",
        "dummy_input = np.random.randn(9,7)\n",
        "# print(sigmoid(dummy_array))\n",
        "# print(relu(dummy_array))\n",
        "dummy_pred, dummy_cache = forward(dummy_input, dummy_params)\n",
        "print(dummy_pred)\n",
        "print(dummy_cache.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Entropy Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.3862861023491801\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss_fn(prediction, target):\n",
        "    sample_loss = -np.sum(target * np.log(prediction + 1e-10), axis=1)\n",
        "\n",
        "    return sample_loss.mean()\n",
        "\n",
        "# Sanity check\n",
        "dummy_target = np.zeros((9, 4))\n",
        "dummy_target[np.arange(9), np.random.randint(0, 4, (9,))] = 1\n",
        "dummy_loss = cross_entropy_loss_fn(dummy_pred, dummy_target)\n",
        "print(dummy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dZ3': array([[ 0.24997416,  0.24998727,  0.25001859, -0.74998001],\n",
            "       [ 0.24997416,  0.24998727, -0.74998141,  0.25001999],\n",
            "       [ 0.24997416, -0.75001273,  0.25001859,  0.25001999],\n",
            "       [ 0.24997416, -0.75001273,  0.25001859,  0.25001999],\n",
            "       [-0.75002584,  0.24998727,  0.25001859,  0.25001999],\n",
            "       [ 0.24997416,  0.24998727, -0.74998141,  0.25001999],\n",
            "       [-0.75002584,  0.24998727,  0.25001859,  0.25001999],\n",
            "       [ 0.24997416,  0.24998727,  0.25001859, -0.74998001],\n",
            "       [ 0.24997416,  0.24998727, -0.74998141,  0.25001999]]), 'dW3': array([[ 5.88901560e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "         1.52892579e-06,  2.97262777e-06],\n",
            "       [ 5.88926156e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "         1.57699338e-06,  3.09764791e-06],\n",
            "       [-1.76697559e-04,  0.00000000e+00,  0.00000000e+00,\n",
            "        -4.67709013e-06, -9.05500035e-06],\n",
            "       [ 5.89147873e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "         1.57117097e-06,  2.98472467e-06]]), 'db3': array([[ 0.02775194,  0.02776504, -0.08331475,  0.02779776]]), 'dX2': array([[ 2.89057787e-05,  1.81159204e-05, -1.24384133e-04,\n",
            "         2.00835730e-04,  6.30511617e-05],\n",
            "       [-9.77689339e-05, -6.82612469e-05,  7.94864724e-05,\n",
            "        -3.56622765e-05, -1.16503819e-04],\n",
            "       [ 6.66143341e-05,  4.34249463e-05, -1.10685291e-06,\n",
            "        -1.61089353e-04, -1.26048618e-04],\n",
            "       [ 6.66143341e-05,  4.34249463e-05, -1.10685291e-06,\n",
            "        -1.61089353e-04, -1.26048618e-04],\n",
            "       [ 2.25740615e-06,  6.72691460e-06,  4.60132461e-05,\n",
            "        -4.10613318e-06,  1.79517032e-04],\n",
            "       [-9.77689339e-05, -6.82612469e-05,  7.94864724e-05,\n",
            "        -3.56622765e-05, -1.16503819e-04],\n",
            "       [ 2.25740615e-06,  6.72691460e-06,  4.60132461e-05,\n",
            "        -4.10613318e-06,  1.79517032e-04],\n",
            "       [ 2.89057787e-05,  1.81159204e-05, -1.24384133e-04,\n",
            "         2.00835730e-04,  6.30511617e-05],\n",
            "       [-9.77689339e-05, -6.82612469e-05,  7.94864724e-05,\n",
            "        -3.56622765e-05, -1.16503819e-04]]), 'dZ2': array([[ 2.89057787e-05,  0.00000000e+00, -0.00000000e+00,\n",
            "         2.00835730e-04,  6.30511617e-05],\n",
            "       [-9.77689339e-05, -0.00000000e+00,  0.00000000e+00,\n",
            "        -3.56622765e-05, -1.16503819e-04],\n",
            "       [ 6.66143341e-05,  0.00000000e+00, -0.00000000e+00,\n",
            "        -1.61089353e-04, -1.26048618e-04],\n",
            "       [ 6.66143341e-05,  0.00000000e+00, -0.00000000e+00,\n",
            "        -1.61089353e-04, -1.26048618e-04],\n",
            "       [ 2.25740615e-06,  0.00000000e+00,  0.00000000e+00,\n",
            "        -4.10613318e-06,  1.79517032e-04],\n",
            "       [-9.77689339e-05, -0.00000000e+00,  0.00000000e+00,\n",
            "        -3.56622765e-05, -1.16503819e-04],\n",
            "       [ 2.25740615e-06,  0.00000000e+00,  0.00000000e+00,\n",
            "        -4.10613318e-06,  1.79517032e-04],\n",
            "       [ 2.89057787e-05,  0.00000000e+00, -0.00000000e+00,\n",
            "         2.00835730e-04,  6.30511617e-05],\n",
            "       [-9.77689339e-05, -0.00000000e+00,  0.00000000e+00,\n",
            "        -3.56622765e-05, -1.16503819e-04]]), 'dW2': array([[-3.36365606e-09,  1.59303662e-09, -1.86561886e-08,\n",
            "         1.76047211e-09, -2.24404299e-09,  1.95869468e-08],\n",
            "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
            "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
            "       [ 5.11028677e-08, -8.52242563e-08,  5.76634827e-08,\n",
            "        -3.20222967e-09, -2.79840678e-09,  2.46121690e-08],\n",
            "       [-1.82357291e-08, -9.79035327e-08, -3.10501908e-08,\n",
            "         1.39999055e-07,  6.83454973e-08,  4.58555826e-09]]), 'db2': array([[-1.08613071e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "        -3.96737145e-06, -1.29413673e-05]]), 'dX1': array([[ 1.33539561e-08, -5.95046332e-09,  4.77665168e-09,\n",
            "         2.38404335e-09,  4.23732684e-08,  2.54579151e-08],\n",
            "       [-1.34253166e-09,  2.01286100e-08, -2.33590054e-08,\n",
            "         1.46437016e-09, -2.17735434e-08, -1.03189691e-08],\n",
            "       [-3.52626029e-09,  7.48687448e-09, -1.25503129e-08,\n",
            "        -8.93603607e-09, -5.38032841e-08, -4.65560213e-08],\n",
            "       [-3.52626029e-09,  7.48687448e-09, -1.25503129e-08,\n",
            "        -8.93603607e-09, -5.38032841e-08, -4.65560213e-08],\n",
            "       [-8.48718870e-09, -2.16679004e-08,  3.11366006e-08,\n",
            "         5.08739377e-09,  3.32024819e-08,  3.14166833e-08],\n",
            "       [-1.34253166e-09,  2.01286100e-08, -2.33590054e-08,\n",
            "         1.46437016e-09, -2.17735434e-08, -1.03189691e-08],\n",
            "       [-8.48718870e-09, -2.16679004e-08,  3.11366006e-08,\n",
            "         5.08739377e-09,  3.32024819e-08,  3.14166833e-08],\n",
            "       [ 1.33539561e-08, -5.95046332e-09,  4.77665168e-09,\n",
            "         2.38404335e-09,  4.23732684e-08,  2.54579151e-08],\n",
            "       [-1.34253166e-09,  2.01286100e-08, -2.33590054e-08,\n",
            "         1.46437016e-09, -2.17735434e-08, -1.03189691e-08]]), 'dZ1': array([[ 1.33539561e-08, -5.95046332e-09,  0.00000000e+00,\n",
            "         0.00000000e+00,  0.00000000e+00,  2.54579151e-08],\n",
            "       [-0.00000000e+00,  0.00000000e+00, -2.33590054e-08,\n",
            "         0.00000000e+00, -0.00000000e+00, -1.03189691e-08],\n",
            "       [-3.52626029e-09,  7.48687448e-09, -1.25503129e-08,\n",
            "        -0.00000000e+00, -0.00000000e+00, -4.65560213e-08],\n",
            "       [-0.00000000e+00,  7.48687448e-09, -0.00000000e+00,\n",
            "        -0.00000000e+00, -0.00000000e+00, -4.65560213e-08],\n",
            "       [-0.00000000e+00, -2.16679004e-08,  0.00000000e+00,\n",
            "         5.08739377e-09,  3.32024819e-08,  3.14166833e-08],\n",
            "       [-0.00000000e+00,  2.01286100e-08, -0.00000000e+00,\n",
            "         0.00000000e+00, -2.17735434e-08, -0.00000000e+00],\n",
            "       [-0.00000000e+00, -2.16679004e-08,  0.00000000e+00,\n",
            "         5.08739377e-09,  3.32024819e-08,  0.00000000e+00],\n",
            "       [ 1.33539561e-08, -5.95046332e-09,  4.77665168e-09,\n",
            "         0.00000000e+00,  0.00000000e+00,  2.54579151e-08],\n",
            "       [-1.34253166e-09,  2.01286100e-08, -2.33590054e-08,\n",
            "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]), 'dW1': array([[-1.05063573e-08, -2.42720796e-09,  2.05714810e-09,\n",
            "         2.37771258e-08,  9.37739514e-09,  2.20561423e-09,\n",
            "         8.36359053e-09],\n",
            "       [-2.87342990e-08, -7.93378750e-10, -3.76216786e-08,\n",
            "         6.62942169e-08, -1.32701539e-08,  1.04310332e-07,\n",
            "        -5.81371769e-08],\n",
            "       [ 1.04665206e-08,  6.40561702e-09,  5.29524910e-08,\n",
            "        -6.66752689e-08,  2.03403120e-08, -3.27353634e-08,\n",
            "         2.74681444e-08],\n",
            "       [ 9.26366354e-09, -1.10563773e-08,  6.10717574e-10,\n",
            "        -1.26687591e-08,  2.53336811e-09, -4.37659871e-09,\n",
            "         4.71862377e-09],\n",
            "       [ 3.81003749e-08, -4.42213945e-08,  1.27609486e-08,\n",
            "        -7.37704180e-08,  2.05583251e-08, -6.58979446e-08,\n",
            "         5.19076078e-08],\n",
            "       [-2.79668517e-08, -2.01227898e-08,  1.43350994e-07,\n",
            "        -6.12152403e-08, -1.94865029e-08, -6.49032895e-08,\n",
            "         1.12320896e-07]]), 'db1': array([[ 2.42656891e-09, -6.39821225e-13, -6.05463023e-09,\n",
            "         1.13053195e-09,  4.95904672e-09, -2.34427758e-09]])}\n"
          ]
        }
      ],
      "source": [
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def d_relu(x):\n",
        "    dydx = np.ones_like(x)\n",
        "    dydx[x < 0] = 0\n",
        "    return dydx\n",
        "\n",
        "def grad(prediction, target, params, cache):\n",
        "    grads = {'dZ' + str(len(params)): prediction - target}\n",
        "    for i in reversed(range(len(params))):\n",
        "        grads['dW'+ str(i+1)] = np.dot(grads['dZ' + str(i+1)].T, cache['X' + str(i)])\n",
        "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
        "        if not i:\n",
        "            break\n",
        "        grads['dX' + str(i)] = np.dot(grads['dZ' + str(i+1)], params['layer' + str(i+1)]['W'])\n",
        "        grads['dZ' + str(i)] =  grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
        "    return grads\n",
        "\n",
        "# Sanity check\n",
        "dummy_grads = grad(dummy_pred, dummy_target, dummy_params, dummy_cache)\n",
        "print(dummy_grads) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Descent Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1 training loss: 2.3025845500565656, test loss: 2.3025857207344522\n",
            "Iteration 2 training loss: 2.302583894013706, test loss: 2.3025850472298988\n",
            "Iteration 3 training loss: 2.3025831082531134, test loss: 2.3025842396053577\n",
            "Iteration 4 training loss: 2.3025819815159534, test loss: 2.302583083188983\n",
            "Iteration 5 training loss: 2.3025802579132204, test loss: 2.3025813179201915\n",
            "Iteration 6 training loss: 2.302577548967282, test loss: 2.302578546643962\n",
            "Iteration 7 training loss: 2.302573230890589, test loss: 2.3025741345696997\n",
            "Iteration 8 training loss: 2.302566280187243, test loss: 2.302567040127482\n",
            "Iteration 9 training loss: 2.302555000500304, test loss: 2.3025555359832737\n",
            "Iteration 10 training loss: 2.3025365637812274, test loss: 2.30253674489006\n",
            "Iteration 11 training loss: 2.3025062353516645, test loss: 2.302505851165224\n",
            "Iteration 12 training loss: 2.302456070258033, test loss: 2.3024547780416373\n",
            "Iteration 13 training loss: 2.302372680691931, test loss: 2.302369921171131\n",
            "Iteration 14 training loss: 2.302233442516134, test loss: 2.3022282948295865\n",
            "Iteration 15 training loss: 2.3020000404784713, test loss: 2.301990981007357\n",
            "Iteration 16 training loss: 2.3016074940818925, test loss: 2.3015919926854362\n",
            "Iteration 17 training loss: 2.3009455953431686, test loss: 2.3009194323560678\n",
            "Iteration 18 training loss: 2.299827835020298, test loss: 2.2997839818415278\n",
            "Iteration 19 training loss: 2.2979404401273613, test loss: 2.2978672155109963\n",
            "Iteration 20 training loss: 2.2947621887263274, test loss: 2.294640374699615\n",
            "Iteration 21 training loss: 2.2894482374886826, test loss: 2.2892467317298535\n",
            "Iteration 22 training loss: 2.2806911627493256, test loss: 2.2803614558629075\n",
            "Iteration 23 training loss: 2.266638077783709, test loss: 2.2661092230261883\n",
            "Iteration 24 training loss: 2.2450857927845416, test loss: 2.24426532905966\n",
            "Iteration 25 training loss: 2.2142870811696738, test loss: 2.2130729520869448\n",
            "Iteration 26 training loss: 2.174185778929885, test loss: 2.172472854195511\n",
            "Iteration 27 training loss: 2.1260631712723006, test loss: 2.123667643614235\n",
            "Iteration 28 training loss: 2.069088595658825, test loss: 2.065619027797819\n",
            "Iteration 29 training loss: 1.9992135204171675, test loss: 1.994144203305256\n",
            "Iteration 30 training loss: 1.9134208008753508, test loss: 1.9062334455904664\n",
            "Iteration 31 training loss: 1.812674383397567, test loss: 1.8030980410783872\n",
            "Iteration 32 training loss: 1.70111394666859, test loss: 1.6890723107333805\n",
            "Iteration 33 training loss: 1.584808546453297, test loss: 1.5706962705007952\n",
            "Iteration 34 training loss: 1.4712101807755258, test loss: 1.4553135925374807\n",
            "Iteration 35 training loss: 1.3661857963483617, test loss: 1.3494994437536192\n",
            "Iteration 36 training loss: 1.2733998719706667, test loss: 1.255218377819916\n",
            "Iteration 37 training loss: 1.1954889342646289, test loss: 1.1788322677956709\n",
            "Iteration 38 training loss: 1.1413105668699455, test loss: 1.1200429898148496\n",
            "Iteration 39 training loss: 1.1401307161339258, test loss: 1.1303606590534052\n",
            "Iteration 40 training loss: 1.3532845175433765, test loss: 1.3252554553016525\n",
            "Iteration 41 training loss: 1.691673215768006, test loss: 1.7090333267830133\n",
            "Iteration 42 training loss: 2.095873088961907, test loss: 2.076541986729873\n",
            "Iteration 43 training loss: 1.6159858979640012, test loss: 1.6151545381681804\n",
            "Iteration 44 training loss: 1.291600792264091, test loss: 1.27839532339399\n",
            "Iteration 45 training loss: 1.120149494380137, test loss: 1.098875673702922\n",
            "Iteration 46 training loss: 0.9694792279207616, test loss: 0.9567971350438195\n",
            "Iteration 47 training loss: 0.8994423703171374, test loss: 0.8788598610552677\n",
            "Iteration 48 training loss: 0.8840535855866422, test loss: 0.8705638663493135\n",
            "Iteration 49 training loss: 0.9258526068116489, test loss: 0.9091230204449765\n",
            "Iteration 50 training loss: 1.1091117728287965, test loss: 1.102855884796601\n",
            "Iteration 51 training loss: 1.241843469071458, test loss: 1.2325001269669458\n",
            "Iteration 52 training loss: 1.0763216498050037, test loss: 1.0581908128819013\n",
            "Iteration 53 training loss: 0.7859484996811915, test loss: 0.7681315825253585\n",
            "Iteration 54 training loss: 0.7211076915223646, test loss: 0.7032229504086506\n",
            "Iteration 55 training loss: 0.6893921720826375, test loss: 0.6723351395180037\n",
            "Iteration 56 training loss: 0.6812280375622575, test loss: 0.663882306361477\n",
            "Iteration 57 training loss: 0.6991954904366676, test loss: 0.6833848814053435\n",
            "Iteration 58 training loss: 0.7523985980162918, test loss: 0.7371164026409152\n",
            "Iteration 59 training loss: 0.8461039351582887, test loss: 0.8343104717808066\n",
            "Iteration 60 training loss: 0.8432528303049848, test loss: 0.8266247181461475\n",
            "Iteration 61 training loss: 0.8210312733498012, test loss: 0.8118926202677383\n",
            "Iteration 62 training loss: 0.7356303864376146, test loss: 0.717749841434542\n",
            "Iteration 63 training loss: 0.6662683792165571, test loss: 0.6527877970644227\n",
            "Iteration 64 training loss: 0.6173652254833387, test loss: 0.600607789140985\n",
            "Iteration 65 training loss: 0.5914832517321469, test loss: 0.5766410647985204\n",
            "Iteration 66 training loss: 0.5762136275518327, test loss: 0.5600675216522877\n",
            "Iteration 67 training loss: 0.5685964503403472, test loss: 0.5543207697694382\n",
            "Iteration 68 training loss: 0.5681889156153866, test loss: 0.5528583536997191\n",
            "Iteration 69 training loss: 0.57084385516352, test loss: 0.5580723909414657\n",
            "Iteration 70 training loss: 0.5766885833921797, test loss: 0.5627516725640263\n",
            "Iteration 71 training loss: 0.5782323543154664, test loss: 0.5665203268152843\n",
            "Iteration 72 training loss: 0.5738198269253447, test loss: 0.560635070821639\n",
            "Iteration 73 training loss: 0.5630673922699265, test loss: 0.5506887586957313\n",
            "Iteration 74 training loss: 0.5469170127858323, test loss: 0.533267196058766\n",
            "Iteration 75 training loss: 0.5293991782374317, test loss: 0.5156441243693999\n",
            "Iteration 76 training loss: 0.5125398311396032, test loss: 0.4979484800062468\n",
            "Iteration 77 training loss: 0.496415892214096, test loss: 0.48159474352997395\n",
            "Iteration 78 training loss: 0.4833990475507822, test loss: 0.4679918644694718\n",
            "Iteration 79 training loss: 0.4711912564302207, test loss: 0.4556464973940408\n",
            "Iteration 80 training loss: 0.4617186011494965, test loss: 0.44583298633160245\n",
            "Iteration 81 training loss: 0.4527803931926485, test loss: 0.4367180334463865\n",
            "Iteration 82 training loss: 0.4458214038892267, test loss: 0.42974826509642816\n",
            "Iteration 83 training loss: 0.4391447798669838, test loss: 0.4226768248896926\n",
            "Iteration 84 training loss: 0.4340825428105275, test loss: 0.418027914137279\n",
            "Iteration 85 training loss: 0.4290175052136568, test loss: 0.41218005696047105\n",
            "Iteration 86 training loss: 0.42565197184086706, test loss: 0.40979378653886067\n",
            "Iteration 87 training loss: 0.4219716558319748, test loss: 0.40473180939217207\n",
            "Iteration 88 training loss: 0.4207321548228818, test loss: 0.4052556439886916\n",
            "Iteration 89 training loss: 0.4184891184880455, test loss: 0.40078609748482596\n",
            "Iteration 90 training loss: 0.4206422947034598, test loss: 0.40577787560414313\n",
            "Iteration 91 training loss: 0.4198777199593431, test loss: 0.4016707685469377\n",
            "Iteration 92 training loss: 0.42764428019117506, test loss: 0.41363175598482294\n",
            "Iteration 93 training loss: 0.4272334478899307, test loss: 0.4085987664523914\n",
            "Iteration 94 training loss: 0.4423622181918762, test loss: 0.42936800428491806\n",
            "Iteration 95 training loss: 0.4378423697542562, test loss: 0.4191962260836448\n",
            "Iteration 96 training loss: 0.45665126945840795, test loss: 0.4442756365834672\n",
            "Iteration 97 training loss: 0.44246352160320696, test loss: 0.4245652848890385\n",
            "Iteration 98 training loss: 0.45402877411097187, test loss: 0.441132124637178\n",
            "Iteration 99 training loss: 0.43354462990808695, test loss: 0.416869821452414\n",
            "Iteration 100 training loss: 0.4327055340881417, test loss: 0.4184596526921222\n",
            "Iteration 101 training loss: 0.41473426668849334, test loss: 0.39901090761491204\n",
            "Iteration 102 training loss: 0.4071387023461033, test loss: 0.3916799932020681\n",
            "Iteration 103 training loss: 0.3946549008059358, test loss: 0.3794386827680864\n",
            "Iteration 104 training loss: 0.3869317541614903, test loss: 0.37076308678163616\n",
            "Iteration 105 training loss: 0.37919712939496125, test loss: 0.36426780710603623\n",
            "Iteration 106 training loss: 0.37371432933126353, test loss: 0.3571676640528519\n",
            "Iteration 107 training loss: 0.36893287004969216, test loss: 0.35424414438026736\n",
            "Iteration 108 training loss: 0.3653593812246433, test loss: 0.3486064165715182\n",
            "Iteration 109 training loss: 0.3620972659735349, test loss: 0.3476590720056048\n",
            "Iteration 110 training loss: 0.359681862709533, test loss: 0.3428057400077574\n",
            "Iteration 111 training loss: 0.35717649961771836, test loss: 0.34301094182659636\n",
            "Iteration 112 training loss: 0.35545715773936276, test loss: 0.33848469560623545\n",
            "Iteration 113 training loss: 0.35336418969909167, test loss: 0.33948947842393656\n",
            "Iteration 114 training loss: 0.35209953010536305, test loss: 0.3350397585206425\n",
            "Iteration 115 training loss: 0.3502607025769393, test loss: 0.3366951691848635\n",
            "Iteration 116 training loss: 0.3493943016925337, test loss: 0.33222447883015027\n",
            "Iteration 117 training loss: 0.34772568776093093, test loss: 0.334514654697534\n",
            "Iteration 118 training loss: 0.34724466193853476, test loss: 0.3299409364412548\n",
            "Iteration 119 training loss: 0.3456466175918677, test loss: 0.3328259965435704\n",
            "Iteration 120 training loss: 0.3455348299999301, test loss: 0.3280861205605552\n",
            "Iteration 121 training loss: 0.3439472197584303, test loss: 0.3315350788138769\n",
            "Iteration 122 training loss: 0.34419582639509066, test loss: 0.3266010872382802\n",
            "Iteration 123 training loss: 0.34248713008632975, test loss: 0.3304711651368366\n",
            "Iteration 124 training loss: 0.3430033979203744, test loss: 0.325290482492669\n",
            "Iteration 125 training loss: 0.3410622310791194, test loss: 0.3293974482785239\n",
            "Iteration 126 training loss: 0.34168519371219025, test loss: 0.3239115029096193\n",
            "Iteration 127 training loss: 0.33939133015558665, test loss: 0.3279992763259171\n",
            "Iteration 128 training loss: 0.33989253126862007, test loss: 0.3221445369736506\n",
            "Iteration 129 training loss: 0.337219705074023, test loss: 0.32597182121283985\n",
            "Iteration 130 training loss: 0.33735791379321395, test loss: 0.3197398407213218\n",
            "Iteration 131 training loss: 0.334371775124903, test loss: 0.3231178826297716\n",
            "Iteration 132 training loss: 0.3339655803317825, test loss: 0.3165801995289255\n",
            "Iteration 133 training loss: 0.33086704629737573, test loss: 0.3194668935169649\n",
            "Iteration 134 training loss: 0.3299718748215258, test loss: 0.3128983503028782\n",
            "Iteration 135 training loss: 0.32699516036661547, test loss: 0.3153513540949947\n",
            "Iteration 136 training loss: 0.32573734515370706, test loss: 0.30903536630026285\n",
            "Iteration 137 training loss: 0.3230639572839631, test loss: 0.3111257317676232\n",
            "Iteration 138 training loss: 0.32165247299520866, test loss: 0.30533824492140793\n",
            "Iteration 139 training loss: 0.3193510382405392, test loss: 0.30712376612246756\n",
            "Iteration 140 training loss: 0.31791986827045526, test loss: 0.3019824099063993\n",
            "Iteration 141 training loss: 0.31596484522678, test loss: 0.30347887735315615\n",
            "Iteration 142 training loss: 0.31458137702172123, test loss: 0.29900028619802516\n",
            "Iteration 143 training loss: 0.3129250632944226, test loss: 0.3002340876243397\n",
            "Iteration 144 training loss: 0.31163516059326657, test loss: 0.2963741201055165\n",
            "Iteration 145 training loss: 0.3101904769701947, test loss: 0.29734463575914544\n",
            "Iteration 146 training loss: 0.30898808624023205, test loss: 0.2940206521785972\n",
            "Iteration 147 training loss: 0.3077095497064486, test loss: 0.29474854181373555\n",
            "Iteration 148 training loss: 0.3065787962457926, test loss: 0.2918730279894174\n",
            "Iteration 149 training loss: 0.3054229041401257, test loss: 0.29238344194374816\n",
            "Iteration 150 training loss: 0.3043635692515716, test loss: 0.28989013518184686\n",
            "Iteration 151 training loss: 0.30329204976978225, test loss: 0.2902123013145265\n",
            "Iteration 152 training loss: 0.30228717676512745, test loss: 0.288020154121176\n",
            "Iteration 153 training loss: 0.30128306940255634, test loss: 0.2881926202934889\n",
            "Iteration 154 training loss: 0.30031914830282713, test loss: 0.2862382979629409\n",
            "Iteration 155 training loss: 0.2993621329826607, test loss: 0.2862848657717238\n",
            "Iteration 156 training loss: 0.2984343476833259, test loss: 0.28452556777430593\n",
            "Iteration 157 training loss: 0.29751583657714353, test loss: 0.284466209296479\n",
            "Iteration 158 training loss: 0.29661490632919885, test loss: 0.28286643989450344\n",
            "Iteration 159 training loss: 0.29572504828673507, test loss: 0.28271674365011784\n",
            "Iteration 160 training loss: 0.294849404016927, test loss: 0.28125280133702574\n",
            "Iteration 161 training loss: 0.29398307411994956, test loss: 0.2810308083177306\n",
            "Iteration 162 training loss: 0.2931302682136136, test loss: 0.2796835129480995\n",
            "Iteration 163 training loss: 0.29228362879454756, test loss: 0.27939483134867543\n",
            "Iteration 164 training loss: 0.2914502042620047, test loss: 0.2781516477159063\n",
            "Iteration 165 training loss: 0.2906243362071329, test loss: 0.27781149788508774\n",
            "Iteration 166 training loss: 0.2898082744004154, test loss: 0.27665342384018643\n",
            "Iteration 167 training loss: 0.28899891015059953, test loss: 0.27626285051136645\n",
            "Iteration 168 training loss: 0.28819847734005327, test loss: 0.27518930358435534\n",
            "Iteration 169 training loss: 0.2874055858313578, test loss: 0.274755551268313\n",
            "Iteration 170 training loss: 0.28662089305446814, test loss: 0.2737522849823066\n",
            "Iteration 171 training loss: 0.28584192475113973, test loss: 0.27328678701580167\n",
            "Iteration 172 training loss: 0.2850704178496173, test loss: 0.2723378628739922\n",
            "Iteration 173 training loss: 0.28430530941113724, test loss: 0.27184502210461536\n",
            "Iteration 174 training loss: 0.28354539733305284, test loss: 0.27094764844814206\n",
            "Iteration 175 training loss: 0.28279080759270997, test loss: 0.2704288848639257\n",
            "Iteration 176 training loss: 0.2820427046332623, test loss: 0.26957823124348945\n",
            "Iteration 177 training loss: 0.2813005512502395, test loss: 0.26904534316279516\n",
            "Iteration 178 training loss: 0.28056430258344767, test loss: 0.2682331423407586\n",
            "Iteration 179 training loss: 0.2798342281675646, test loss: 0.26768938851258156\n",
            "Iteration 180 training loss: 0.27911005138841355, test loss: 0.2669115961072961\n",
            "Iteration 181 training loss: 0.2783908433099095, test loss: 0.26635629191112653\n",
            "Iteration 182 training loss: 0.2776758274281713, test loss: 0.2656044854169599\n",
            "Iteration 183 training loss: 0.27696543218458664, test loss: 0.2650405715498703\n",
            "Iteration 184 training loss: 0.27625987857591716, test loss: 0.26431434340862486\n",
            "Iteration 185 training loss: 0.2755587531033153, test loss: 0.2637481378153907\n",
            "Iteration 186 training loss: 0.27486296426092455, test loss: 0.2630441999283006\n",
            "Iteration 187 training loss: 0.2741707791923732, test loss: 0.26247571155113336\n",
            "Iteration 188 training loss: 0.27348288369977586, test loss: 0.2617959401973848\n",
            "Iteration 189 training loss: 0.27279940147116966, test loss: 0.2612239821315904\n",
            "Iteration 190 training loss: 0.27211956856768293, test loss: 0.2605629889114909\n",
            "Iteration 191 training loss: 0.27144337458687934, test loss: 0.259993208791462\n",
            "Iteration 192 training loss: 0.27077093505440836, test loss: 0.2593484052967056\n",
            "Iteration 193 training loss: 0.270102765376375, test loss: 0.2587787663839077\n",
            "Iteration 194 training loss: 0.2694384476166127, test loss: 0.25814473633381907\n",
            "Iteration 195 training loss: 0.26877757110856376, test loss: 0.25757542272035494\n",
            "Iteration 196 training loss: 0.2681202427148677, test loss: 0.25695356519479795\n",
            "Iteration 197 training loss: 0.2674671407246082, test loss: 0.2563867522399309\n",
            "Iteration 198 training loss: 0.266817647382168, test loss: 0.2557752733822755\n",
            "Iteration 199 training loss: 0.2661721749228944, test loss: 0.25520807128034695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 200 training loss: 0.26553059374324534, test loss: 0.2546080486575411\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f87451a8f40>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQD0lEQVR4nO3deXyU5b3//9c9ySQz2ckeIEBABJFVFMQNVBSpdanWWqu1WqvVYqvSVkvPt27nnOKv1qUL1Vbr0qp1Oe5LtSyCIptsgshOICxJCIHsyyQz9++PKwtj1gkJk8m8n4/HPDJzb3MNg+Tt51puy7ZtGxEREZEgcQS7ASIiIhLeFEZEREQkqBRGREREJKgURkRERCSoFEZEREQkqBRGREREJKgURkRERCSoFEZEREQkqCKD3YDO8Pl8HDhwgPj4eCzLCnZzREREpBNs26a8vJz+/fvjcLRd/wiJMHLgwAGys7OD3QwRERHpgr179zJw4MA294dEGImPjwfMh0lISAhya0RERKQzysrKyM7Obvo93paQCCONXTMJCQkKIyIiIiGmoyEWGsAqIiIiQaUwIiIiIkGlMCIiIiJBFRJjRkREROT483q91NXVtbk/IiKCyMjIY152Q2FEREREWqioqGDfvn3Ytt3ucTExMWRlZREVFdXl91IYERERET9er5d9+/YRExNDWlpaq5UP27bxeDwUFRWRm5vL8OHD213YrD0KIyIiIuKnrq4O27ZJS0vD7Xa3eZzb7cbpdLJnzx48Hg8ul6tL76cBrCIiItKqzowF6Wo1xO8ax3wFERERkWOgMCIiIiJBpTAiIiIiQaUwIiIiIkEV3rNpHn+cDW/ncqTEgoZBOrbV+NzCirCYeg7mtWXx5SaLomILC7PNbjgOy8Ky4JypFo4I8/qrzRYHDzafax91HJbFmWdbOJ0WOJ3sOBBDfmkMEfExRCbE4B6YQvywdDJOGYA7KTpYfzoiIhLmOlpjpLPHdCS8w8grrzB2xYr2j1nV/HR0R9db2vx0VMOjTfObn57Q8Pg6Hxb7IrJJnHIy8TPOgKlT4cwzoRtGLouIiLQlIiICAI/H0+7UXoCqqioAnE5nl98vvMPIDTfwRcp5HDlsYwOWbcNRD8sBU89ufv3lRptDhzD7aN7eeO7ZZ9s4GrZv3gxFBxuOwcZqOA7bPJ9yuk1khA11dezYUEXJgSqcdVVE1VWQUHuIft4iYqhmoDcPlubB0n8DUJYyhFUn/5CxT/6E9JNSgvZHJyIifVdkZCQxMTEUFRXhdDpbnb5r2zZVVVUcPHiQpKSkpgDTFZbdHfWVHlZWVkZiYiKlpaUkJCQEuznHhe2zOby1iCOrtnNC2Vr47DPsDz/EKi0FoMhK48ub/8i0J67GchzbPQFERES+zuPxkJubi8/na/e4pKQkMjMzW12TpLO/vxVGQoi3opo1v36d5Kce4oSaTQAs6f9dJm/5B674rpfHREREWuPz+fB4PG3udzqd7VZEOvv7W4MPQkhEnJtJf7yOIcVrWTbjATw4mXrgZVaMuJ7aKm+wmyciIn2Mw+HA5XK1+TiWrhm/9+mWq8hxFRkTxRkf3suW376JByfT8l9m6Yib8Nb3+iKXiIhICwojIWzsnIvZcv8r1BPB+fuex/vsP4LdJBERkYApjIS4sfd9i9rf/C8AUb+aDUVFQW6RiIhIYBRG+oDY38yGsWPh8GHs2bPp/UOSRUREmimM9AVOJzz1FLZlYb3wAp8++HGwWyQiItJpCiN9xaRJfH7KrQBEPfqQqiMiIhIyFEb6kKFP/BIvDk4v+w/r/vllsJsjIiLSKQojfUjqaTl8MeRyAIrvfTyobREREekshZE+JuV/ZwNw9p4X2LXiYJBbIyIi0jGFkT5m8DVnsC3xNFzUsvmOJ4PdHBERkQ4pjPQ1lkXNj+8AYNTq56nzaCSriIj0bgojfdCoOZdR43CT49uFd9WaYDdHRESkXQojfVBkUhyub18CgOutl4PcGhERkfYpjPRVV19tfr76Kvh8wW2LiIhIOxRG+qqZM7Hj4mDvXjY+vSLYrREREWmTwkhf5XazLvsyAPb9/pUgN0ZERKRtCiN9WPyPTFfNuB3/R02VumpERKR3Uhjpw074yYVUWTH0tw+w7vkNwW6OiIhIqxRG+jDLFc32gecBUPzih0FujYiISOsURvo434yZAKSv/XeQWyIiItI6hZE+btjtFwEwoXoZuV+UBrk1IiIiLSmM9HEJ44aS5z4RJ/XkPbso2M0RERFpQWEkDMR8y1RHplapq0ZERHofhZEwkPp9M26EDz8EWzfOExGR3kVhJBxMnQouF+zdC5s3B7s1IiIifhRGwoHbTdGIMwH46L+WBLkxIiIi/hRGwkTeoLMBiFj2aZBbIiIi4k9hJEykX2nCyMiDn1Jbo3EjIiLSeyiMhImB3z6dOiIZyD42vJcXtHa8/z488IDG0YqISDOFkTBhxcaQmzwRgAOvBq+r5pvfhPvvh3ffDVoTRESkl1EYCSMV401XjXN58MeN7N0b7BaIiEhvoTASRvpdasLI0P2fUl8fnDac028jV/A6gwcH5/1FRKT3URgJI4O/Z6b3jrQ3c3jboaC04ema63idbxOzd2tQ3l9ERHofhZEw4khLwT75ZADSty0NShtS6gtMWw4dDMr7i4hI76MwEmass84yT5YvP+7vXV0NkXXVAHjKao77+4uISO+kMBJuJpoZNfWfrzvub11eDjFUATB2RO1xf38REemdFEbCTPXICQCUfLyWstLju9hHdVkdkXgByExSZURERAyFkTDjPm00dUSSSjGb5+87ru9de6Sq+UWNwoiIiBgKI+HG5WJfwigACv699ri+tae0uun5gd3qphEREUNhJAyVDjsFAO9xHjdydBhZt0yVERERMRRGwpDzNDNuJCn3+FZG6krVTSMiIi0FFEbmzp3LaaedRnx8POnp6Vx++eVs3drx4lWvvfYaI0eOxOVyMWbMGD744IMuN1iOXeY3TGVkeMU6KiuP3/vWlzdXRqhVN42IiBgBhZElS5Ywa9YsVqxYwfz586mrq+PCCy+ksp3faMuWLeOaa67hpptuYt26dVx++eVcfvnlfPnll8fceOmalPPG4cMim318+XHRcXvfU0Y2V0asWlVGRETEsGy76zdzLyoqIj09nSVLlnDOOee0eszVV19NZWUl7733XtO2008/nfHjx/Pkk0926n3KyspITEyktLSUhISErjZXjnIweQTpR7aR++RH5Pz4wuPzph99BBddBMC/R/+SmRt/d3zeV0REgqKzv7+PacxIaWkpAMnJyW0es3z5cqZPn+63bcaMGSxvZwXQ2tpaysrK/B7SvdIvNONGckqO4yDWqqMqIx5104iIiNHlMOLz+bjzzjs588wzGT16dJvHFRQUkJGR4bctIyODgoKCNs+ZO3cuiYmJTY/s7OyuNlPacooZN8La4zeIdev65jEjjjp104iIiNHlMDJr1iy+/PJLXn755e5sDwBz5syhtLS06bF3795uf4+wN24cADWrv8TnOz5vuWtTcxgZfYLCiIiIGJFdOen222/nvffe45NPPmHgwIHtHpuZmUlhYaHftsLCQjIzM9s8Jzo6mujo6K40TTrJO3wkEYBj13b27KwnZ3iX/ioE5qhumv7J6qYREREjoMqIbdvcfvvtvPnmmyxatIicnJwOz5kyZQoLFy702zZ//nymTJkSWEulW0UMyabachNFHXsW5x6X97Srj5raq3VGRESkQUBhZNasWbzwwgu89NJLxMfHU1BQQEFBAdVH/ZK5/vrrmTNnTtPrO+64gw8//JBHHnmELVu2cP/997N69Wpuv/327vsUEjiHg4LEEQAcXr7luLyldVRl5Ei+woiIiBgBhZEnnniC0tJSpk2bRlZWVtPjlVdeaTomLy+P/Pz8ptdnnHEGL730En/7298YN24c//d//8dbb73V7qBXOT4qBp4EQP2XxyeMOGqbQ+v2TeqmERERI6CBAp1ZkmTx4sUttl111VVcddVVgbyVHA8njYQvIWb35uPydlZNcxhxelUZERERQ/emCWMJp40EIP3wFrq+9F3nRdQ2d9NEKYyIiEgDhZEwljnNhJETvFsoLOj5NHL6uKMqIz5104iIiKEwEsaiRw/Hh0UyR4gq7fl71MRaR1VGbFVGRETEUBgJZ243jpwhACQfPA6DWI+adRXtUxgRERFDYSTcnWRm1LC55wexHtjVHEaiqD0u41RERKT3UxgJc74TzbiR/Qt7vjJSvLe5myYhqkZhREREAIWRsLfbZcLI1nd6PoxEeY8awFpfg0N/+0REBIWRsJcx1YSRnNotlJX17Hu5fM2VEXw+qK/v2TcUEZGQoDAS5mInNoQRdvvdVbcnRPv8r19TokGsIiKiMCKpqVRGxANQuHJ3j76V267ye12wW2FEREQURsSyKEoYBkDZF7t67G28XnDjXxnxlGvhMxERURgRoCpjKAD1W3sujNRU1BNFnd+2unJVRkRERGFEADvHhJGovTt77D2OHi9SacUCCiMiImIojAipk003zZn9e64yElnXHEYqIhIBqK9UN42IiCiMCJAxxVRGMit7LoxQ1TB41eWi1uEGwFupyoiIiCiMCMBQE0bYtYueWhb10F5TGamJiKEuwgVAfYXCiIiIKIwIwODB2A4HVFeTt6qgR96iINeEkSM1bhLTo83bZqqbRkREFEYEwOnkoGsQAKte7pmumrpS001T64ghdYCpjAxMVWVEREQURqRBWarpqqn9qmdm1NSXm8qIJ8INLhNGqFEYERERhRFpUDfIzKixcnumMuItN5URT2QM1bbppik9qG4aERFRGJEGzhNNZST2YA+FkQpTGamLdPPlDlMZWfWJKiMiIqIwIg0Sxpswkla2C5+v+6/fGEbqnW58ThNGbHXTiIgICiPSIOU0E0Zy7J0cOND917crTTdNfVQMdpTppqFG3TQiIqIwIg0iR5gxI1kUkLupqoOjAzd+hKmMjBjvxhdlKiNWrSojIiKiMCKN+vXDE5MEwEmu3G6/fKLTBJzkgTHY0ZpNIyIizRRGpEnUSNNVk1rWA4NYqxvuTeN2Q7TpprE86qYRERGFETna4MHmZ15et196/3ZTGckvdTdVRiyPKiMiIqIwIkepSTersO5YuKfbr73rK1MZ2bw7hgHDTBgZNkBhREREFEbkKPsiTWVky/zur4xE1DZ30+SMNN00Q3RvGhERASKD3QDpPRJONpWR1Mo9+Hzg6MaoGuEx3TRWbAw0jF/VAFYREQFVRuQoyRNMZSTbzqOwsHuvHeExlREr1k1NQxqpOqwwIiIiCiNylMihpjKSRT55Ozzdem1nnamMOGJj2LDNdNNsWqduGhERURiRo6WlUetw4cDm4Np93XppZ72pjETEuYmIcTVsU2VEREQURuRolsXhWFMdKd/UvYNYm8JIvJuI2IYw4lUYERERhRH5msoUE0bqdnTv9N6Byaab5sRxMUTEmG6aSJ+6aURERGFEviZpvBnEOvPk7q2MuG1TGUke4CYyzlRGonyqjIiIiMKIfE3qBFMZSa/u5oXPqhpuvhcTQ2S8woiIiDRTGBF/PbQkvLfSVEYqvG6csaabJspWN42IiCiMyNcNMpWRIxvyqKjopmv6fETUmeBxuCaGxAxTGYl3qjIiIiIKI/J1DZWR6MI8dmy3u+WSdlV10/PoJDf9skwYcdkKIyIiojAiXzdgAD4sYqgmf+OhbrlkbUlzGHEnuyHadNNQVwc+X7e8h4iIhC6FEfEXHU2JKxOAki+6ZxBrzREzeLWGaNyxDuxoV9M+b5XGjYiIhDuFEWmhrJ/pqqnZ1j2DWGuPmMpINW4iI/ELI4cPqKtGRCTcKYxIC55MM4jV7qYZNXWlpjJSbcVgWeCIisSHZd6rXJUREZFwpzAiLVgNg1hdBd3TTVNbaiojtZa74Q2spjv31pWrMiIiEu4URqQF9whTGUko6Z7KSEZCw+qrA91N22otE0Y8ZQojIiLhTmFEWkidaCojU3O6pzISZ5lumsTMmKZtHsvMqKmvVDeNiEi4UxiRFlwnmspI/OFuWoW1umFqr7u5MuJxqJtGREQMhRFpqWEVVoqKmu8pcwwO7jbXOFx7VGWkIYxoaq+IiCiMSEtJSXhj4wH44r29x3y5HV+aysjm3ObKSGyy6aZJjVNlREQk3CmMSEuWRWG0qY6sefPYu2p8FSaMeKObKyOZg01lZFC6woiISLhTGJFWVaebQazeXcc+iNWuNN00vujmygiuhoXPatVNIyIS7hRGpFV2tqmMRB449spI443ybFdzGKmPNN00taWqjIiIhDuFEWlV1HBTGYk73A3Te6tNZcR2N3fTrNlkKiMrlyiMiIiEO4URaVXCyaYyklqVR13dsV3LamVqr89pwoivWt00IiLhTmFEWpUw2oSRQeSxb9+xXctRYyojVmxzZcTnNN001KgyIiIS7hRGpFWOHNNNk81e9uT6julaIwebysiJ45srI96GwayN40lERCR8KYxI67Ky8DkiiKKOUwcWHNOlUmJM4BgwrDmMNI0f6YZF1UREJLQpjEjrIiNxZA8EIK74GAexNgaOmOZuGhrCiFVVeWzXFhGRkKcwIm1rXBY+79im95YWmMpIWd1R64zExgLN40lERCR8KYxImyrTzLiRla8eW2WkYJcJHDvzmysj6UPM8wFJqoyIiIQ7hRFp06EYUxnZtvDYKiNRXlMZiUpsroycdKqpjAzLUmVERCTcKYxIm2JPMmGkX1kevmOYUBPlawgjSUeNGWkcP1KpyoiISLhTGJE2JY1tmN5r76GwsOvXcftM9SM66ajZNDGmMlJfpsqIiEi4UxiRNkUONZWRwexh9+6uXcO2wYWpjLiTm8PIF9tNZWTnBlVGRETCncKItG2wqYwkUcqBzaVdukRtjU1MQxhxJTd30ziTTGUkyqvKiIhIuFMYkbbFxlIenQJAycauDWKtLG5e7v3oykh0PxNM3D5VRkREwp3CiLSrPMl01dRu7dr0XjfNy71Hxh8VRpJNZcTlU2VERCTcKYxIuxIaBrFed07XKiMxNIQNpxMiI5u2u1NMZSSWymOaqSMiIqFPYUTaFdcwvTfhSBcXPqtuqIy43X6b3ammMuKknqrSui63T0REQp/CiLSvYRAre7oWRo7sN5URjzPGb3tjZQSgqkjjRkREwlnAYeSTTz7hkksuoX///liWxVtvvdXu8YsXL8ayrBaPgoJjuxOsHB/2IBNG9nyaR2kXJtRsXW8qIwfL/SsjDlcUXisCgEiPxo2IiISzgMNIZWUl48aNY968eQGdt3XrVvLz85se6enpgb61BIE12HTTRB7Yw86dgZ9fV9pQGYnwr4xgWUTEmW3J0aqMiIiEs8iOD/E3c+ZMZs6cGfAbpaenk5SUFPB5EmQN3TRZ5PP5Dg+nnBIV0On15aYyUud0t9wZGwvl5VClyoiISDg7bmNGxo8fT1ZWFhdccAGfffZZu8fW1tZSVlbm95AgSUvD43DhwObwhn0Bn94cRmJa7LMb7k9TX6rKiIhIOOvxMJKVlcWTTz7J66+/zuuvv052djbTpk1j7dq1bZ4zd+5cEhMTmx7Z2dk93Uxpi2VRkmi6aqq3BD6I1Vtuqh7eViojOwvMjJrPl6gyIiISzgLupgnUiBEjGDFiRNPrM844g507d/LYY4/xz3/+s9Vz5syZw+zZs5tel5WVKZAEUU3GYDiyDW9u4GuN+CpNZcQX3TKMNI4jUWVERCS89XgYac2kSZNYunRpm/ujo6OJjo4+ji2S9tiDBsMWiMoPvDJiN4wH8bpadtPURenOvSIiEqR1RtavX09WVlYw3lq6IPoE000TVxx4GBkzzFRGBgxrWRmpjzIBxVeuyoiISDgLuDJSUVHBjh07ml7n5uayfv16kpOTGTRoEHPmzGH//v384x//AODxxx8nJyeHk08+mZqaGp5++mkWLVrEf/7zn+77FNKjkk8xM2qunBR4N82gNBNGsoa1rIzUu0xlxFehyoiISDgLOIysXr2ac889t+l149iOH/zgBzz33HPk5+eTl9f8S8vj8fDzn/+c/fv3ExMTw9ixY1mwYIHfNaR3ixpmKiPuwi6swto4bdfdsjLia+i6sStUGRERCWcBh5Fp06Zh23ab+5977jm/13fffTd33313wA2TXqRxSfi8PPD5wNH53r1De6tJBSp9bmK/ts92N2zROiMiImFN96aRjg0ciO1wQG0tC146GNCpa5eaoLF1b8tumsT+Zlv/RFVGRETCmcKIdMzppCRuIAAb38kN6NQIjxkz4ohr2U1z8mmmMjJ6qCojIiLhTGFEOqU6IwcA747AwkhknQkjkfEtKyM0rMBKpSojIiLhTGFEOsXOGQpA1P5dAZ3nrDdVj8j4Nu5NA9iVqoyIiIQzhRHpFNdJpjKSWJxLO+OXW3DWm8qIM7FlZWT9NrNt5ceqjIiIhDOFEemUxAmmMjLIu4vCws6fF+01VY+oxJaVESvOVEacdaqMiIiEM4UR6ZTI4aYykkMuuQEMG4n2mcpIdFLLMNJYLYmuV2VERCScKYxI5ww1lZFs9rJra12nTrFtyEo0VY/4jJbdNM4kUxlprJ6IiEh4UhiRzsnIwOdyE4GP757RuWXhLQsSIk1lxJ3csjISlWQCisuryoiISDhTGJHOsSwcOUMAiNgTwIyaahNGmqbxHsWVYiojbluVERGRcKYwIp3X0FXT2UEjVZU2djv3pnGnmIASSyUeT7e0UEREQpDCiHRadaYZxPrRk50LI7u2eLAa5wG3FkZSGyoj1FBX6+ueRoqISMhRGJFOqx9kKiMl63ZRW9vx8TWHj+p+aaWb5ui1R2ItddWIiIQrhRHptLgxzdN79+zp+PjaEjNepJ4IcDpbHnB0tUR37hURCVsKI9Jp1jBTGRnKLnZ1YgxrYxipcbRyXxoAh6M5kOj+NCIiYUthRDovx1RGUikm78uyDg+vKzXVDk9EK/elaXCkzowbWfOpKiMiIuFKYUQ6Lz6eSncKAEfWdjyI1VNqKiOeyDYqI0CNZfZ5jqgyIiISrhRGJCCVGaarxrOl436axspIvbPtykhNpKmMeEpUGRERCVcKIxKYocMAiN63o8NDTx5qKiMxKW2HkTqnqYzUl6oyIiISrhRGJCD9Jp8IwC8u297hsSMGmTCSPKDtbpo6p6mM1JerMiIiEq4URiQgzlHDAXBs39bxwe2svtqoPtoEFV+ZKiMiIuFKYUQCc6KpjLC948pI/q6GdUbaGTPijTaVEV+FKiMiIuFKYUQCM9xURjhwgH89VdHuoW89XwLA3oqkNo9pvD9NWqwqIyIi4UphRALTrx+VMakAbPug/UGsUeWHAXCkJLd5zEmnmsrIpJNVGRERCVcKIxKwqgGmOuLb0v64EVeVCSMR6W2HkaZ71mgFVhGRsKUwIgGzRphxI6697Y8biak1YSQqo50wEmsqI7o3jYhI+FIYkYDFjTeVkf6V2ygtbf2Y+npIqDdhxDWg7TCyYYepjCx6V5UREZFwpTAiAXONNZWR4Wxvc1JNaSmkUAxA7MC2w4jXpcqIiEi4UxiRwDXMqDmRbWxrY9jIkSOQTMOYkbS2w0jjbBpHjSojIiLhSmFEAnfCCYC5e2/Z7sOtHpKQAJlRDftSUtq8VEx6HADOmvanCYuISN+lMCKBi4vDl9UfgFvPb72fJj2xlihPQ7Ujue3KSNwgsy++/jBeb/c2U0REQoPCiHSJY0QHK7EeOdJwoMOUSdoQn2OqJqkcajpFRETCi8KIdE3jSqxtDBop2mq6aHxJ/UwgaYMz0yyglsohDhXZ3dtGEREJCQoj0jUjRgCw4E+b+eKLlrs/etmEkYP17awxApBqwkgUdTiqNG5ERCQcKYxI14wdC8DAko2sX99yt++gmdZbE9NBGImJAZcLgBOTDwXcjNJSePxxOHAg4FNFRKSXUBiRrmkII8PZzpa1LdcIsYtNZaQuroMwAk3VEQ4FHkZ+8hO46y649tqATxURkV5CYUS6JiODqvh0IvBRsXJTi92OEhNGvEltT+tt0hhGiosDbsbJRz4ln0xuS3kl4HNFRKR3UBiRLvOMNNUR5+YNLfZFlpkwYvfruDKyp9KEkRf/EHhl5MRt75FJIRPy3gn4XBER6R0URqTLYk4fB8CQsi9a9LBEVZgw4kjtOIxUx5rqie9g4GEk9vA+ABLrAz9XRER6B4UR6bKoU01lZCwb2LjRf5+r2oSRyPROjBlJMZURR0ng3TTuI/sBKPhSYUREJFQpjEjXNQxiHe/YQGWF/xoho7NMGEka2nEYichomN5bFligqKuDAZgwklh3iNragE4XEZFeQmFEuu6kk7AjI0nyHeGbE/b77Rocb8JIyvCOw0hUlummiakMLIwU5NsMxHTTpFHUlck4IiLSCyiMSNdFR2ONHGmef33ls8aZMe3cl6aRO9tURmJri7EDWITVKjmCmxoAYqimaE/LKcYiItL7KYzIsWnoqvGu20B9vdlUUwPeQw137O1EGInPMWEk2XeIqgDyxEDLvxpTtkulERGRUKQwIsemIYy8cf8GPv7YbNr+VR0RleXmRUrH64y4Bpow0j/qENXVAbz3fv8wUrm7KICTRUSkt1AYkWMzzkzvHe1d3xRGKvaa2+/6sCAxscNLWKmNd+4tJjWl8/00tTv3+b2u3qvKiIhIKFIYkWMzcSIAJ7GFNfNN10z1fvOzPCIJIiI6vkbjCqweD1R0/mZ57//NvzLiOKwwIiISihRG5NikpVE31NzB17V2GeXlUHPAhJHKqE6sMQLmZnlut3kewJSY6EP+YeTysxRGRERCkcKIHDPnuWcBMMW3lKVLof6gCSNV7k6GEaAk0nTVvPTHzgeK+FLTTeN1xZgNRRozIiISihRG5NidZcLIWSxl0SLwHjTTemtjOx9GKlymq6Zmf+dXYe1XbSojnhFmEK0WGhERCU0KI3LsGsLIaXzOZwtr4LCpjNTHdz6MeBJMGOns/WnKyyHLZ8JI5XAziHbVBwojIiKhSGFEjt2wYXjTMojGw08mr2ZUZueXgm/k69cwBbiT1Y383BpSMVWU+tHjAag9oDAiIhKKFEbk2FkWEeeY6sh1g5cyIs2EkcGndLzGSNMlGmbURJZ2rpumeIOpilRbbmLGDgcg2Vuk+9OIiIQghRHpHg1dNbz+Onz0kXneidVXG0VmNtwsr7xz1Y3khvEiFYkDiMtJAyCVQxrDKiISghRGpHucfbb5uXo17NwJ6elw5ZWdPj16gAkjMVWdCyMj4kwYSRs/EEe6OTeFYooKfQE0WkREegOFEeke48Y1r7Z63nnmxnkDB3b69LjBpktnSFwxvs7kiX0Nq68OGNC05HwkXo7sLg2k1SIi0gtEBrsB0kdERsJbb0FuLlx/fedWXj1K3BBT3Rg/8FCnInLtrv1Egwkj0dFURCQQ5y2jfFcR0C/Q1ouISBCpMiLdZ9o0uPHGgIMI0LwkfCdn03z2mumm+bLEVF8qG9Yp8RZqRo2ISKhRGJHeofHuvsXFYHd8s7zEqnwArP5ZAKSeZMLIFecojIiIhBqFEekd0tPNT4+HF+eVdHh4nMdMH47JNiEmomEQq6bTiIiEHoUR6R1cLiqjkgCo3FnQ7qE+HyR6TRhpHPhKmpneqyXhRURCj8KI9BoV8abLpS4vv93jSo7YpDSsvpqYY9YyOWibysiClxVGRERCjcKI9Bq1ySaM+Pa3H0YO51XgpB6AqEwTRqrcJoyU7lQYEREJNQoj0mv40k0YiSxqP4yU5poumlqiwe0252SZbpqEWo0ZEREJNQoj0ms4Bpgw4jrSfhhpHC9SHZsClgWAM8tURhLrVRkREQk1CiPSa0QNzgQgvqL9MHJCPzNeJCmn+d43Uf1NGEn2HcLr7aEGiohIj1AYkV4jbripjJwQl99+oDhsKiNH34jPnZEAQDzlVFX1VAtFRKQnKIxIrxF3ggkj49Pz213E1VPQMoxEp8SZa1BBRUWPNVFERHqAwoj0HlkmjJDffjfNBy+YMPLFvuYwYsWbMBJLFZVl6qcREQklCiPSezSGkbIy7Mq2+1ocJWbMiDcxpXljfHzT0xP6q59GRCSUKIxI75GQgCfSTNV97U9tr8LqLDOVkYi05soI0dHNN+grL++xJoqISPcLOIx88sknXHLJJfTv3x/Lsnjrrbc6PGfx4sWccsopREdHc8IJJ/Dcc891oanS51kWJW5THane1XZXjavKhBFnxlFhxLIgznTVaNCIiEhoCTiMVFZWMm7cOObNm9ep43Nzc7n44os599xzWb9+PXfeeSc/+tGP+OijjwJurPR91Qlmeq93X9thJKbGhBFX/2S/7eW2CSNrP1EYEREJJZGBnjBz5kxmzpzZ6eOffPJJcnJyeOSRRwA46aSTWLp0KY899hgzZswI9O2lj6tLzYL9tDmI1bYhoc6MGYkdlOK3r8QbRzyQv11hREQklPT4mJHly5czffp0v20zZsxg+fLlbZ5TW1tLWVmZ30PCg51pummiilsPIxUV0A9TGUkY4l8Z8USZykh9icKIiEgo6fEwUlBQQEZGht+2jIwMysrKqK6ubvWcuXPnkpiY2PTIzs7u6WZKLxGZbcKIu7T1MFJfZ5PqaL2bps5lZtR4SzWAVUQklPTK2TRz5syhtLS06bF3795gN0mOE1eOCSOJVa2HkX5RlUT66gCwUvzDiNdlKiN2mSojIiKhJOAxI4HKzMyksLDQb1thYSEJCQm4G+64+nXR0dFER0f3dNOkF4o/0YSRoa586ush8ut/Q4vNeBGioiAmxm+X190QRjSbRkQkpPR4ZWTKlCksXLjQb9v8+fOZMmVKT7+1hKDG+9MMdee3DCJAXWHDUvApzXfsbWTHmjBiKYyIiISUgMNIRUUF69evZ/369YCZurt+/Xry8vIA08Vy/fXXNx1/6623smvXLu6++262bNnCX/7yF1599VXuuuuu7vkE0rc0rsJ66BDU1bXY/d4/TBjZV53cYp/dsM6IVaUwIiISSgIOI6tXr2bChAlMmDABgNmzZzNhwgTuvfdeAPLz85uCCUBOTg7vv/8+8+fPZ9y4cTzyyCM8/fTTmtYrrUtNNSup2jbVew622N1YGal2twwjJ08yYeTy6QojIiKhJOAxI9OmTcO27Tb3t7a66rRp01i3bl2gbyXhyOGgIi6DuNIDPHlfPne9OMBvt6/IjBmpj28ZRpzJZjaNo0KzaUREQkmvnE0j4a0qxUzlLlmf23LnYVMZ8fZLablPy8GLiIQkhRHpdSLHngxAzI6N+Hz++yIabpJnJbesjBRVmzCy+XOFERGRUKIwIr1O4tljATjRs5Fdu/z3RZWbMBKZ3jKMVFomjJTlK4yIiIQShRHpdSLGjwFgLBtYs8Z/n7vajBmJymwZRlypJoy4vQojIiKhRGFEep8xJowMYxdfrvAPFkMSTGUkfkjLMSPRqWYAa6yvvEX3joiI9F4KI9L7pKVRlZgJQMlnm/x2nZhiwkjqiS0rI+40UxmJo4LKyh5uo4iIdBuFEemVvKNMdeTi7A3+Oxpm09DKANbolOYwogk1IiKhQ2FEeqX4M8wg1osGbGza9tGHNnY7YcSKN2EklioqSr0930gREekWCiPSOzWMG2GjCSN5efDAzOVYHg92bCxkZLQ8p3GdEaDqUNXxaKWIiHSDHr9rr0iXNIQR7/oN/OwnNoOHWFzLiwBYV1wBrd3V2eXCdjiwfD7G5FQA8cexwSIi0lUKI9I7jRqF7XAQUXKYN5/I54gjlb28YvZdd13r51gWVnw8lJbiqCwHso5bc0VEpOsURqR3crmwTjwRtmxhgmMDDl8dqRTjTcsg4rzz2j4vLg5KS7UkvIhICNGYEem9xppBrH8+5Rlu5ikAIq69BiLbztAlXjNuZO0nCiMiIqFCYUR6ryuvBCBn9WtcyrtmW1tdNA2Ka00Y2btZYUREJFQojEjv9Z3vwIcfwvDh5vXJJ8Mpp7R7Sn20CSO+MoUREZFQoTEj0rvNmGGm9777LkycCJbV7uH17oYwUlp+PFonIiLdQGFEer/oaPj2tzt1qNdtpvPaGsAqIhIy1E0jfYodayojlsKIiEjIUBiRPsVuWIXVURVYGDlwAHJzwduwirzPB7W13d06ERFpjcKI9ClWQxiJqA4sjDz1SBnvDv0ZL874Bxu/8HHmmfDrX/dEC0VE5OsURqRPGXmqCSMXnRXYANbUJa/zM/7E9Qt/wOArJhK54lP+8AdYv74HGikiIn4URqRPiU4xYSSqNrDKSHTetqbnCbvW87HjfC7w/psf/7i560ZERHqGwoj0LfENN8cLYACrzwcpxdsBOHzbf8EVVxDpq+N1riRq1ad88klPNFRERBopjEifUlJvKiO5GzsfRvbvhxzfDgASZ5wOL78MF19MDNW8xzfZ+VlBj7RVREQMhRHpU6ojTBgpL+h8GNm21eYETBiJGDkcnE547TWKkkeQSBn2EpVGRER6ksKI9CnRqSaMxPgq8Pk6d86+1QXEUYkXB+TkmI1uN2UnTwHA2r6tnbNFRORYKYxIn+JuCCPxlFNV1blzJiWbqkhV2mCIimrannSauSfOBYMVRkREepLCiPQproYwEkdFp8ewnuQ0YSR+/Al+21OmnAjAYM/27mugiIi0oDAifYqVYGbTxFLFvj2dnJO7vSFsnOAfRjjRhBG2qTIiItKTFEakb2lYgRVg3Wcd99PU10PhZ6YyYp8w3H9nYzg5fJgtnxV3WxNFRMSfwoj0LS4XPsv8te7M9N7cXDjwiamM2MO+VhmJiaEkbiAAH/5JXTUiIj1FYUT6FsvCTk0D4H9v2dPh4UdP63WMGN5if9UAs82zSWFERKSnKIxInxMx6VQArNWfd3jsvrUHiacCH1bztN6jOEaacSNHLxcvIiLdS2FE+p5Jk8zPlSs7PLR0jal4lCYOgujoFvsTJprKSGbZNsoDu/eeiIh0ksKI9D2TJwOw/61VzJvX/qEln5suGs/gll00ADHjTWVkONvZsqX7migiIs0URqTvOe00AAZUbmf5+4fbPCw/H2LyG+5Jc8oJrR803ISUE9nGV5vs7m2niIgACiPSFyUnUz3QhAjvis+x28gQixbBeNYD4Bo/svWDhg7FZzmIo5K8VbphnohIT1AYkT7JeZYZNzL8yEr27m39mAum1THDtcS8OOec1g+KiqK2vxnY+oMpGsQqItITFEakT4o8w4wbmcQqPvus9WPS967BWVMB/frBuHFtXss9xlRZBtVqeq+ISE9QGJG+qWFGzWRW8vRTbfTTfPyx+XnuueBo5z+FxmXhv/qqGxsoIiKNFEakbxo3DtvpJI1D7Pp4Nxs3+u/+6CPY/ewi8+K889q/1sSJAOS/s4rVq3ugrSIiYU5hRPomlwtr/HgAZo39tMUg1hf+XkvG9qXmxbnntn+thqnCiTvX8sYrdd3cUBERURiRvusb3wDgFzFPMHZs8+aNG6Hw7RW4qcGTnAEnndT+dYYPp9adSAzVHP50Uw82WEQkPCmMSN91220QFQUrVsCyZWzbBvv2wZVXwpke00XjvPA8sKz2r+NwUDvWrF3i3riyzanCIiLSNQoj0ndlZMB11wHge+RRrroKhgyB7dttLo36EADr/A7GizSImWa6akZXrWL37p5orIhI+FIYkb5t9mwArLfeJLVsF14v3BzxDBM8q8DphBkzOnWZyDOaZ+esWdNjrRURCUsKI9K3nXwyXHQRls/H/Mzr+Ohn7/OE8w6z77//G7KzO3edhqnCo/iKjct0xzwRke6kMCJ93wMPgNuNY8VyLvzjN4moqYSpU+EXv+j8NTIzKU8ehAObmqWa3ysi0p0URqTvmzQJNm2Ciy82r5OS4B//gIiIgC4TcbqpjvzPpau6uYEiIuFNYUTCQ04OvPsuLF8Oa9fCoEEBXyLmXDOI1bl2ZXe3TkQkrEUGuwEix41lwemnd/38KVPMz08/BZ+v/SXkRUSk0/SvqUhnTZqELzYODh3izvM3dny8iIh0isKISGc5nXjPOAeAiMULtN6IiEg3URgRCYBz5nQAprOAjz4KcmNERPoIhRGRQEw3YeQcPmHBB54gN0ZEpG9QGBEJxOjR1CWnE0sV5fNXUFER7AaJiIQ+hRGRQFgWkTNMdeSM6gU8/nhwmyMi0hcojIgEyJp+PmDGjTz8MBw6FOQGiYiEOIURkUA1jBuZxCp+fU0uTmeQ2yMiEuIURkQCNWgQXHABkXi559AvSUwMdoNEREKbwohIVzz6qFmB9fXXYfFijhyBwsJgN0pEJDRpOXiRrhg9Gm67DebNw/OTO5jmWEtEVASLFpn78LWmqgpeeQV27oT9+2HgQPjWt2DCBLNSvYhIuLJs27aD3YiOlJWVkZiYSGlpKQkJCcFujohRXAzDh8ORIzwe82vuqvpfMjLggQfg6qvB5YKyMkhPN4dXV5sAcviw/2VGjoSXXjKhRESkL+ns729104h0VUoK/PnPANxZ9VtmZ7xIYSHceiv06wduNwwdau6pB+b1Iz/dzQtT/8bGMd/j7eE/Z4xrO1u2mHvw/fOfQfwsIiJBpG4akWPxve/Bxo3w0EP8vuQmvnFNDTct/B57DroBqKmBvasLGbziFXjhBW74/POmU0cDl/IoKzMu4duF83C7s4P0IUREgkvdNCLHyueDK66At98GwE5Oxp56LnUZA3Du2oZj4Xzwes2xERGmDHLeebBmDXzwAdg2dfH9cD73tLmOiEgf0dnf3wojIt2hpgYefxyefBL27Gm5f9IkuO46M5ikcRAJwObNcP31sHq1ef3nP1P83Vns2wfjxh2XlouI9BiFEZFg8Hph0SLYssVMmYmPh+98xwx0bYvHA7/6FTz2GHZkJNdmLmSx9xxWr4b+/Y9f00VEupvCiEgosW1TOXnpJYoj0hjvXcPA07NZvBiio4PdOBGRrtFsGpFQYlnw1FMwfjwp3iLeiriSdStquP12k1NERPoyhRGR3iImBt58E5KTmej9nCe5jaeftvnrX4PdMBGRntWlMDJv3jyGDBmCy+Vi8uTJrFq1qs1jn3vuOSzL8nu4XK4uN1ikTxsyxCzT6nBwA88xi3n89KewdGmwGyYi0nMCDiOvvPIKs2fP5r777mPt2rWMGzeOGTNmcPDgwTbPSUhIID8/v+mxp7XZBiJiTJ8Ov/sdAI9bdzGl/hPuuUfdNSLSdwUcRh599FFuvvlmbrzxRkaNGsWTTz5JTEwMzzzzTJvnWJZFZmZm0yMjI+OYGi3S582eDddcQ6Rdz3uuq3hn3l7dv0ZE+qyAwojH42HNmjVMnz69+QIOB9OnT2f58uVtnldRUcHgwYPJzs7msssuY9OmTe2+T21tLWVlZX4PkbBiWfD00zBuHAk1B0m55UqzlomISB8UUBg5dOgQXq+3RWUjIyODgoKCVs8ZMWIEzzzzDG+//TYvvPACPp+PM844g3379rX5PnPnziUxMbHpkZ2tZbIlDB01oJXPP4fbbuOZv9tcdx3U1ga7cSIi3afHZ9NMmTKF66+/nvHjxzN16lTeeOMN0tLS+Gs7UwTmzJlDaWlp02Pv3r093UyR3iknp2lAK889xxc//gsvvgjf/CaUlwe7cSIi3SOgMJKamkpERASFhYV+2wsLC8nMzOzUNZxOJxMmTGDHjh1tHhMdHU1CQoLfQyRsTZ8O/9//B8Bj3MnFroUsWGBub1NUFOS2iYh0g4DCSFRUFBMnTmThwoVN23w+HwsXLmTKlCmduobX62Xjxo1kZWUF1lKRcPbzn8M11+Dw1vO2fQlXJCxg9Wo46yzYvTvYjRMROTYBd9PMnj2bp556iueff57Nmzdz2223UVlZyY033gjA9ddfz5w5c5qOf/DBB/nPf/7Drl27WLt2Lddddx179uzhRz/6Ufd9CpG+zrLgmWfg4ouJqK3mtdpL+H7ah2zbBpMnw7vvBruBIiJdFxnoCVdffTVFRUXce++9FBQUMH78eD788MOmQa15eXk4HM0Z58iRI9x8880UFBTQr18/Jk6cyLJlyxg1alT3fQqRcOByweuvw3e+g+Odd3j+yCUMzH6WuXuvo7g42I0TEek63ShPJNR4PHDDDfCvfwGw5or/5ZRXf4UVYf4noKgI0tKC2D5g2zb4/e/hllvg1FNpaldtLQwcGNy2icjxoxvlifRVUVHwwgtmHAkw8Y3/wrpgOuTlceQIjB8PM2bAsmXBaV5FBVxyibnv3/nnm1nJR47A8OEwYgR8+WVw2iUivZfCiEgocjhM6eGvfzXrkXz8MYwZw87fPMfBQpv//AfOPNNMxPnkk+PTpNpaqK+H228Hz7Zc/sbNDK35iiNHoF8/mDS6CkdVOTfcAHV1x6dNIhIaFEZEQtktt8D69TBlCpSVceq8Gyk593J+ec0+IiNh4UKYOtUEk7//HSore64pL75ouoeef97m79zEzTzNsozLufDMSsjP54M9o9hlDWPfmoLGmcoiIoDCiEjoGz4cPv0U5s4Fp5PYBe/wu9eHUXzdz7j7+/k4nabL5pZb/BdK69bRYsXFfPrOEUpK4Bt8wHl8DIB773b46U/hssuI3LeHNLuI/+H/cf/9ptlnnQWLFnVjO0QkJGkAq0hf8sUXcMcdsGSJee1yUfH923g27W62lWXypz81Hzp1qhl+cu655nHqqeB0tn/54mJ44AF46SXw+SAzEz57eS/9zpuAXV3Nllv/QP9XHiNx/1dwwQUwf37zyQkJUFaGD4uJrGE9E0hNhZdfNmNLRKTv6ezvb4URkb7Gts0Yknvvhc8+M9siI80AkquvhquuoqQulpQUEygaORyQmgr9+5vl5v/7v832HTvg+983WWLVKigp8XszNvSfyZgDH/m3ITkZdu40F3n0UfP+CxaYMS7/+hdVE88i95s/I9kuJuv2K4M//UdEeoTCiEi4s21Tmbj/fjj6rtpJSdg3/pA9Iy/k44OjeW9tfxYvsTh8uPmQG26AZ581zysrIS6ued/Vw9fyuxP+RsR5U7GOHKb/b2+H6GgzcvXxx8HrNT/vuMOMan38cVN2Of98yMszU2qOvgPx5Mnw6ae8+KqTs8+GQYN67E9ERI4zhRERabZtG7z6Kjz3nKlYHG3YMHzXfZ/iCRdQUlTHwSKLqLMmcdrZLqivx/50KctWRbIr/XRG7Hif0x7/HlZVlf81Hn4YfvELWLvWzN297jpTamnNE0+YaklGBmzcCGVlHJ71GzL++iAREeYy999viikiEtoURkSkJZ8PPvwQ/vlPM75k2zZTyfi6uDiYNs0sEtJ4Y8ykJCgtNRWX00+HggJzY5yzzoLFiyEiIvD2vPIKfPe72A4HPxu7hD+vPwuASy81a7rFxHTxc4pIr6AwIiIdq6yEt94y4WTLFvPbv6QE8vObj0lJMT8b15y/5RaYN89UPr76CoYOPbbU8IMfwD/+ge12s/nCOznvw7sprE3ijDPM4Nbs7K5fWkSCS2FERLrGtk1F5OOPYdQouOgiEzxWrDDLq154oblxX3cpKzNLtjaszuZJSuPi+ndYUHE6Lhf89rdw113d93Yicvx09ve3emVFxJ9lwaRJ5nG0M8/smfdLSDDdPO+8A7/6FVFbtvCR6zz+a9TLPPTVpej/P0T6Pi16JiLBZ1lw2WWwejV84xs4aqr57ZZvsfPb93DDd5oHy/7tb2a28O7dwWuqiHQ/hRER6T1iY+Htt+GWW7B8Pob+3++IGDcaXn8d2+vj4YfN8ik5OWahtmef9R/eIiKhSWFERHqXyEizONo775jRq7m58O1vw7ix/P3CV5h+rhfLMj07P/yhWaRtxAizMqyIhCaFERHpnS65xMzW+c1vIDERa9MmzvnLd5mfP5qih/7On+/ayYTxNpZlZijv29d8alUVTJgA11wDDz4Ir71mlj+prQ3exxGRtmk2jYj0fiUl8Mc/wmOP+a9Hn5ZGzbeuYfnYH5Nw+igmTjSb1683YeTrHA4YMgRmz4ZZs8y2igrTM5SRYe61k5lpVrNva802Eek8Te0Vkb6nrMyscfL227BuHXg8zfsSE02KGDWKqku/yydx32DLjki2bHOwfkccmzeb08FMF54zxzzfsAHGjfN/m8jI5nByyy3mAWadt6eeMuu/DRpklmCprzfjbwcMML1K0dE9/qcgEjIURkSkb/N4zFooTz5pxpccfde/rzvlFOwrrqQsJpND24qJG5RMxg+/CRkZbNwIs+/0kV/ooKCgeW23RnPnwq9+ZZ6vWwennNL228yeDY88Yp7v22fujJyZabqNPB4z8HbUKHC7zU2NzzILzlJTAwcOmFCTna2l8KXv0DojItK3RUXBjBnmUVpqyhaHDpm7A7/4Imzf3nzs2rVYa9eSCCQ2bvsvC046iTFFRcw/dMjczO+XV1IfHUvNmk1U1UaQO/Yyki8+G77YCl99RUbaOG6+eRTFxXAkt4Tqw9WUuLPwek34GDKk+S23b4ddu8yj0Vdfwfvvm+cxMc1hZMUKMzsIzISiU081lZfoaFOh+da3aOqCEumLVBkRkb7Htk05IiLCBJV334X33oO6OtOVs2WLWdOkK0aPNkFo3brm+/RccAH255/DsmVYI0fCDTdQ7Yil4uX3sA8domjqtyk659tUfLqOqOVLOJhwAhl3fJcLLouB/HzWfXSQs2eNpa7e8ut5atR4E2Qw9xa89VbztiNHmspLdrapuERFdflPTKRHqJtGRKQ9e/bApk1mbnBiIvznPyawOBxw8slw+DC88QYUFZlVYkeMMCNj6+qar2FZJpB0RXKyGWiycaN5PWoUvh/fRvFXhXiWrqQo4QQ+mzybNaUnMPumUkZnFUNODn/9m8Wtt7a8nNNpAsmjj8J555ltpaWm22nAAI1lkeBQGBEROVb19ab7p39/E1KOHIEPPjDPp041YeSVV2DVKtOPcs458Nln8NJL5m7I3/gG9OsHzzzTHHzOOw+WLm1eRtayTFKoqWn5/g6HGWiyc6d5ffrpHPrh3Wz4KpKylV+xtWYIb1pXsHWXs2mS0fz5MH26ef6Pf5j7EEJzBSU723QBZWfDFVf4dy2JdDeFERGR3sK2TYUlLc2ED6/XjG0pLTWDRaKizDSdN980d0E+/XQTej74oPkaERHmvK/Lzsb+xsVUb91Dbf5hor99KTE/+xH068dT9+3jN4/2o7A2qdVmLVzYXEV55hkzw2jAAPNITzfNbXxccAFkZZljfT5NfZbOURgREQl1mzfD3r0wfrxJAI8/Di+8YOYUjxwJS5ZAYWHL8yIjzfE+H7bTiefCSygcdyGVew5RnX+EjQlnsSBiBv/ziJvBA8zc5Af+J4L772+7KR9/DNOmmedPPAE//7l/WDn68d3vNldcSkrMlOp+/SAurntv+Cy9n8KIiEhfV1NjuoS2boVhw0y54qmnTLcRmIEkR49xOVpMjJljXFwMCQnUnXM+RUMnU3mwkuqiCvYkjOGLhLPJq0im+mA5D/4+hpxJaYC5WeG997bdrMWLTS8WwF/+0rzAXGSkWaMlOdmEk379zLVOPdXs37LFzCxq3NevnxnOk5RkgoyqMaFHYUREJFzl5ppxKJmZZqzK88+becVZWaZL6IMPIC8v8OtmZ8Po0Xhr6/FUeChPy6EocyyHSabyiIcjNW62WSP44f8OI3uwA3w+Hn0miTn/5Wh1lhD4B5d58+D221s/zrLMpKiLLzav5883i/ImJrb+OOcc090EJrN5PAo0waB1RkREwlVOTvPzMWPg97/332/bJqSAGRyyZw989JHpFkpKMoFl9WpTYfF4TLDxeEyX0d69RABuwM0S0lt7//ebn852ubhr+DB8icnU19l4IlxUpAymJH4QlXVORi5wwI5UGDSIUdXJ/OSMCIoq3OyozGJfWQIlJaa4Y9smTDTats1MfmrLW281h5HXX4frrjOBprXgcvfdcPbZ5tjt280fRUICxMe3/JmaqplJPUFhREQk3FiWWS+lUXo6nHZay+Mau3icTjPwY+1a89va5TIDardvN+vpV1WZAFNaarqMCgqar1FTg7VpExFABBANxANZrTTr3IZHk9hY7AQXRETiTeiHdV8WpCaD08m1HjdnX5TGYWcGZd4YKmqjOOxNJL8+jYLafgzzRsNeN8THU14SD0Rg22YMy9G3NwL40Y+an69aBT/9adt/dM8/D9dfb54vXGi6oFoLLQkJcOWVzV1Qhw6ZpWm+flxcnPmjDHcKIyIi0jqns/l5QoIZwdo4irU9NTXN/SF798KOHeaOhGB+7tljlqz1es306YMHTbdRWZnZVllpnldWYlVWAhB5sBB2bGl6i6SGR5uubH56K/Dj2Fh88YnUxyRQ50qgxpVIdVQiVZGJZL2XCF8kQnw8p+6P4/FJsRypi6O4No5DNXEUVcdRWBVHQUU8CTHxNP7qPHjQZK+2jBzZHEZWroRvfrP142JjzdjkxlC0cSP8v//XHFYafzY+zjyzOUtWVZk/zqOPC8XbCYRgk0VEpFdzuZqfDxtmHoGqrDQVFo/HPIqLIT+fpn6bqiqTBoqKTPiprTX7Dh40P2trobq6qbpjVVYSUVlJBAeIBuKOfq+lzU9HNDzadBWmnyY+nu/ExHPJsHg8UXHUOuOpioyn2hFHhRVPOXGM+jweykxKyN4Vx61D4jhYHW/CTWUcpd5YKoijqjKGyMjmwSx795rbLbXlD39oDiPr1jXfVqBRQ/OIi4N77qFpkbzdu+G++/yDTWOIuewyMxMqWBRGRESk94mN7VqI+braWigvN5WW0tLmn609Kir8H5WV5md5ufnZOAq3thZqa4ngkH+o+brPm5+OBZ5o51D7J274ZSzExTE9KpbCIbHURMRSHRFHlSOWKmIpt+Mo98Uydm0s/MEc229PLNfExXKoOpYSbxyVxFJZax4Fh+KoqXYBZj71gQNmIbzWnHZacMOIZtOIiIh0hsfTHE4aA0rj88bXR4eX1n42PiorzaOH2Q4HVkwMxMZS54rjiOeokGPFUmnHUmHHcuZFccTfM8ssuteNNJtGRESkO0VFmUVSkpO753q2bbqSGqswjY+OXnfmmOpqACyfrykkOSlsffYTwN+AG6/q9jDSWQojIiIiwWBZZvG5mJjuv7bXa8bVBBJgsrO7vx2dpDAiIiLS10REmJGp8fHBbkmnaC06ERERCSqFEREREQkqhREREREJKoURERERCSqFEREREQkqhREREREJKoURERERCSqFEREREQkqhREREREJKoURERERCSqFEREREQkqhREREREJKoURERERCaqQuGuvbdsAlJWVBbklIiIi0lmNv7cbf4+3JSTCSHl5OQDZ2dlBbomIiIgEqry8nMTExDb3W3ZHcaUX8Pl8HDhwgPj4eCzL6rbrlpWVkZ2dzd69e0lISOi26/Ym+oyhr69/PtBn7Av6+ueDvv8Ze+Lz2bZNeXk5/fv3x+Foe2RISFRGHA4HAwcO7LHrJyQk9Mm/WEfTZwx9ff3zgT5jX9DXPx/0/c/Y3Z+vvYpIIw1gFRERkaBSGBEREZGgCuswEh0dzX333Ud0dHSwm9Jj9BlDX1//fKDP2Bf09c8Hff8zBvPzhcQAVhEREem7wroyIiIiIsGnMCIiIiJBpTAiIiIiQaUwIiIiIkEV1mFk3rx5DBkyBJfLxeTJk1m1alWwm9Qlc+fO5bTTTiM+Pp709HQuv/xytm7d6nfMtGnTsCzL73HrrbcGqcWBu//++1u0f+TIkU37a2pqmDVrFikpKcTFxXHllVdSWFgYxBYHbsiQIS0+o2VZzJo1Cwi97/CTTz7hkksuoX///liWxVtvveW337Zt7r33XrKysnC73UyfPp3t27f7HXP48GGuvfZaEhISSEpK4qabbqKiouI4for2tfcZ6+rquOeeexgzZgyxsbH079+f66+/ngMHDvhdo7Xv/aGHHjrOn6RtHX2PN9xwQ4v2X3TRRX7H9ObvsaPP19p/k5Zl8fDDDzcd05u/w878fujMv595eXlcfPHFxMTEkJ6ezi9/+Uvq6+u7rZ1hG0ZeeeUVZs+ezX333cfatWsZN24cM2bM4ODBg8FuWsCWLFnCrFmzWLFiBfPnz6euro4LL7yQyspKv+Nuvvlm8vPzmx6/+93vgtTirjn55JP92r906dKmfXfddRfvvvsur732GkuWLOHAgQNcccUVQWxt4D7//HO/zzd//nwArrrqqqZjQuk7rKysZNy4ccybN6/V/b/73e/44x//yJNPPsnKlSuJjY1lxowZ1NTUNB1z7bXXsmnTJubPn897773HJ598wi233HK8PkKH2vuMVVVVrF27lt/85jesXbuWN954g61bt3LppZe2OPbBBx/0+15/+tOfHo/md0pH3yPARRdd5Nf+f/3rX377e/P32NHnO/pz5efn88wzz2BZFldeeaXfcb31O+zM74eO/v30er1cfPHFeDweli1bxvPPP89zzz3Hvffe230NtcPUpEmT7FmzZjW99nq9dv/+/e25c+cGsVXd4+DBgzZgL1mypGnb1KlT7TvuuCN4jTpG9913nz1u3LhW95WUlNhOp9N+7bXXmrZt3rzZBuzly5cfpxZ2vzvuuMMeNmyY7fP5bNsO7e8QsN98882m1z6fz87MzLQffvjhpm0lJSV2dHS0/a9//cu2bdv+6quvbMD+/PPPm47597//bVuWZe/fv/+4tb2zvv4ZW7Nq1SobsPfs2dO0bfDgwfZjjz3Ws43rJq19xh/84Af2ZZdd1uY5ofQ9duY7vOyyy+zzzjvPb1sofYdf//3QmX8/P/jgA9vhcNgFBQVNxzzxxBN2QkKCXVtb2y3tCsvKiMfjYc2aNUyfPr1pm8PhYPr06SxfvjyILesepaWlACQnJ/ttf/HFF0lNTWX06NHMmTOHqqqqYDSvy7Zv307//v0ZOnQo1157LXl5eQCsWbOGuro6v+9z5MiRDBo0KGS/T4/HwwsvvMAPf/hDv5tDhvp32Cg3N5eCggK/7ywxMZHJkyc3fWfLly8nKSmJU089temY6dOn43A4WLly5XFvc3coLS3FsiySkpL8tj/00EOkpKQwYcIEHn744W4tfx8PixcvJj09nREjRnDbbbdRXFzctK8vfY+FhYW8//773HTTTS32hcp3+PXfD53593P58uWMGTOGjIyMpmNmzJhBWVkZmzZt6pZ2hcSN8rrboUOH8Hq9fn+wABkZGWzZsiVIreoePp+PO++8kzPPPJPRo0c3bf/e977H4MGD6d+/Pxs2bOCee+5h69atvPHGG0FsbedNnjyZ5557jhEjRpCfn88DDzzA2WefzZdffklBQQFRUVEt/oHPyMigoKAgOA0+Rm+99RYlJSXccMMNTdtC/Ts8WuP30tp/g437CgoKSE9P99sfGRlJcnJySH6vNTU13HPPPVxzzTV+NyH72c9+ximnnEJycjLLli1jzpw55Ofn8+ijjwaxtZ130UUXccUVV5CTk8POnTv59a9/zcyZM1m+fDkRERF96nt8/vnniY+Pb9EFHCrfYWu/Hzrz72dBQUGr/6027usOYRlG+rJZs2bx5Zdf+o2nAPz6Z8eMGUNWVhbnn38+O3fuZNiwYce7mQGbOXNm0/OxY8cyefJkBg8ezKuvvorb7Q5iy3rG3//+d2bOnEn//v2btoX6dxjO6urq+M53voNt2zzxxBN++2bPnt30fOzYsURFRfHjH/+YuXPnhsSy49/97nebno8ZM4axY8cybNgwFi9ezPnnnx/ElnW/Z555hmuvvRaXy+W3PVS+w7Z+P/QGYdlNk5qaSkRERIvRwoWFhWRmZgapVcfu9ttv57333uPjjz9m4MCB7R47efJkAHbs2HE8mtbtkpKSOPHEE9mxYweZmZl4PB5KSkr8jgnV73PPnj0sWLCAH/3oR+0eF8rfYeP30t5/g5mZmS0GlNfX13P48OGQ+l4bg8iePXuYP39+h7dmnzx5MvX19ezevfv4NLCbDR06lNTU1Ka/l33le/z000/ZunVrh/9dQu/8Dtv6/dCZfz8zMzNb/W+1cV93CMswEhUVxcSJE1m4cGHTNp/Px8KFC5kyZUoQW9Y1tm1z++238+abb7Jo0SJycnI6PGf9+vUAZGVl9XDrekZFRQU7d+4kKyuLiRMn4nQ6/b7PrVu3kpeXF5Lf57PPPkt6ejoXX3xxu8eF8neYk5NDZmam33dWVlbGypUrm76zKVOmUFJSwpo1a5qOWbRoET6frymI9XaNQWT79u0sWLCAlJSUDs9Zv349DoejRddGqNi3bx/FxcVNfy/7wvcIplo5ceJExo0b1+Gxvek77Oj3Q2f+/ZwyZQobN270C5WNwXrUqFHd1tCw9PLLL9vR0dH2c889Z3/11Vf2LbfcYiclJfmNFg4Vt912m52YmGgvXrzYzs/Pb3pUVVXZtm3bO3bssB988EF79erVdm5urv3222/bQ4cOtc8555wgt7zzfv7zn9uLFy+2c3Nz7c8++8yePn26nZqaah88eNC2bdu+9dZb7UGDBtmLFi2yV69ebU+ZMsWeMmVKkFsdOK/Xaw8aNMi+5557/LaH4ndYXl5ur1u3zl63bp0N2I8++qi9bt26ppkkDz30kJ2UlGS//fbb9oYNG+zLLrvMzsnJsaurq5uucdFFF9kTJkywV65caS9dutQePny4fc011wTrI7XQ3mf0eDz2pZdeag8cONBev36933+bjTMQli1bZj/22GP2+vXr7Z07d9ovvPCCnZaWZl9//fVB/mTN2vuM5eXl9i9+8Qt7+fLldm5urr1gwQL7lFNOsYcPH27X1NQ0XaM3f48d/T21bdsuLS21Y2Ji7CeeeKLF+b39O+zo94Ntd/zvZ319vT169Gj7wgsvtNevX29/+OGHdlpamj1nzpxua2fYhhHbtu0//elP9qBBg+yoqCh70qRJ9ooVK4LdpC4BWn08++yztm3bdl5enn3OOefYycnJdnR0tH3CCSfYv/zlL+3S0tLgNjwAV199tZ2VlWVHRUXZAwYMsK+++mp7x44dTfurq6vtn/zkJ3a/fv3smJgY+1vf+padn58fxBZ3zUcffWQD9tatW/22h+J3+PHHH7f69/IHP/iBbdtmeu9vfvMbOyMjw46OjrbPP//8Fp+7uLjYvuaaa+y4uDg7ISHBvvHGG+3y8vIgfJrWtfcZc3Nz2/xv8+OPP7Zt27bXrFljT5482U5MTLRdLpd90kkn2b/97W/9fpEHW3ufsaqqyr7wwgvttLQ02+l02oMHD7ZvvvnmFv9T15u/x47+ntq2bf/1r3+13W63XVJS0uL83v4ddvT7wbY79+/n7t277ZkzZ9put9tOTU21f/7zn9t1dXXd1k6robEiIiIiQRGWY0ZERESk91AYERERkaBSGBEREZGgUhgRERGRoFIYERERkaBSGBEREZGgUhgRERGRoFIYERERkaBSGBEREZGgUhgRERGRoFIYERERkaBSGBEREZGg+v8BShobqMw1FcgAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "params = init_params([feature_train.shape[1], 128, 10])\n",
        "num_iters = 200\n",
        "learning_rate = 1e-5\n",
        "losses_train, losses_test = [], []\n",
        "# Optimization loop\n",
        "for i in range(num_iters):\n",
        "    pred_train, cache = forward(feature_train, params)\n",
        "    pred_test, _ = forward(feature_test, params)\n",
        "    grads = grad(pred_train, target_train, params, cache)\n",
        "    loss_train = cross_entropy_loss_fn(pred_train, target_train)\n",
        "    loss_test = cross_entropy_loss_fn(pred_test, target_test)\n",
        "    for j in range(len(params)):\n",
        "        params['layer' + str(j + 1)]['W'] = params['layer' + str(j + 1)]['W'] - learning_rate * grads['dW' + str(j + 1)]\n",
        "        params['layer' + str(j + 1)]['b'] = params['layer' + str(j + 1)]['b'] - learning_rate * grads['db' + str(j + 1)]\n",
        "    print(f\"Iteration {i+1} training loss: {loss_train}, test loss: {loss_test}\")\n",
        "    losses_train.append(loss_train)\n",
        "    losses_test.append(loss_test)\n",
        "\n",
        "plt.plot(range(num_iters), losses_train, 'b--', range(num_iters), losses_test, 'r')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Accuracy on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9292\n"
          ]
        }
      ],
      "source": [
        "pred_test, _ = forward(feature_test, params)  # make predictions on test features\n",
        "pred_class_test = pred_test.argmax(axis=1)\n",
        "pred_correctness = pred_class_test==test_target_array\n",
        "num_correct = pred_correctness.sum()\n",
        "accuracy = num_correct / test_target_array.size\n",
        "print(accuracy)\n",
        "# prediction_correctness = category_test==target_test  # Find out which predictions are correct\n",
        "# num_correct = prediction_correctness.sum()  # Calculate how many correct predictions are made\n",
        "# accuracy = num_correct / target_test.shape[0]  # Calculate accuracy rate: correct # / total #"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
