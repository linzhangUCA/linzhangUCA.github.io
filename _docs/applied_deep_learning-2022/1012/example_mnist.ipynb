{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Shape of X [N, C, H, W]: torch.Size([16, 1, 28, 28])\n",
            "Shape of y: torch.Size([16]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    # break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterating and Visualizing the Dataset\n",
        "We can index `Datasets` manually like a list: `training_data[index]`. We use `matplotlib` to visualize some samples in our training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "img, label = training_data[sample_idx]\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the model, we pass it the input data. This executes the model’s `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. . We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module.\n",
        "\n",
        "Read more about [building neural networks in PyTorch](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class probability: tensor([[0.0930, 0.1003, 0.0964, 0.0950, 0.0990, 0.1060, 0.1108, 0.1058, 0.1012,\n",
            "         0.0925]], grad_fn=<SoftmaxBackward0>)\n",
            "Predicted class: tensor([6])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class probability: {pred_probab}\")\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n",
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "$$\\mathcal{L}(\\hat{y}, y) = -\\sum^C_{c=1}y_{c}log(\\hat{y}_{c})$$\n",
        "\n",
        "SGD stands for Stochastic Gradient Descent. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. In SGD, it uses only a single sample to perform each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305950  [    0/60000]\n",
            "loss: 0.364677  [ 6400/60000]\n",
            "loss: 0.266934  [12800/60000]\n",
            "loss: 0.282978  [19200/60000]\n",
            "loss: 0.195925  [25600/60000]\n",
            "loss: 0.276035  [32000/60000]\n",
            "loss: 0.141773  [38400/60000]\n",
            "loss: 0.319531  [44800/60000]\n",
            "loss: 0.334387  [51200/60000]\n",
            "loss: 0.243594  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.2%, Avg loss: 0.160642 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.099323  [    0/60000]\n",
            "loss: 0.158228  [ 6400/60000]\n",
            "loss: 0.093924  [12800/60000]\n",
            "loss: 0.163002  [19200/60000]\n",
            "loss: 0.090128  [25600/60000]\n",
            "loss: 0.172046  [32000/60000]\n",
            "loss: 0.075606  [38400/60000]\n",
            "loss: 0.228207  [44800/60000]\n",
            "loss: 0.231622  [51200/60000]\n",
            "loss: 0.178181  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.108289 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, adam_optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Training your model](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model can now be used to make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: \"2\", Actual: \"2\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[1][0], test_data[1][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit ('3.10.2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "0799405b1ea7a876aa76d5542349e91d1c61274aea2a6473a450413f24e1657a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
